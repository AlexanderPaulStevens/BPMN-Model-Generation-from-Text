{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#Introduction to this colab\n","\n","Welcome, this is a **stable** version that renders the necessary data for the relational extraction phase. Currently, this code extracts: **Document Name, Source Token ID, Source Type, Source Sentence ID, Source POS Tag, Target Token ID, Target Type, Target Sentence ID, Target POS Tag, Dependency Tags, and Relation Type from the structured data**. We did this for the original PET data as well as our own. The files you need to run this colab are \"LESCHNEIDER_formatted_relations_combined.json\" and \"PETv1.1-relations.json\""],"metadata":{"id":"lTyuoePo7rId"}},{"cell_type":"code","source":["# @title Installing Dependencies\n","!pip install spacy\n","!python -m spacy download en_core_web_md\n","!pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a1tQFcDttXFW","executionInfo":{"status":"ok","timestamp":1715704304422,"user_tz":-120,"elapsed":32073,"user":{"displayName":"Charlotte Schneider","userId":"15797801204403852136"}},"outputId":"d0013956-cb1d-4053-f671-fc23a45a344e","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n","Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n","Collecting en-core-web-md==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.3)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.4)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.7.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.25.2)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.2)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.2.2)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n","Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.5)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.1)\n","Installing collected packages: en-core-web-md\n","Successfully installed en-core-web-md-3.7.1\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_md')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}]},{"cell_type":"code","source":["# @title importing libraries and setting file paths\n","import json\n","import pandas as pd\n","import numpy as np\n","from itertools import product\n","import spacy\n","import json\n","from itertools import product\n","from sklearn.preprocessing import LabelEncoder\n","from google.colab import drive\n","import nltk\n","from nltk import pos_tag\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('averaged_perceptron_tagger')\n","\n","drive.mount('/content/drive')\n","# The paths\n","LESCHEIDER_PATH = '/content/drive/MyDrive/THESIS/DATA/LESCHNEIDER DATA/Documents/FORMATTED_RELATIONS/LESCHNEIDER_formatted_relations_combined.json'\n","PET_PATH = '/content/drive/MyDrive/THESIS/DATA/PET/actual PET data from Patrizio Bellan/PETv1.1-relations.json'"],"metadata":{"id":"Eg1vmX6Hck4r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715705748151,"user_tz":-120,"elapsed":1915,"user":{"displayName":"Charlotte Schneider","userId":"15797801204403852136"}},"outputId":"60a1f59e-3363-42fb-84d2-25af53a2ab20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["These are all the support functions for the dependency tag extraction."],"metadata":{"id":"o00ssYIgxCho"}},{"cell_type":"code","source":["#@title Support Functions\n","def format_sentences(tokens, sentence_IDs):\n","\n","    # Initialize a dictionary to hold sentence IDs as keys and their corresponding formatted sentences as values\n","    sentences = {}\n","\n","    # Set characters that should not be preceded by a space\n","    no_preceding_space_chars = {\"'\", \",\", \".\", \"s\", \";\", \"?\", \"!\", \":\", \"-\"}\n","\n","    for token, sentence_id in zip(tokens, sentence_IDs):\n","        # Initialize the sentence key in the dictionary if not already present\n","        if sentence_id not in sentences:\n","            sentences[sentence_id] = ''\n","\n","        # Clean token if it is a subword part that BERT might have split\n","        clean_token = token.replace('##', '')\n","\n","        # Determine if a space should be added\n","        should_add_space = True\n","        if clean_token in no_preceding_space_chars:  # Check against no_preceding_space_chars\n","            should_add_space = False\n","        if sentences[sentence_id] == '':  # Do not add space before the first token\n","            should_add_space = False\n","\n","        # Add space before the token if the condition is met\n","        if should_add_space:\n","            sentences[sentence_id] += ' ' + clean_token\n","        else:\n","            sentences[sentence_id] += clean_token\n","\n","    return sentences\n","\n","def get_entity_chunks(tokens, ner_tags, sentence_ids):\n","    chunks = []\n","    current_chunk = []\n","    start_index = None\n","    last_sentence_id = None  # Variable to track the sentence ID of the previous token\n","    sentence_start_index = 0  # Index where the current sentence starts in the tokens list\n","\n","    for i, (token, tag, sentence_id) in enumerate(zip(tokens, ner_tags, sentence_ids)):\n","        if sentence_id != last_sentence_id:\n","            # Reset start_index relative to the sentence when sentence_id changes\n","            sentence_start_index = i\n","            last_sentence_id = sentence_id\n","\n","        if tag.startswith('B-'):\n","            if current_chunk:\n","                # Append the current chunk before starting a new one\n","                chunks.append((' '.join(current_chunk), start_index, last_sentence_id))\n","                current_chunk = [token]\n","            else:\n","                current_chunk = [token]\n","            start_index = i - sentence_start_index  # Calculate start index relative to the start of the sentence\n","\n","        elif tag.startswith('I-') and current_chunk:\n","            current_chunk.append(token)\n","\n","        elif tag == 'O' and current_chunk:\n","            # Complete the current chunk if it exists\n","            chunks.append((' '.join(current_chunk), start_index, last_sentence_id))\n","            current_chunk = []\n","            start_index = None\n","\n","    if current_chunk:  # Add the last chunk if it exists\n","        chunks.append((' '.join(current_chunk), start_index, last_sentence_id))\n","\n","    return chunks\n","\n","def get_root_of_chunk(nlp, chunk_text):\n","\n","    doc = nlp(chunk_text)\n","    # Usually, the root token is the one whose head is outside the phrase itself or is itself\n","    for token in doc:\n","        if token.head == token or token.head not in doc:\n","            return token\n","    return doc[0]  # Fallback to the first token if no clear root is found\n","\n","def root_index_lookup(root, chunk, chunk_start_index, sentence, nlp):\n","\n","    # Calculate the number of tokens in the chunk\n","    chunk_length = len(nlp(chunk))\n","\n","    # Define the upper bound of the search\n","    end_index = chunk_start_index + chunk_length\n","\n","    #print(f\" This the the START index of the chunk: {chunk_start_index}\")\n","    #print(f\" This is the END index of the chunk: {end_index}\")\n","\n","    for i in range(chunk_start_index, end_index-1):\n","\n","        #print(f\" type sentence[i].text compared to root: {type(sentence[i].text)} and {type(root)}\")\n","        #print(f\"compared with root {sentence[i].text} <-> {root}\")\n","\n","        if sentence[i].text == root.text:\n","            return i\n","\n","    return -1  # Return -1 if the root word is not found within the bounds\n","\n","def check_dependency_path(source_token, target_token):\n","\n","    source_path = []\n","    target_path = []\n","    current_token = source_token\n","\n","    # Trace path from source_token to root\n","    while current_token.head != current_token:\n","        source_path.append(current_token)\n","        current_token = current_token.head\n","    source_path.append(current_token)  # Include root\n","\n","    current_token = target_token\n","    # Trace path from target_token to root\n","    while current_token.head != current_token:\n","        target_path.append(current_token)\n","        current_token = current_token.head\n","    target_path.append(current_token)  # Include root\n","\n","    # Find lowest common ancestor\n","    set_source = set(source_path)\n","    common_ancestors = [token for token in target_path if token in set_source]\n","    if common_ancestors:\n","        # Return the path from source to LCA and LCA to target\n","        lca = common_ancestors[0]\n","        source_to_lca = source_path[:source_path.index(lca)+1]\n","        lca_to_target = target_path[:target_path.index(lca)+1]\n","        return [token.dep_ for token in source_to_lca + lca_to_target[::-1]]\n","    return []\n","\n","def analyze_chunk_dependency(sentence_text, source_phrase, target_phrase, nlp):\n","\n","    doc = nlp(sentence_text)\n","\n","    #Since the chunks are sometimes too big and DET are often not relevant we try to pinpoint the root word\n","    #For example in the source_phrase \"The MPON\" we see that MPON is more relevant than The\n","    #print(\"\\nSENTENCE:\",doc)\n","    #print(f\"original source tokens: \\\"{source_phrase[0]}\\\"\")\n","    #print(f\"original target tokens: \\\"{target_phrase[0]}\\\"\")\n","    #print(f\"original source tokens INDEX POS: \\\"{source_phrase[1]}\\\"\")\n","    #print(f\"original target tokens INDEX POS: \\\"{target_phrase[1]}\\\"\")\n","\n","    source_root_token = get_root_of_chunk(nlp, source_phrase[0])\n","    target_root_token = get_root_of_chunk(nlp, target_phrase[0])\n","    #print(f\"extracted source root tokens: \\\"{source_root_token}\\\"\")\n","    #print(f\"extracted target root tokens: \\\"{target_root_token}\\\"\")\n","\n","    # Directly match root tokens based on text and position\n","\n","    source_tokens = [token for token in doc if token.text == source_root_token.text and token.i == root_index_lookup(source_root_token,source_phrase[0], source_phrase[1], doc, nlp)]\n","    target_tokens = [token for token in doc if token.text == target_root_token.text and token.i == root_index_lookup(target_root_token,target_phrase[0], target_phrase[1], doc, nlp)]\n","\n","    #print(\"acquired root source in text \", source_tokens)\n","    #print(\"acquired root target in text \", target_tokens)\n","\n","    if not source_tokens or not target_tokens:\n","        return [\"/\"]  # Return \"/\" indicating no tokens found\n","\n","    all_dependencies = set()  # Use a set to avoid duplicate entries\n","    dependency_found = False  # Reintroducing the boolean to track if any dependency was found\n","\n","    for s_token in source_tokens:\n","        for t_token in target_tokens:\n","            deps = check_dependency_path(s_token, t_token)\n","            if deps:\n","                all_dependencies.update(deps)\n","                dependency_found = True  # Set to True if any dependency is found\n","\n","\n","    if not dependency_found:\n","        #print(\"NO DEPENDENCY FOUND\")\n","        return [\"/\"]  # Return \"/\" if no dependencies were found\n","\n","    return list(all_dependencies)\n","\n","def encode_dependency_path(dependency_path, label_encoder):\n","\n","    encoded_path = []\n","    if not dependency_path:\n","        return -1\n","    else:\n","        for tag in dependency_path:\n","            try:\n","                # Normalize the tag to lower case and encode it\n","                encoded_tag = label_encoder.transform([tag.lower()])[0]\n","            except ValueError:\n","                # If the tag is unknown, assign a default value of -1\n","                encoded_tag = -1\n","            encoded_path.append(encoded_tag)\n","\n","        return encoded_path\n","\n","def initialize_label_encoder():\n","    # Not at all exhaustive\n","    possible_tags = ['acl', 'advcl', 'advmod', 'amod', 'appos', 'attr', 'aux', 'auxpass',\n","    'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass',\n","    'dative', 'dep', 'det', 'discourse', 'dislocated', 'dobj', 'expl',\n","    'fixed', 'flat', 'goeswith', 'iobj', 'intj', 'list', 'mark', 'meta',\n","    'neg', 'nounmod', 'npmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd',\n","    'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep',\n","    'prt', 'punct', 'quantmod', 'relcl', 'root', 'xcomp', 'npadvmod',\n","    'complm', 'infmod', 'partmod', 'hmod', 'hyph', 'num', 'number',\n","    'nmod', 'nn', 'npadvmod', 'possessive', 'rcmod', '/']\n","\n","    label_encoder = LabelEncoder()\n","    label_encoder.fit(possible_tags)\n","    return label_encoder\n","\n","def find_neighboring_tags(entity_idx, ner_tags, direction='prev'):\n","    step = -1 if direction == 'prev' else 1\n","    start, end = (entity_idx - 1, -1) if direction == 'prev' else (entity_idx + 1, len(ner_tags))\n","    for i in range(start, end, step):\n","        if i >= 0 and i < len(ner_tags) and (ner_tags[i].startswith('B-') or ner_tags[i] == 'O'):\n","            return ner_tags[i]\n","    return 'NONE'"],"metadata":{"id":"ADWYG6GdsBik","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zsTY4Rug0qfG","cellView":"form"},"outputs":[],"source":["#@title Main Function\n","\n","def create_df(path, nlp_in):\n","\n","    with open(path, 'r') as file:\n","      data = json.load(file)\n","\n","    nlp = nlp_in\n","    #nlp = spacy.load(\"en_core_web_trf\")  # takes forever, performs comparibly to en_core_web_md\n","    label_encoder = initialize_label_encoder() #To translate the paths into numbers for easier processing\n","\n","    transformed_data = []\n","\n","    for document in data:\n","\n","        # progress monitoring\n","        print(f\"\\rProcessing progess {round((data.index(document)+1)/len(data)*100,2)}%...\", end= \"  \", flush = True)  # Add logging to monitor progress due to slow processing\n","\n","        tokens = document['tokens']\n","        ner_tags = document['ner_tags']\n","        sentence_ids = document['sentence-IDs']\n","        doc_name = document['document name']\n","        sentences = format_sentences(tokens, sentence_ids)\n","        # Generate POS tags for the tokens\n","        pos_tags = pos_tag(tokens)\n","\n","\n","        document_chunks = get_entity_chunks(document['tokens'], document['ner_tags'], document['sentence-IDs'])\n","\n","        # Initialize entities list using comprehensive condition checks for 'B-' prefixes\n","        entities = {\n","            (sentence_id, token_id): {\n","                'token': tokens[idx],\n","                'type': ner_tags[idx],\n","                'sentence_id': sentence_id,\n","                'pos_tag': pos_tags[idx][1],\n","                'token_id': token_id,\n","                'index': idx\n","            }\n","            for idx, (token_id, sentence_id) in enumerate(zip(document['tokens-IDs'], sentence_ids))\n","            if ner_tags[idx].startswith('B-')\n","        }\n","\n","        relations_dict = {\n","            (doc_name, rel['source-head-sentence-ID'], rel['source-head-word-ID'], rel['target-head-sentence-ID'],\n","             rel['target-head-word-ID']): rel['relation-type']\n","            for rel in document['relations']\n","        }\n","\n","        # Generate all combinations of entities and check for relations within the same sentence\n","        for ((src_sentence_id, src_token_id), source), ((tgt_sentence_id, tgt_token_id), target) in product(\n","                entities.items(), repeat=2):\n","            if (src_sentence_id, src_token_id) != (tgt_sentence_id, tgt_token_id):  # Explicitly prevent self-comparison\n","                if src_sentence_id == tgt_sentence_id:\n","\n","\n","                    sentence = sentences[src_sentence_id]\n","\n","                    # Extract full chunks for the source and target using their token_ids and sentence_ids.\n","                    # It saves a tuple where the first element is the actual text and the second the start index position relative to the sentence.\n","                    source_chunk = next(((chunk[0], chunk[1]) for chunk in document_chunks\n","                                         if chunk[1] <= src_token_id and chunk[2] == src_sentence_id and src_token_id <\n","                                         chunk[1] + len(chunk[0].split())),\n","                                        (source['token'], src_token_id))\n","\n","                    target_chunk = next(((chunk[0], chunk[1]) for chunk in document_chunks\n","                                         if chunk[1] <= tgt_token_id and chunk[2] == tgt_sentence_id and tgt_token_id <\n","                                         chunk[1] + len(chunk[0].split())), (target['token'], tgt_token_id))\n","\n","\n","                    results = analyze_chunk_dependency(sentence, source_chunk, target_chunk, nlp)\n","                    results = encode_dependency_path(results, label_encoder)\n","                else:\n","                    results = [\"/\"]\n","                    results = encode_dependency_path(results, label_encoder)\n","\n","                relation_key = (doc_name, src_sentence_id, src_token_id, tgt_sentence_id, tgt_token_id)\n","                relation_type = relations_dict.get(relation_key, \"no_relation\")\n","\n","                # Get neighboring B-tags or 'O' for source and target\n","                src_prev_tag = find_neighboring_tags(source['index'], ner_tags, 'prev')\n","                src_next_tag = find_neighboring_tags(source['index'], ner_tags, 'next')\n","                tgt_prev_tag = find_neighboring_tags(target['index'], ner_tags, 'prev')\n","                tgt_next_tag = find_neighboring_tags(target['index'], ner_tags, 'next')\n","\n","                row = {\n","                    'document_name': doc_name,\n","                    'source_token': source['token'],\n","                    'source_type': source['type'],\n","                    'source_pos_tag': source['pos_tag'],  # Include source POS tag\n","                    'source_sentence_ID': src_sentence_id,\n","                    'source_token_ID': src_token_id,\n","                    'source_prev_tag': src_prev_tag,\n","                    'source_next_tag': src_next_tag,\n","                    'target_token': target['token'],\n","                    'target_type': target['type'],\n","                    'target_pos_tag': target['pos_tag'],  # Include target POS tag\n","                    'target_sentence_ID': tgt_sentence_id,\n","                    'target_token_ID': tgt_token_id,\n","                    'target_prev_tag': tgt_prev_tag,\n","                    'target_next_tag': tgt_next_tag,\n","                    'token_distance': abs(src_token_id - tgt_token_id),\n","                    'sentence_distance': abs(src_sentence_id - tgt_sentence_id),\n","                    'dependency_tags': results,\n","                    'relation_type': relation_type\n","                }\n","                transformed_data.append(row)\n","\n","\n","\n","\n","\n","    df_relations = pd.DataFrame(transformed_data)\n","    print(\"\\n---------------------------------\")\n","    print(\"Processing Done!\")\n","    print(\"---------------------------------\\n\")\n","    filtered_df = df_relations[['source_token', 'target_token', 'dependency_tags', 'relation_type']]\n","    filtered_df = filtered_df[filtered_df['dependency_tags'].apply(lambda x: x != [0])]\n","    filtered_df_again = filtered_df[filtered_df['relation_type'] != \"no_relation\"]\n","    df_true_relations = df_relations[df_relations['relation_type'] != \"no_relation\"]\n","\n","    pd.set_option('display.max_rows', 500)\n","    pd.set_option('display.max_columns', 5)# Example: 500 rows\n","\n","    print(f\"total number of elements in dataframe: {len(df_relations)}\")\n","    print(f\"total number of non empty dependency rows in dataframe: {len(filtered_df)}\")\n","    print(f\"total number of non empty dependency rows WITH RELATION in dataframe: {len(filtered_df_again)}\")\n","    print(f\"total number of non empty relation rows in dataframe: {len(df_true_relations)}\")\n","    print(\"\\n\")\n","\n","    return df_relations\n"]},{"cell_type":"code","source":["# Create a DataFrame\n","nlp = spacy.load(\"en_core_web_md\")\n","\n","#------------\n","#------------\n","# LESCHNEIDER\n","df_relations1 = create_df(LESCHEIDER_PATH, nlp)\n","print(\"----------TESTING----------\")\n","labels = df_relations1.relation_type\n","print(f\"These are the unique labels \\n{np.unique(labels)}\\n\")\n","print(f\"These are the heads of the table \\n{df_relations1.head(5)}\")\n","print(\"----------TESTING----------\")\n","df_relations1.to_csv('/content/drive/MyDrive/THESIS/CODING/NAM_TESTING/TESTING DATA/RE_TRAINING_DATA/total_relation_entity_pairs_DEPENDENCY_CONTEXT_md_LESCHNEIDER.csv', index=False)  #rename to liking\n","\n","#------------\n","#------------\n","#PET\n","df_relations2 = create_df(PET_PATH, nlp)\n","df_relations2.to_csv('/content/drive/MyDrive/THESIS/CODING/NAM_TESTING/TESTING DATA/RE_TRAINING_DATA/total_relation_entity_pairs_DEPENDENCY_CONTEXT_md_PET1.1.csv', index=False)  #rename to liking\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vBZ0EE_-w5x0","executionInfo":{"status":"ok","timestamp":1715706097429,"user_tz":-120,"elapsed":337981,"user":{"displayName":"Charlotte Schneider","userId":"15797801204403852136"}},"outputId":"bfd35bb8-5c41-43e3-9bd2-68107a21d086"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n","  warnings.warn(Warnings.W111)\n"]},{"output_type":"stream","name":"stdout","text":["Processing progess 100.0%...  \n","---------------------------------\n","Processing Done!\n","---------------------------------\n","\n","total number of elements in dataframe: 11054\n","total number of non empty dependency rows in dataframe: 54\n","total number of non empty dependency rows WITH RELATION in dataframe: 6\n","total number of non empty relation rows in dataframe: 425\n","\n","\n","----------TESTING----------\n","These are the unique labels \n","['actor performer' 'actor recipient' 'flow' 'further specification'\n"," 'no_relation' 'same gateway' 'uses']\n","\n","These are the heads of the table \n","               document_name source_token  ... dependency_tags relation_type\n","0  doc-20.1 - order shipping          the  ...             [0]   no_relation\n","1  doc-20.1 - order shipping          the  ...             [0]   no_relation\n","2  doc-20.1 - order shipping          the  ...             [0]   no_relation\n","3  doc-20.1 - order shipping          the  ...             [0]   no_relation\n","4  doc-20.1 - order shipping          the  ...             [0]   no_relation\n","\n","[5 rows x 19 columns]\n","----------TESTING----------\n","Processing progess 100.0%...  \n","---------------------------------\n","Processing Done!\n","---------------------------------\n","\n","total number of elements in dataframe: 91800\n","total number of non empty dependency rows in dataframe: 210\n","total number of non empty dependency rows WITH RELATION in dataframe: 19\n","total number of non empty relation rows in dataframe: 1830\n","\n","\n"]}]}]}