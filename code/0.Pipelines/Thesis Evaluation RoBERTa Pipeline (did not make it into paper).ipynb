{"cells":[{"cell_type":"markdown","metadata":{"id":"TGmyYgFOpZLT"},"source":["#1. INTRODUCTION\n","\n","\n","\n","This script will contain a complete pipeline from text to BPMN diagram that uses ```RoBERTa``` for the NER task, ```CATBOOST``` for the RE and the ```en_coreference_web_trf``` pretrained model from spacy-experimental used for coreference resolution. The script will load a pre-trained model. Refer to the latest colab for NER (```CrossValidation with RoBERTa Large```) and RE (```CatBoost_Crossvalidation_RE.ipynb```) to train and save the models.\n","\n","The purpose of this colab is to test the performance of this pipeline against the goldstandard information model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJ4KnaXU1-uh","collapsed":true},"outputs":[],"source":["#@title Google Colab & Input Variables\n","#@markdown Please select the desired PATH\n","%%capture\n","import os\n","import json\n","import copy\n","import gdown\n","flag_model_loading = '/content/model_loading_flag'\n","# Check if the flag file exists\n","if not os.path.exists(flag_model_loading):\n","  # List of Google Drive links, viewer mode only.\n","  links = {'CATBOOST_FULLDATA':'https://drive.google.com/file/d/1a5JGJVwM9KGWOZsqszfE0gpzeeoPuAH8/view?usp=share_link',\n","          'BERT_MODEL_FULLDATA':'https://drive.google.com/drive/folders/1-5PS_Rmu58QCstMquakjeQM0NufOOF0i?usp=share_link',\n","          'BERT_TOKENIZER_FULLDATA':'https://drive.google.com/drive/folders/1-GlY0MjXZkhikb9lfAWCP_0gouGWJ_1L?usp=share_link',\n","          'BERT_MODEL_BASE_UNCASED_6DOCS_OUT':'https://drive.google.com/drive/folders/1-Kj7wx52mc6jXzJhSiv80qBkUW4RawFO?usp=share_link',\n","          'BERT_MODEL_BASE_CASED_6DOCS_OUT':'https://drive.google.com/drive/folders/1-UQn4fDBghbBu_Ysl5gzplmTDxDc7QhB?usp=share_link',\n","          'RoBERTa_MODEL_6DOCS_OUT':'https://drive.google.com/drive/folders/1-58USnezAreSrMtHd7ssHG-IgR1JSnTe?usp=sharing',\n","          'RoBERTa_TOKENIZER_6DOCS_OUT':'https://drive.google.com/drive/folders/1-508ReXo8v-ZuO27qJRbBt9MnvT62m9y?usp=sharing',\n","          'BERT_TOKENIZER_BASE_UNCASED_6DOCS_OUT': 'https://drive.google.com/drive/folders/1-GgGZBruoqJBrTGB-QJV9beOnZHNQjSi?usp=share_link',\n","          'BERT_TOKENIZER_BASE_CASED_6DOCS_OUT': 'https://drive.google.com/drive/folders/1-Cem5F1Dwxe2NdzsmnQcvkP4L9IOtdtH?usp=share_link',\n","          'CATBOOST_6DOCS_OUT': 'https://drive.google.com/file/d/1MFWeI73gJRGjRpX6RgV-lXVTCJsfg-4L/view?usp=share_link',\n","          'BERT_MODEL_DOC20_15_OUT': 'https://drive.google.com/drive/folders/1vHu8vxJVNidU6lEsSWY-dCEx1KAnyFP1?usp=share_link',\n","          'BERT_TOKENIZER_DOC20_15_OUT': 'https://drive.google.com/drive/folders/1vHu8vxJVNidU6lEsSWY-dCEx1KAnyFP1?usp=share_link',\n","          'CATBOOST_DOC15_OUT': 'https://drive.google.com/file/d/1Roub03hQMSDk_AmTyRDM3UX3Ult6xBD9/view?usp=share_link'\n","          }\n","\n","  # Function to extract ID from shareable link\n","  def extract_id(shareable_link):\n","      if 'file/d/' in shareable_link:\n","          return shareable_link.split('/file/d/')[1].split('/')[0]\n","      elif 'drive/folders/' in shareable_link:\n","          return shareable_link.split('/drive/folders/')[1].split('?')[0]\n","      else:\n","          raise ValueError(\"Invalid shareable link\")\n","\n","  # Function to download content based on ID type\n","  def download_content(shareable_link, destination):\n","      file_or_folder_id = extract_id(shareable_link)\n","      if 'file/d/' in shareable_link:\n","          url = f\"https://drive.google.com/uc?id={file_or_folder_id}\"\n","          gdown.download(url, destination, quiet=True)\n","      elif 'drive/folders/' in shareable_link:\n","          gdown.download_folder(id=file_or_folder_id, output=destination)\n","      else:\n","          raise ValueError(\"Invalid shareable link\")\n","\n","  # Define the variables using simpler names\n","  BERT_MODEL_SELECTION = \"RoBERTa_MODEL_6DOCS_OUT\" #@param [\"RoBERTa_MODEL_6DOCS_OUT\"] {type:\"string\"}\n","  BERT_TOKENIZER_SELECTION = \"RoBERTa_TOKENIZER_6DOCS_OUT\" #@param [\"RoBERTa_TOKENIZER_6DOCS_OUT\"] {type:\"string\"}\n","  CATBOOST_MODEL_SELECTION = \"CATBOOST_6DOCS_OUT\" #@param [\"CATBOOST_6DOCS_OUT\", \"CATBOOST_DOC15_OUT\"] {type:\"string\"}\n","\n","  selection = {\n","    \"BERT_MODEL\": links[BERT_MODEL_SELECTION],\n","    \"BERT_TOKENIZER\": links[BERT_TOKENIZER_SELECTION],\n","    \"CATBOOST_MODEL\": links[CATBOOST_MODEL_SELECTION]}\n","\n","  # Download each link to the specified destination\n","  for key, value in selection.items():\n","      destination = f'/content/{key}'\n","      download_content(value, destination)\n","\n","\n","  BERT_MODEL_PATH = '/content/BERT_MODEL'\n","  BERT_TOKENIZER_PATH = '/content/BERT_TOKENIZER'\n","  CATBOOST_MODEL_PATH = '/content/CATBOOST_MODEL'\n","\n","\n","\n","\n","  # Create the flag file to indicate that imports have been done\n","  with open(flag_model_loading, 'w') as f:\n","      f.write('Model Loaded')\n","\n","else:\n","    print(\"Model already loaded. Skipping step.\")\n","print(\"All downloads completed!\")"]},{"cell_type":"markdown","metadata":{"id":"o1ieWTM_3PZN"},"source":["\n","Now we load the **testing data**. These documents were kept separate before training BERT. Refer to section 3 in \"```CrossValidation with RoBERTa Large```\" for the code. Note that this testing data will also be used for Relational Extraction and in the rest of this pipeline. See folder '/FINAL/Data/DEPENDENCIES/' for other testing docs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7WvxZpy_3SXL"},"outputs":[],"source":["#@title Testing Document Input\n","#@markdown Please select a testing document\n","#@markdown **note that if you chose the 6DOC_OUT BERT model you cannot select document 20.15**\n","def load_and_group_ner_data(file_path):\n","    grouped_data = {}\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            entry = json.loads(line)\n","            document_name = entry['document name']  # Adjusted to use 'document name'\n","            if document_name not in grouped_data:\n","                grouped_data[document_name] = []\n","            grouped_data[document_name].append(entry)\n","\n","    # Sort each group by 'sentence-ID'\n","    for doc in grouped_data.values():\n","        doc.sort(key=lambda x: x['sentence-ID'])\n","\n","    return list(grouped_data.values())\n","\n","class ModelDocConflict(Exception):\n","    def __init__(self):\n","        self.message = \"You are trying to select a document which the model has seen before! Either select another model or another document.\"\n","        super().__init__(self.message)\n","\n","testing_documents = {'document_1.json': 'https://drive.google.com/file/d/1-UJ2WSglf7lZCflRP0yW2eDGBpF4Bxhu/view?usp=share_link',\n","                     'document_2.json': 'https://drive.google.com/file/d/1-cI4jrLVn8MPIfV6AmufXEeiYvT7Ku8m/view?usp=share_link',\n","                     'document_3.json': 'https://drive.google.com/file/d/1-_p2zV_hU6Vn_sKwfs7CNY_hfLvkIkvx/view?usp=share_link',\n","                     'document_4.json': 'https://drive.google.com/file/d/1-_So5HZ_b2JuJuR7MY6Mk18l9WNMmDPn/view?usp=share_link',\n","                     'document_5.json': 'https://drive.google.com/file/d/1-_RqmR9c2GL5YzKRI9aiKSD4yntcYXdu/view?usp=share_link',\n","                     'document_6.json': 'https://drive.google.com/file/d/1-V1y5_i4d_3MGguhAgkuzhRfuXU2RMPx/view?usp=share_link',\n","                     'doc-20.15 - library request.json': 'https://drive.google.com/file/d/1-ASmXjjGG9j3aVFLUCtvt9N3qyWzlWkQ/view?usp=share_link'}\n","\n","\n","document_name_selection = 'document_1.json' #@param ['document_1.json', 'document_2.json', 'document_3.json', 'document_4.json', 'document_5.json', 'document_6.json', 'doc-20.15 - library request.json'] {type:\"string\"}\n","\n","selection = {\n","    \"TESTING_DOC\": testing_documents[document_name_selection]}\n","\n","# Download each link to the specified destination\n","for key, value in selection.items():\n","    destination = f'/content/{key}'\n","    download_content(value, destination)\n","\n","file_path = '/content/TESTING_DOC'\n","\n","if document_name_selection == 'doc-20.15 - library request.json' and BERT_MODEL_PATH == BERT_MODEL_6DOCS_OUT:\n","  raise ModelDocConflict\n","\n","test_input_data = load_and_group_ner_data(file_path)\n","flat_data = [item for group in test_input_data for item in group]\n","testing_doc_name = copy.deepcopy(flat_data[0]['document name'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sJUvHrMXVG2E"},"outputs":[],"source":["#@title goldstandard data use selection\n","goldstandard_NER_check = \"False\" #@param [\"True\", \"False\"] {type: \"string\"}\n","goldstandard_RE_check = \"False\" #@param [\"True\", \"False\"] {type: \"string\"}\n","\n","use_goldstandardNER= False\n","if goldstandard_NER_check == \"True\":\n","  use_goldstandardNER = True\n","\n","use_goldstandardRE = False\n","if goldstandard_RE_check == \"True\":\n","  use_goldstandardRE = True"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"0CqPH5XVzb4D"},"outputs":[],"source":["#@title Installations and Downloads (takes 4 minutes)\n","%%capture\n","# Define the path to the flag file, we do this so you can rerun the whole colab on different examples without having to wait 4 minutes each time.\n","flag_file_installations = '/content/installed_flag'\n","\n","# Check if the flag file exists\n","if not os.path.exists(flag_file_installations):\n","    # Installations\n","    !pip install spacy\n","    !pip install nltk\n","    !pip install catboost\n","    !pip install spacy-experimental==0.6.2\n","    !pip install https://github.com/explosion/spacy-experimental/releases/download/v0.6.1/en_coreference_web_trf-3.4.0a2-py3-none-any.whl\n","    !python -m spacy download en_core_web_md\n","    !pip install graphviz\n","\n","    # Create the flag file\n","    with open(flag_file_installations, 'w') as f:\n","        f.write('Installed')\n","else:\n","    print(\"Packages already installed. Skipping installations.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D-SSMbQSznG3"},"outputs":[],"source":["#@title Import Libraries\n","%%capture\n","# Define the path to the flag file\n","flag_file_imports = '/content/imports_flag'\n","\n","# Check if the flag file exists\n","if not os.path.exists(flag_file_imports):\n","    # Perform the imports and any necessary downloads\n","    import random\n","    import torch\n","    import spacy\n","    import numpy as np\n","    import pandas as pd\n","    import seaborn as sns\n","    import matplotlib.pyplot as plt\n","    import gdown\n","    from tqdm import tqdm\n","    from collections import defaultdict\n","    from torch.optim import AdamW\n","    from torch.utils.data import DataLoader, Dataset, random_split, Subset\n","    from transformers import (\n","        BertTokenizer, BertForTokenClassification,\n","        get_linear_schedule_with_warmup,\n","        AutoModelForTokenClassification,\n","        TrainingArguments, Trainer,\n","        RobertaTokenizer, RobertaForTokenClassification\n","    )\n","    from sklearn.model_selection import (\n","        GroupShuffleSplit, GroupKFold, train_test_split\n","    )\n","    from sklearn.metrics import (\n","        precision_recall_fscore_support, confusion_matrix,\n","        accuracy_score, f1_score\n","    )\n","    from sklearn.preprocessing import LabelEncoder\n","    from catboost import CatBoostClassifier, Pool, metrics, cv\n","    from itertools import product\n","    import nltk\n","    from nltk import pos_tag\n","    from nltk.tokenize import word_tokenize\n","    from graphviz import Digraph\n","    from IPython.display import Image, display\n","\n","    # NLTK downloads\n","    nltk.download('averaged_perceptron_tagger')\n","\n","    # Create the flag file to indicate that imports have been done\n","    with open(flag_file_imports, 'w') as f:\n","        f.write('Imports completed')\n","else:\n","    print(\"Imports already completed. Skipping imports.\")"]},{"cell_type":"markdown","metadata":{"id":"rQ2tkQD2g6En"},"source":["#2. NER Task\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"eifjlIngfX1_"},"outputs":[],"source":["# @title Variable setup for BERT\n","\n","# Set a fixed seed for all random operations\n","SEED = 42\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","random.seed(SEED)\n","\n","# If you're using CUDA:\n","torch.cuda.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)  # For multi-GPU setups\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","#Is needed for the translation between floats and labels, as BERT's output is a float.\n","label_map = {\n","    \"O\": 0,\n","    \"B-Actor\": 1, \"I-Actor\": 2,\n","    \"B-Activity\": 3, \"I-Activity\": 4,\n","    \"B-Activity Data\": 5, \"I-Activity Data\": 6,\n","    \"B-Further Specification\": 7, \"I-Further Specification\": 8,\n","    \"B-XOR Gateway\": 9, \"I-XOR Gateway\": 10,\n","    \"B-Condition Specification\": 11, \"I-Condition Specification\": 12,\n","    \"B-AND Gateway\": 13, \"I-AND Gateway\": 14\n","}\n","\n","short_label_map = {\n","    \"O\": 0,\n","    \"Actor\": 1,\n","    \"Activity\": 2,\n","    \"Activity Data\": 3,\n","    \"Further Specification\": 4,\n","    \"XOR Gateway\": 5,\n","    \"Condition Specification\": 6,\n","    \"AND Gateway\": 7\n","}\n","\n","# Reverse map for evaluation purposes\n","reverse_label_map = {v: k for k, v in label_map.items()}\n","NUM_LABELS = len(label_map)  # Correctly reflects the actual classification labels\n","\n","# Constants, can be changed for model optimisation\n","MAX_LEN = 128  # Or any max length suited to your data\n","BATCH_SIZE = 8\n","EPOCHS = 10"]},{"cell_type":"markdown","metadata":{"id":"1usum5N3xNOO"},"source":["##2.1 LOADING THE MODEL\n"]},{"cell_type":"markdown","metadata":{"id":"5mloozYgwLIX"},"source":["\n","The code below are pre-requisite code that needs to be run before prediction can happen on the testing document. This code is similar to the code used for training BERT but with some modifications."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3yuZaR1t51p3"},"outputs":[],"source":["# @title Helper Code for Prediction & Eval\n","def convert_IOB2_to_chunks(nested_label_list, reverse_label_map):\n","\n","    all_chunks = []\n","    #print(nested_label_list)\n","    for sentence in nested_label_list:\n","        #print(sentence)\n","        chunks = []\n","        current_chunk = []\n","        current_type = None\n","\n","        for idx, label_idx in enumerate(sentence):\n","            label = reverse_label_map[label_idx]\n","\n","            if label.startswith('B-'):\n","                if current_chunk:\n","                    chunks.append((current_type, current_chunk))\n","                    current_chunk = []\n","                current_type = label[2:]\n","                current_chunk = [idx, idx]  # Start a new chunk\n","\n","            elif label.startswith('I-') and current_type == label[2:]:\n","                current_chunk[1] = idx  # Extend the current chunk\n","\n","            elif label == 'O':\n","                if current_type != 'O':  # Start a new 'O' chunk if the last chunk wasn't 'O'\n","                    if current_chunk:\n","                        chunks.append((current_type, current_chunk))\n","                        current_chunk = []\n","                    current_type = 'O'\n","                current_chunk = [idx, idx] if not current_chunk else [current_chunk[0], idx]\n","\n","            else:  # For non-matching 'I-' or different entity types\n","                if current_chunk:\n","                    chunks.append((current_type, current_chunk))\n","                    current_chunk = []\n","                current_type = None\n","\n","        if current_chunk:  # Add the last chunk if exists\n","            chunks.append((current_type, current_chunk))\n","\n","        all_chunks.append(chunks)\n","\n","    return all_chunks\n","\n","def convert_int_2string(nested_label_list, reverse_label_map):\n","\n","    all_labels = []\n","    for sentence in nested_label_list:\n","        string_labels = [reverse_label_map[label_idx] for label_idx in sentence]\n","        all_labels.append(string_labels)\n","\n","    return all_labels\n","\n","\n","def calculate_metrics_per_sentence_by_label(true_chunks_all_sentences, pred_chunks_all_sentences):\n","    # Initialize counters for true positives, false positives, and false negatives per label\n","    tp_dict = defaultdict(int)\n","    fp_dict = defaultdict(int)\n","    fn_dict = defaultdict(int)\n","    support_dict = defaultdict(int)\n","\n","\n","    labels = set(label for sentence in true_chunks_all_sentences + pred_chunks_all_sentences for label, _ in sentence)\n","\n","    #For the calculation of support\n","    for true_chunks in true_chunks_all_sentences:\n","        for label, _ in true_chunks:\n","            support_dict[label] += 1\n","\n","    #For the calculation of precision, recall and F1\n","    for true_chunks, pred_chunks in zip(true_chunks_all_sentences, pred_chunks_all_sentences):\n","        true_set = set((label, tuple(indices)) for label, indices in true_chunks)\n","        pred_set = set((label, tuple(indices)) for label, indices in pred_chunks)\n","\n","        # True Positives per label\n","        for label, indices in true_set & pred_set:\n","            tp_dict[label] += 1\n","\n","        # False Positives per label\n","        for label, indices in pred_set - true_set:\n","            fp_dict[label] += 1\n","\n","        # False Negatives per label\n","        for label, indices in true_set - pred_set:\n","            fn_dict[label] += 1\n","\n","    # Calculate metrics per label\n","    metrics_per_label = {}\n","    for label in labels:\n","        tp = tp_dict[label]\n","        fp = fp_dict[label]\n","        fn = fn_dict[label]\n","        precision = tp / (tp + fp) if tp + fp > 0 else 0\n","        recall = tp / (tp + fn) if tp + fn > 0 else 0\n","        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n","        support = support_dict[label]\n","\n","        metrics_per_label[label] = {\n","            \"Precision\": precision,\n","            \"Recall\": recall,\n","            \"F1\": f1,\n","            \"Support\": support\n","        }\n","\n","    return metrics_per_label\n","\n","def evaluate_model(model, val_dataloader, reverse_label_map, tokenizer):\n","    # Set the device to GPU if available, else CPU\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    # Switch the model to evaluation mode to disable dropout layers\n","    model.eval()\n","\n","    # Initialize variables to accumulate loss and store predictions and true labels\n","    eval_loss = 0\n","    nb_eval_steps = 0\n","    true_labels = []\n","    pred_labels = []\n","    documents = []\n","    doc_names = []\n","\n","    # Iterate over batches in the validation dataloader\n","    for batch in val_dataloader:\n","        # Move batch data to the same device as the model\n","        batch = {k: v.to(device) if k != 'doc_name' else v for k, v in batch.items()}\n","\n","        # Perform inference (forward pass) without computing gradients\n","        with torch.no_grad():\n","            outputs = model(**{k: v for k, v in batch.items() if k != 'doc_name'})\n","            logits = outputs.logits\n","            eval_loss += outputs.loss.item()\n","\n","        # Convert logits to predicted class indices\n","        preds = np.argmax(logits.detach().cpu().numpy(), axis=2)\n","\n","        # Collect true and predicted labels for each sentence in the batch\n","        for i in range(batch[\"input_ids\"].shape[0]):\n","            input_ids = batch[\"input_ids\"][i].cpu().numpy()\n","            mask = batch[\"attention_mask\"][i].cpu().numpy()\n","            true_sequence = batch[\"labels\"][i].cpu().numpy()[mask == 1]\n","            pred_sequence = preds[i][mask == 1]\n","            filtered_input_ids = input_ids[mask == 1]\n","\n","            # Decode tokens and preserve the 'Ġ' prefix, this will be removed in a later stage\n","            sentence_tokens = [tokenizer.convert_ids_to_tokens([tok_id])[0] for tok_id in filtered_input_ids]\n","\n","            # Append individual sentence-level lists\n","            true_labels.append(true_sequence.tolist())  # Convert to list for consistency\n","            pred_labels.append(pred_sequence.tolist())\n","            documents.append(sentence_tokens)\n","            doc_names.append(batch['doc_name'][i])\n","\n","        nb_eval_steps += 1\n","\n","    # Convert IOB2 labels to chunks\n","    true_labels_entities = convert_IOB2_to_chunks(true_labels, reverse_label_map)\n","    pred_labels_entities = convert_IOB2_to_chunks(pred_labels, reverse_label_map)\n","\n","    # Calculate performance metrics\n","    label_metrics = calculate_metrics_per_sentence_by_label(true_labels_entities, pred_labels_entities)\n","    header = \"{:<35} {:<10} {:<10} {:<10} {:<10}\\n\".format(\"Label\", \"Precision\", \"Recall\", \"F1\", \"Support\")\n","    row_format = \"{:<35} {:<10} {:<10} {:<10} {:<10}\\n\"\n","    print(header)\n","    for label, metrics in label_metrics.items():\n","        print(row_format.format(label, \"{:.4f}\".format(metrics['Precision']), \"{:.4f}\".format(metrics['Recall']), \"{:.4f}\".format(metrics['F1']), \"{:.4f}\".format(metrics['Support'])))\n","\n","    return documents, doc_names, true_labels_entities, pred_labels_entities, true_labels, pred_labels\n","\n","\n","class NERDataset(Dataset):\n","    def __init__(self, doc_name, sentences, labels, tokenizer, max_len, label_map):\n","        self.doc_name = doc_name #To keep track of what document we are analyzing\n","        self.sentences = sentences  # List of sentences (each sentence is a list of words)\n","        self.labels = labels        # List of label sequences corresponding to each sentence\n","        self.tokenizer = tokenizer  # BERT tokenizer\n","        self.max_len = max_len      # Maximum sequence length\n","        self.label_map = label_map  # Mapping from label strings to integers\n","\n","    def __len__(self):\n","        return len(self.sentences)\n","\n","    def __getitem__(self, idx):\n","\n","\n","        words = self.sentences[idx]\n","        word_labels = self.labels[idx]\n","\n","        # Tokenize words and align labels with tokens\n","        tokens = []\n","        aligned_labels = []\n","        for word, label in zip(words, word_labels):\n","            word_tokens = self.tokenizer.tokenize(word, add_prefix_space=True)\n","            tokens.extend(word_tokens)\n","            # Extend the label to all subwords\n","            aligned_labels.extend([label] * len(word_tokens))\n","\n","        # Truncate tokens and labels if they exceed max_len\n","        tokens = tokens[:self.max_len - 2]\n","        aligned_labels = aligned_labels[:self.max_len - 2]\n","\n","        # Convert tokens and labels to model inputs\n","        input_ids = self.tokenizer.convert_tokens_to_ids(['<s>'] + tokens + ['</s>'])\n","        attention_mask = [1] * len(input_ids)\n","        label_ids = [self.label_map['O']] + [self.label_map[label] for label in aligned_labels] + [self.label_map['O']]\n","\n","        # Padding\n","        padding_length = self.max_len - len(input_ids)\n","        input_ids += [self.tokenizer.pad_token_id] * padding_length\n","        attention_mask += [0] * padding_length\n","        label_ids += [self.label_map['O']] * padding_length\n","\n","        return {\n","            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n","            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n","            'labels': torch.tensor(label_ids, dtype=torch.long),\n","            'doc_name': self.doc_name[idx]\n","        }\n","\n","\n","def dataloader_internal(document):\n","    val_dataset = NERDataset(\n","                doc_name=[entry['document name'] for entry in document],\n","                sentences=[entry['tokens'] for entry in document],\n","                labels=[entry['ner-tags'] for entry in document],\n","                tokenizer=tokenizer,\n","                max_len=MAX_LEN,\n","                label_map=label_map\n","            )\n","\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n","\n","    return val_loader\n","\n","# Function to process model evaluation output and restructure data for the SpaCy Neuralcoref to parse later\n","def process_evaluation_output(documents, true_chunks_list, pred_chunks_list, doc_names):\n","    processed_output = []\n","    sentence_ID = 0\n","    target_labels = ['Further Specification', 'AND Gateway', 'Activity', 'Activity Data', 'Actor', 'Condition Specification', 'XOR Gateway']\n","\n","    for sentence, true_chunks, pred_chunks, doc_name in zip(documents, true_chunks_list, pred_chunks_list, doc_names):\n","        formatted_sentence = ''\n","        no_preceding_space_chars = {\"'\", \",\", \".\", \"s\", \";\", \"?\", \"!\", \":\", \"-\"}\n","        sharp_adjustments = {}\n","        sharp_adjustment_count = 0\n","\n","        for i, token in enumerate(sentence):\n","\n","            clean_token = token.replace('Ġ', '')\n","\n","\n","            # Determine if a space should be added before the current token\n","            should_add_space = True\n","            if i == 0:  # Do not add space before the first token\n","                should_add_space = False\n","            elif not token.startswith('Ġ'):  # Do not add space for continuation tokens\n","\n","                sharp_adjustment_count += 1\n","                should_add_space = False\n","            elif clean_token in no_preceding_space_chars:  # Check against no_preceding_space_chars\n","                should_add_space = False\n","            elif sentence[i - 1] == \" \":  # Do not add space if the previous token is an explicit space\n","                should_add_space = False\n","\n","            # Add space before the token if the condition is met\n","            if should_add_space:\n","                formatted_sentence += ' ' + clean_token\n","            else:\n","                formatted_sentence += clean_token\n","\n","            # Record the adjustment count at this position\n","            sharp_adjustments[i] = sharp_adjustment_count\n","\n","        # Remove leading and trailing spaces, and fix tokens starting with 'Ġ'\n","        formatted_sentence = formatted_sentence.strip()\n","\n","        # Remove leading and trailing special tokens <s> and </s>\n","        if formatted_sentence.startswith('<s>'):\n","            formatted_sentence = formatted_sentence[3:].strip()\n","        if formatted_sentence.endswith('</s>'):\n","            formatted_sentence = formatted_sentence[:-4].strip()\n","\n","        # Initialize list to hold relevant token spans\n","        relevant_true_spans = []\n","        relevant_pred_spans = []\n","\n","        print(f\"Sentence ID: {sentence_ID}, True Chunks: {true_chunks}, Sharp Adjustments: {sharp_adjustments}\")\n","\n","        for label, span in true_chunks:\n","\n","            if label in target_labels:\n","                print(f\"True Chunk - Label: {label}, Span: {span}\")\n","                if not sentence[span[0]].startswith('Ġ'):\n","                    continue\n","                adjusted_start = span[0] - 1 - sharp_adjustments.get(span[0] - 1, 0)\n","                adjusted_end = span[1] - 1 - sharp_adjustments.get(span[1] - 1, 0)\n","                if span[1] < len(sentence) and not sentence[span[1]].startswith('Ġ'):\n","                    adjusted_end -= 1\n","                adjusted_end = max(adjusted_start, adjusted_end)\n","                adjusted_span = (adjusted_start, adjusted_end)\n","                relevant_true_spans.append((label, sentence_ID, adjusted_span))\n","                print(f\"Adjusted True Span: {adjusted_span}\")\n","\n","        for label, span in pred_chunks:\n","\n","            if label in target_labels:\n","                print(f\"Pred Chunk - Label: {label}, Span: {span}\")\n","                if not sentence[span[0]].startswith('Ġ'):\n","                    continue\n","                adjusted_start = span[0] - 1 - sharp_adjustments.get(span[0] - 1, 0)\n","                adjusted_end = span[1] - 1 - sharp_adjustments.get(span[1] - 1, 0)\n","                if span[1] < len(sentence) and not sentence[span[1]].startswith('Ġ'):\n","                    adjusted_end -= 1\n","                adjusted_end = max(adjusted_start, adjusted_end)\n","                adjusted_span = (adjusted_start, adjusted_end)\n","                relevant_pred_spans.append((label, sentence_ID, adjusted_span))\n","                print(f\"Adjusted Pred Span: {adjusted_span}\")\n","        print('\\n--')\n","        sentence_data = {\n","            \"document_name\": doc_name,\n","            \"sentence\": [formatted_sentence],\n","            \"sentence_indexed\": sentence,\n","            \"relevant_true_spans\": relevant_true_spans,\n","            \"relevant_pred_spans\": relevant_pred_spans\n","        }\n","\n","        processed_output.append(sentence_data)\n","        sentence_ID += 1\n","\n","    return processed_output\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gx3Eay7ywnKE"},"source":["Now that all the pre-requisites and prep work are done, we can load the pre-trained model using the cell below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-kvdpjkDxZp2"},"outputs":[],"source":["#Change depending on your config\n","\n","model_path = copy.copy(BERT_MODEL_PATH)\n","tokenizer_path = copy.copy(BERT_TOKENIZER_PATH)\n","\n","if BERT_MODEL_SELECTION == \"BERT_MODEL_BASE_CASED_6DOCS_OUT\" or BERT_MODEL_SELECTION == \"BERT_MODEL_BASE_UNCASED_6DOCS_OUT\" or BERT_MODEL_SELECTION == \"BERT_MODEL_DOC20_15_OUT\":\n","    model = BertForTokenClassification.from_pretrained(model_path)\n","    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n","\n","elif BERT_MODEL_SELECTION == \"RoBERTa_MODEL_6DOCS_OUT\":\n","    model = RobertaForTokenClassification.from_pretrained(model_path)\n","    tokenizer = RobertaTokenizer.from_pretrained(tokenizer_path)"]},{"cell_type":"markdown","metadata":{"id":"RMaaYDM_5vbf"},"source":["##2.2 PREDICTING WITH PRE-TRAINED BERT MODEL\n"]},{"cell_type":"markdown","metadata":{"id":"67fZyctUHPVS"},"source":["The following code takes the test data (flat_data variable, see section 2.1) and let's the model label the tokens. However, this is done only for Actor and Activity Data currently. Then a specialized processing function extracts the relevant information (like the label, sentence ID and token ID spans) from the evaluation of the model. **NOTE: THE PREDICTION HAPPENS HERE**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Om4Jfmui7V14","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722019393092,"user_tz":-120,"elapsed":5161,"user":{"displayName":"Charlotte Schneider","userId":"15797801204403852136"}},"outputId":"1e2f5481-f517-4eb7-b021-b22fcdf9c809"},"outputs":[{"output_type":"stream","name":"stdout","text":["Label                               Precision  Recall     F1         Support   \n","\n","O                                   0.0000     0.0000     0.0000     24.0000   \n","\n","Activity Data                       0.0000     0.0000     0.0000     11.0000   \n","\n","Further Specification               0.0000     0.0000     0.0000     1.0000    \n","\n","Actor                               0.0000     0.0000     0.0000     6.0000    \n","\n","Activity                            0.0000     0.0000     0.0000     14.0000   \n","\n","Sentence ID: 0, True Chunks: [('O', [0, 0]), ('Actor', [1, 2]), ('Activity', [3, 3]), ('Activity Data', [4, 7]), ('O', [8, 15])], Sharp Adjustments: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15}\n","True Chunk - Label: Actor, Span: [1, 2]\n","True Chunk - Label: Activity, Span: [3, 3]\n","True Chunk - Label: Activity Data, Span: [4, 7]\n","\n","--\n","Sentence ID: 1, True Chunks: [('O', [0, 0]), ('Actor', [1, 6]), ('O', [7, 17]), ('Activity', [18, 18]), ('Activity Data', [19, 20]), ('O', [21, 21]), ('Activity', [22, 22]), ('Activity', [23, 23]), ('Activity Data', [24, 27]), ('O', [28, 29])], Sharp Adjustments: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29}\n","True Chunk - Label: Actor, Span: [1, 6]\n","True Chunk - Label: Activity, Span: [18, 18]\n","True Chunk - Label: Activity Data, Span: [19, 20]\n","True Chunk - Label: Activity, Span: [22, 22]\n","True Chunk - Label: Activity, Span: [23, 23]\n","True Chunk - Label: Activity Data, Span: [24, 27]\n","\n","--\n","Sentence ID: 2, True Chunks: [('O', [0, 2]), ('Activity Data', [3, 10]), ('O', [11, 11]), ('Activity', [12, 12]), ('Activity', [13, 13]), ('O', [14, 14]), ('Actor', [15, 16]), ('O', [17, 18])], Sharp Adjustments: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18}\n","True Chunk - Label: Activity Data, Span: [3, 10]\n","True Chunk - Label: Activity, Span: [12, 12]\n","True Chunk - Label: Activity, Span: [13, 13]\n","True Chunk - Label: Actor, Span: [15, 16]\n","\n","--\n","Sentence ID: 3, True Chunks: [('O', [0, 0]), ('Activity Data', [1, 5]), ('O', [6, 6]), ('Activity', [7, 7]), ('O', [8, 8]), ('Actor', [9, 11]), ('Activity', [12, 12]), ('Activity Data', [13, 14]), ('Further Specification', [15, 20]), ('O', [21, 22])], Sharp Adjustments: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22}\n","True Chunk - Label: Activity Data, Span: [1, 5]\n","True Chunk - Label: Activity, Span: [7, 7]\n","True Chunk - Label: Actor, Span: [9, 11]\n","True Chunk - Label: Activity, Span: [12, 12]\n","True Chunk - Label: Activity Data, Span: [13, 14]\n","True Chunk - Label: Further Specification, Span: [15, 20]\n","\n","--\n","Sentence ID: 4, True Chunks: [('O', [0, 1]), ('Activity Data', [2, 3]), ('O', [4, 4]), ('Activity', [5, 5]), ('O', [6, 6]), ('Activity Data', [7, 11]), ('O', [12, 12]), ('Activity', [13, 13]), ('O', [14, 15])], Sharp Adjustments: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15}\n","True Chunk - Label: Activity Data, Span: [2, 3]\n","True Chunk - Label: Activity, Span: [5, 5]\n","True Chunk - Label: Activity Data, Span: [7, 11]\n","True Chunk - Label: Activity, Span: [13, 13]\n","\n","--\n","Sentence ID: 5, True Chunks: [('O', [0, 2]), ('Actor', [3, 4]), ('Activity', [5, 5]), ('Activity', [6, 6]), ('Activity Data', [7, 11]), ('O', [12, 13]), ('Activity', [14, 14]), ('Activity Data', [15, 20]), ('O', [21, 22])], Sharp Adjustments: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22}\n","True Chunk - Label: Actor, Span: [3, 4]\n","True Chunk - Label: Activity, Span: [5, 5]\n","True Chunk - Label: Activity, Span: [6, 6]\n","True Chunk - Label: Activity Data, Span: [7, 11]\n","True Chunk - Label: Activity, Span: [14, 14]\n","True Chunk - Label: Activity Data, Span: [15, 20]\n","\n","--\n","Sentence ID: 6, True Chunks: [('O', [0, 3]), ('Actor', [4, 8]), ('Activity', [9, 9]), ('Activity Data', [10, 14]), ('O', [15, 21])], Sharp Adjustments: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21}\n","True Chunk - Label: Actor, Span: [4, 8]\n","True Chunk - Label: Activity, Span: [9, 9]\n","True Chunk - Label: Activity Data, Span: [10, 14]\n","\n","--\n","\n","\n","These are the True Goldstandard Labels for visualisation\n","Out of a total of 0 true token labels, 0 were predicted\n"]}],"source":["# @title Prediction Code and Keeping a List\n","# Transform the data into a useable format\n","# So the next rerun is faster\n","flag_core_web_md = '/content/core_web_md'\n","\n","# Check if the flag file exists\n","if not os.path.exists(flag_core_web_md):\n","  nlp = spacy.load('en_core_web_md')\n","  # Create the flag file\n","  with open(flag_core_web_md, 'w') as f:\n","      f.write('Installed')\n","\n","val_loader = dataloader_internal(flat_data) #contains the document\n","# Prediction generation:\n","documents, doc_names, true_chunks_list, pred_chunks_list, true_labels_IOB, pred_labels_IOB = evaluate_model(model, val_loader, reverse_label_map,\n","                                                                          tokenizer)\n","# Process the evaluation output\n","\n","NER_processed_output = process_evaluation_output(documents, true_chunks_list, pred_chunks_list, doc_names)\n","\n","\n","print(\"\\n\")\n","\n","NER_prediction = {'true_data': [], 'pred_data': []}\n","\n","\n","for sentence_data in NER_processed_output:\n","  doc = nlp(sentence_data['sentence'][0])\n","  tokens = [token.text for token in doc]\n","\n","\n","  for chunk in sentence_data['relevant_true_spans']:\n","\n","    if chunk[2][0] != chunk[2][1]:\n","      word = ' '.join(tokens[chunk[2][0]:chunk[2][1]+1])\n","\n","    else:\n","      word = tokens[chunk[2][0]]\n","\n","    NER_prediction['true_data'].append({'tokens': word,\n","                           'label': chunk[0],\n","                           'sentence_id': chunk[1],\n","                           'span':chunk[2]\n","                           })\n","\n","  for chunk in sentence_data['relevant_pred_spans']:\n","\n","    if chunk[2][0] != chunk[2][1]:\n","      word = ' '.join(tokens[chunk[2][0]:chunk[2][1]+1])\n","\n","    else:\n","      word = tokens[chunk[2][0]]\n","\n","    NER_prediction['pred_data'].append({'tokens': word,\n","                            'label': chunk[0],\n","                            'sentence_id': chunk[1],\n","                            'span':chunk[2]\n","                            })\n","\n","# for visualisation\n","verbose = True\n","if verbose:\n","  print(\"These are the True Goldstandard Labels for visualisation\")\n","  for NER in NER_prediction['true_data']:\n","    print('\\n', NER)\n","  print(f\"Out of a total of {len(NER_prediction['true_data'])} true token labels, {len(NER_prediction['pred_data'])} were predicted\")\n"]},{"cell_type":"code","source":["print(\"documents: \",documents)\n","print(\"true_chunks_list: \",true_chunks_list)\n","print(\"pred_chunks_list: \",pred_chunks_list)\n","print(\"doc_names: \", doc_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6UMpxEtCpgJp","executionInfo":{"status":"ok","timestamp":1722019393092,"user_tz":-120,"elapsed":12,"user":{"displayName":"Charlotte Schneider","userId":"15797801204403852136"}},"outputId":"ad3f3050-9de2-4582-c6a6-9297b94614ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["documents:  [['[UNK]', 'The', 'party', 'sends', 'a', 'warrant', 'possession', 'request', 'asking', 'a', 'warrant', 'to', 'be', 'released', '.', '[UNK]'], ['[UNK]', 'The', 'C', '##lient', 'Service', 'Back', 'Office', 'as', 'part', 'of', 'the', 'Small', 'C', '##lai', '##ms', 'Regis', '##try', 'Operations', 'receives', 'the', 'request', 'and', 'retrieve', '##s', 'the', 'SC', '##T', 'file', '.', '[UNK]'], ['[UNK]', 'Then', ',', 'the', 'SC', '##T', 'War', '##rant', 'Po', '##sses', '##sion', 'is', 'forward', '##ed', 'to', 'Queensland', 'Police', '.', '[UNK]'], ['[UNK]', 'The', 'SC', '##T', 'physical', 'file', 'is', 'stored', 'by', 'the', 'Back', 'Office', 'awaiting', 'a', 'report', 'to', 'be', 'sent', 'by', 'the', 'Police', '.', '[UNK]'], ['[UNK]', 'When', 'the', 'report', 'is', 'received', ',', 'the', 'respective', 'SC', '##T', 'file', 'is', 'retrieved', '.', '[UNK]'], ['[UNK]', 'Then', ',', 'Back', 'Office', 'attach', '##es', 'the', 'new', 'SC', '##T', 'document', ',', 'and', 'stores', 'the', 'expanded', 'SC', '##T', 'physical', 'file', '.', '[UNK]'], ['[UNK]', 'After', 'that', ',', 'some', 'other', 'MC', 'internal', 'staff', 'receives', 'the', 'physical', 'SC', '##T', 'file', '(', 'out', 'of', 'scope', ')', '.', '[UNK]']]\n","true_chunks_list:  [[('O', [0, 0]), ('Actor', [1, 2]), ('Activity', [3, 3]), ('Activity Data', [4, 7]), ('O', [8, 15])], [('O', [0, 0]), ('Actor', [1, 6]), ('O', [7, 17]), ('Activity', [18, 18]), ('Activity Data', [19, 20]), ('O', [21, 21]), ('Activity', [22, 22]), ('Activity', [23, 23]), ('Activity Data', [24, 27]), ('O', [28, 29])], [('O', [0, 2]), ('Activity Data', [3, 10]), ('O', [11, 11]), ('Activity', [12, 12]), ('Activity', [13, 13]), ('O', [14, 14]), ('Actor', [15, 16]), ('O', [17, 18])], [('O', [0, 0]), ('Activity Data', [1, 5]), ('O', [6, 6]), ('Activity', [7, 7]), ('O', [8, 8]), ('Actor', [9, 11]), ('Activity', [12, 12]), ('Activity Data', [13, 14]), ('Further Specification', [15, 20]), ('O', [21, 22])], [('O', [0, 1]), ('Activity Data', [2, 3]), ('O', [4, 4]), ('Activity', [5, 5]), ('O', [6, 6]), ('Activity Data', [7, 11]), ('O', [12, 12]), ('Activity', [13, 13]), ('O', [14, 15])], [('O', [0, 2]), ('Actor', [3, 4]), ('Activity', [5, 5]), ('Activity', [6, 6]), ('Activity Data', [7, 11]), ('O', [12, 13]), ('Activity', [14, 14]), ('Activity Data', [15, 20]), ('O', [21, 22])], [('O', [0, 3]), ('Actor', [4, 8]), ('Activity', [9, 9]), ('Activity Data', [10, 14]), ('O', [15, 21])]]\n","pred_chunks_list:  [[('O', [0, 15])], [('O', [0, 29])], [('O', [0, 18])], [('O', [0, 22])], [('O', [0, 8]), ('O', [10, 15])], [('O', [0, 7]), ('O', [12, 22])], [('O', [0, 21])]]\n","doc_names:  ['doc-3.1', 'doc-3.1', 'doc-3.1', 'doc-3.1', 'doc-3.1', 'doc-3.1', 'doc-3.1']\n"]}]},{"cell_type":"code","source":["for sentence, label in zip(documents, true_labels_IOB):\n","  print(sentence)\n","  print(label)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8d1Yd0rBg84F","executionInfo":{"status":"ok","timestamp":1722019393092,"user_tz":-120,"elapsed":6,"user":{"displayName":"Charlotte Schneider","userId":"15797801204403852136"}},"outputId":"577592db-e26e-4331-c9a5-b011751542b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['[UNK]', 'The', 'party', 'sends', 'a', 'warrant', 'possession', 'request', 'asking', 'a', 'warrant', 'to', 'be', 'released', '.', '[UNK]']\n","[0, 1, 2, 3, 5, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0]\n","['[UNK]', 'The', 'C', '##lient', 'Service', 'Back', 'Office', 'as', 'part', 'of', 'the', 'Small', 'C', '##lai', '##ms', 'Regis', '##try', 'Operations', 'receives', 'the', 'request', 'and', 'retrieve', '##s', 'the', 'SC', '##T', 'file', '.', '[UNK]']\n","[0, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 5, 6, 0, 3, 3, 5, 6, 6, 6, 0, 0]\n","['[UNK]', 'Then', ',', 'the', 'SC', '##T', 'War', '##rant', 'Po', '##sses', '##sion', 'is', 'forward', '##ed', 'to', 'Queensland', 'Police', '.', '[UNK]']\n","[0, 0, 0, 5, 6, 6, 6, 6, 6, 6, 6, 0, 3, 3, 0, 1, 2, 0, 0]\n","['[UNK]', 'The', 'SC', '##T', 'physical', 'file', 'is', 'stored', 'by', 'the', 'Back', 'Office', 'awaiting', 'a', 'report', 'to', 'be', 'sent', 'by', 'the', 'Police', '.', '[UNK]']\n","[0, 5, 6, 6, 6, 6, 0, 3, 0, 1, 2, 2, 3, 5, 6, 7, 8, 8, 8, 8, 8, 0, 0]\n","['[UNK]', 'When', 'the', 'report', 'is', 'received', ',', 'the', 'respective', 'SC', '##T', 'file', 'is', 'retrieved', '.', '[UNK]']\n","[0, 0, 5, 6, 0, 3, 0, 5, 6, 6, 6, 6, 0, 3, 0, 0]\n","['[UNK]', 'Then', ',', 'Back', 'Office', 'attach', '##es', 'the', 'new', 'SC', '##T', 'document', ',', 'and', 'stores', 'the', 'expanded', 'SC', '##T', 'physical', 'file', '.', '[UNK]']\n","[0, 0, 0, 1, 2, 3, 3, 5, 6, 6, 6, 6, 0, 0, 3, 5, 6, 6, 6, 6, 6, 0, 0]\n","['[UNK]', 'After', 'that', ',', 'some', 'other', 'MC', 'internal', 'staff', 'receives', 'the', 'physical', 'SC', '##T', 'file', '(', 'out', 'of', 'scope', ')', '.', '[UNK]']\n","[0, 0, 0, 0, 1, 2, 2, 2, 2, 3, 5, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0]\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4lizFU3htW0v","colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"status":"error","timestamp":1722019393467,"user_tz":-120,"elapsed":378,"user":{"displayName":"Charlotte Schneider","userId":"15797801204403852136"}},"outputId":"bca72e2c-e92d-4720-b951-cd1dc17d1216"},"outputs":[{"output_type":"error","ename":"KeyError","evalue":"None","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-4023cec1eee3>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mnew_pred_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_pred_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mNER_TRUE_out\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mconvert_int_2string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_true_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse_label_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mNER_PRED_out\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mconvert_int_2string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_pred_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse_label_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mrow_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{:<10} {:<10}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-9d6f25488a6f>\u001b[0m in \u001b[0;36mconvert_int_2string\u001b[0;34m(nested_label_list, reverse_label_map)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnested_label_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mstring_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreverse_label_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-9d6f25488a6f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnested_label_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mstring_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreverse_label_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: None"]}],"source":["# @title Code to filter out BERT Tokenizer ##\n","\n","new_tokens = []\n","new_true_labels = []\n","new_pred_labels = []\n","\n","# In what follows we try to combine subtokens not containing 'Ġ' and filter their labels to only capture the main token label.\n","# We go through each sentence in the document\n","for sentence_tokens, sentence_true_labels, sentence_pred_labels in zip(documents, true_labels_IOB, pred_labels_IOB):\n","    filtered_tokens = []\n","    filtered_true_labels = []\n","    filtered_pred_labels = []\n","\n","    concatenated_token = \"\"\n","    main_label_true = None\n","    main_label_pred = None\n","\n","    # for each sentence we go through each token\n","    for i, (token, true_label, pred_label) in enumerate(zip(sentence_tokens, sentence_true_labels, sentence_pred_labels)):\n","        if token.startswith(\"Ġ\") or token.endswith('s>'):\n","            if concatenated_token:  # So if this value is not empty \"\"\n","                filtered_tokens.append(concatenated_token)\n","                filtered_true_labels.append(main_label_true)\n","                filtered_pred_labels.append(main_label_pred)\n","\n","            concatenated_token = token[1:]\n","            main_label_true = true_label\n","            main_label_pred = pred_label\n","        else:\n","            concatenated_token += token\n","\n","        # If the next token is the start of a new word or this is the last token, we add the current token as standalone token\n","        if i == len(sentence_tokens) - 1 or sentence_tokens[i + 1].startswith(\"Ġ\"):\n","            filtered_tokens.append(concatenated_token)\n","            filtered_true_labels.append(main_label_true)\n","            filtered_pred_labels.append(main_label_pred)\n","            concatenated_token = \"\"\n","\n","    new_tokens.append(filtered_tokens)\n","    new_true_labels.append(filtered_true_labels)\n","    new_pred_labels.append(filtered_pred_labels)\n","\n","NER_TRUE_out= convert_int_2string(new_true_labels, reverse_label_map)\n","NER_PRED_out= convert_int_2string(new_pred_labels, reverse_label_map)\n","row_format = \"{:<10} {:<10}\"\n"]},{"cell_type":"markdown","metadata":{"id":"VOpBSN0FQnMf"},"source":["#3. RE Task"]},{"cell_type":"markdown","metadata":{"id":"DpeTZJzejZVD"},"source":["##3.1.1 Loading Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8lksPh-4RHl7"},"outputs":[],"source":["# Create an instance of the CatBoostClassifier\n","best_model_neg_sample = CatBoostClassifier()\n","\n","# Load the model from the file\n","model_path = copy.copy(CATBOOST_MODEL_PATH)\n","best_model_neg_sample.load_model(model_path, format=\"cbm\")"]},{"cell_type":"markdown","metadata":{"id":"vzo5T1SW4Xp9"},"source":["##3.1.2 Loading Goldstandard RE data on testing document"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nRxy2HvF1ede"},"outputs":[],"source":["#@title Paths to goldstandard RE data\n","selection = {\n","    \"LESCHNEIDER\": 'https://drive.google.com/file/d/1C6OMDQYYh49WzK3ALSiCSAxKSwotqiV5/view?usp=share_link',\n","    \"PET\": 'https://drive.google.com/file/d/1H_Z12WlxOWL_9AWwhzU-EOgY7Yqj3858/view?usp=share_link'}\n","\n","# Download each link to the specified destination\n","for key, value in selection.items():\n","    destination = f'/content/{key}'\n","    download_content(value, destination)\n","\n","\n","LESCHEIDER_PATH = '/content/LESCHNEIDER'\n","PET_PATH = '/content/PET'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tUMaEgPG2v65"},"outputs":[],"source":["#@title Reading the files\n","\n","with open(LESCHEIDER_PATH, 'r') as file:\n","      data_LESCHNEIDER = json.load(file)\n","\n","with open(PET_PATH, 'r') as file:\n","      data_PET = json.load(file)\n","\n","totalRE = copy.deepcopy(data_LESCHNEIDER)\n","totalRE.extend(data_PET) # To add PET to LESCHNEIDER"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJ5zdAOf31vs"},"outputs":[],"source":["#@title Helper function to extract correct RE format\n","\n","def extract_entity_chunk_RE(start_sentence_id, start_token_id, df):\n","\n","    # because the first token of a chunk is uniquely identifyable by it's token and sentence ID in the df:\n","    start_token =  df[(df['Token_ID'] == start_token_id) & (df['Sentence_ID'] == start_sentence_id)].iloc[0]['Token']\n","\n","    # Locate the start token in the DataFrame\n","    target_row = df[(df['Token'] == start_token) &\n","                    (df['Token_ID'] == start_token_id) &\n","                    (df['Sentence_ID'] == start_sentence_id)]\n","\n","    if target_row.empty:\n","        return None\n","\n","    # Get the NER tag of the start token\n","    ner_tag = target_row.iloc[0]['NER'] #NER tag including B-tag\n","    entity_type = ner_tag.split('-')[1]  # Extract the entity type\n","\n","    # Initialize the entity chunk and token ID span\n","    entity_chunk = [] # Containing the whole string entity\n","    token_id_span = []\n","\n","    # Add the start token to the entity chunk\n","    index = target_row.index[0]\n","    entity_chunk.append(df.loc[index, 'Token']) #Containing the first word/token in string format\n","    token_id_span.append(df.loc[index, 'Token_ID'])\n","\n","    # Traverse forwards to collect the rest of the entity\n","    index += 1 # we start from index +1 since we already added the first index to both entity_chunk and token_id_span lists\n","    while index < len(df) and df.loc[index, 'NER'] == f'I-{entity_type}' and df.loc[index, 'Sentence_ID'] == start_sentence_id:\n","        entity_chunk.append(df.loc[index, 'Token'])\n","        token_id_span.append(df.loc[index, 'Token_ID'])\n","        index += 1\n","\n","    output = tuple([' '.join(entity_chunk), entity_type, start_sentence_id, (token_id_span[0], token_id_span[-1])])\n","\n","    return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nIxejQA14d5F"},"outputs":[],"source":["#@title Creating goldstandard RE format\n","# creating a intermediary variable that captures the testing document\n","testing_document = None\n","for document in totalRE: # totalRE is a list containing dictionaries where each represents a document, this is the goldstandard RE data.\n","\n","  if document['document name'] == testing_doc_name: # testing_doc_name was captured during selection of the testing document\n","    testing_document = copy.deepcopy(document)\n","    break # Because there will only be one match\n","\n","# Creating the DataFrame for easier lookup of specific token given sentence and token IDs. Contains goldstandard annotations for the testing doc\n","testing_document_df = pd.DataFrame({\n","    'Token': testing_document['tokens'],\n","    'Token_ID': testing_document['tokens-IDs'],\n","    'Sentence_ID': testing_document['sentence-IDs'],\n","    'NER': testing_document['ner_tags']\n","})\n","\n","# Putting Goldstandard RE information into format\n","goldstandard_relations= list()\n","for relation in testing_document['relations']:\n","\n","  # Given the Token ID and Sentence ID the extract_entity_chunk_RE function looks up the first word/token in the dataframe and returns it's chunk and correct RE format\n","  s_tuple = extract_entity_chunk_RE(relation['source-head-sentence-ID'], relation['source-head-word-ID'], testing_document_df)\n","  t_tuple = extract_entity_chunk_RE(relation['target-head-sentence-ID'], relation['target-head-word-ID'], testing_document_df)\n","\n","  goldstandard_relations.append({\n","      \"relation_type\": relation['relation-type'],\n","      \"source_chunk\": s_tuple,\n","      \"target_chunk\": t_tuple\n","  })"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eni7GUyE7u39"},"outputs":[],"source":["#@title Visualisation of the goldstandard_relations\n","print(f\"There are {len(goldstandard_relations)} goldstandard relations present in document {testing_document['document name']}\\n\")\n","\n","# For Visualisation:\n","for relation in goldstandard_relations:\n","  print('{')\n","  print('relation_type: ', relation['relation_type'])\n","  print('source_chunk: ', relation['source_chunk'])\n","  print('target_chunk: ', relation['target_chunk'])\n","  print('}\\n')"]},{"cell_type":"markdown","metadata":{"id":"-q7GMJIxkBmz"},"source":["##3.2 CREATING RE TEST DATA\n","\n","In this subsection, we will create the RE data using information from the NER step. So the previous code needs to be run as well. In addition, if you want to test using the predicted data you shoud set '''use_goldstandardNER''' to False."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2Ti_tQUv7gU"},"outputs":[],"source":["# @title Pre-Processing of NER Data into RE dictionary\n","# First we create a formatted relations dictionary from the existing NER data step.\n","doc_name = doc_names[0] #!use doc_name\n","\n","# Initialize an empty list to hold the tokens and Token IDs\n","tokens = []\n","token_IDs = []\n","sentence_IDs = []\n","sentence_ID = 0\n","# Iterate over each sublist in the documents list\n","for sentence in new_tokens:\n","    # Filter out empty strings and strings that are just spaces\n","    filtered_sentence = [word for word in sentence if not word.endswith('s>')]\n","    # Extend the tokens list with the filtered sublist\n","    tokens.extend(filtered_sentence)\n","    # Generate token IDs for the current sublist and append to token_IDs\n","    token_IDs.extend([i for i in range(len(filtered_sentence))])\n","    # Generate sentence IDs for the current sublist and extend the sentence_IDs list\n","    sentence_IDs.extend([sentence_ID] * len(filtered_sentence))\n","    # Increment the sentence ID for the next sublist\n","    sentence_ID += 1\n","\n","if use_goldstandardNER == True:\n","  # Initialize an empty list to hold the flattened NER tags\n","  NER_tags = []\n","  # Iterate over each sublist in the NER_tags_data list\n","  for sentence in NER_TRUE_out:\n","      # Remove the first and last element of each sublist because these tags denote white space and flatten\n","      if len(sentence) > 2:  # Ensure there are at least three elements to remove first and last\n","          NER_tags.extend(sentence[1:-1])\n","\n","else:\n","  # Initialize an empty list to hold the flattened NER tags\n","  NER_tags = []\n","  # Iterate over each sublist in the NER_tags_data list\n","  for sentence in NER_PRED_out:\n","      # Remove the first and last element of each sublist because these tags denote white space and flatten\n","      if len(sentence) > 2:  # Ensure there are at least three elements to remove first and last\n","          NER_tags.extend(sentence[1:-1])\n","\n","relational_data = {\n","    \"document name\": doc_names[0],\n","    \"tokens\": tokens,\n","    \"tokens-IDs\": token_IDs,\n","    \"ner_tags\": NER_tags,\n","    \"sentence-IDs\": sentence_IDs,\n","    \"relations\": []\n","}\n","\n","print(relational_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"27xtjrj0VT-A"},"outputs":[],"source":["# @title Data and Feature Generation code for Catboost\n","\n","def format_sentences(tokens, sentence_IDs):\n","    \"\"\"\n","    Concatenates and formats tokens based on their sentence IDs.\n","\n","    Parameters:\n","    tokens (list of str): The list of tokens.\n","    sentence_IDs (list of int): List indicating which sentence each token belongs to.\n","\n","    Returns:\n","    dict: A dictionary with sentence IDs as keys and formatted sentences as values.\n","    \"\"\"\n","    # Initialize a dictionary to hold sentence IDs as keys and their corresponding formatted sentences as values\n","    sentences = {}\n","\n","    # Set characters that should not be preceded by a space\n","    no_preceding_space_chars = {\"'\", \",\", \".\", \"s\", \";\", \"?\", \"!\", \":\", \"-\"}\n","\n","    for token, sentence_id in zip(tokens, sentence_IDs):\n","        # Initialize the sentence key in the dictionary if not already present\n","        if sentence_id not in sentences:\n","            sentences[sentence_id] = ''\n","\n","        # Clean token if it is a subword part that BERT might have split\n","        clean_token = token.replace('##', '')\n","\n","        # Determine if a space should be added\n","        should_add_space = True\n","        if clean_token in no_preceding_space_chars:  # Check against no_preceding_space_chars\n","            should_add_space = False\n","        if sentences[sentence_id] == '':  # Do not add space before the first token\n","            should_add_space = False\n","\n","        # Add space before the token if the condition is met\n","        if should_add_space:\n","            sentences[sentence_id] += ' ' + clean_token\n","        else:\n","            sentences[sentence_id] += clean_token\n","\n","    return sentences\n","\n","def get_entity_chunks(tokens, ner_tags, sentence_ids):\n","    chunks = []\n","    current_chunk = []\n","    start_index = None\n","    last_sentence_id = None  # Variable to track the sentence ID of the previous token\n","    sentence_start_index = 0  # Index where the current sentence starts in the tokens list\n","\n","    for i, (token, tag, sentence_id) in enumerate(zip(tokens, ner_tags, sentence_ids)):\n","        if sentence_id != last_sentence_id:\n","            # Reset start_index relative to the sentence when sentence_id changes\n","            sentence_start_index = i\n","            last_sentence_id = sentence_id\n","\n","        if tag.startswith('B-'):\n","            if current_chunk:\n","                # Append the current chunk before starting a new one\n","                chunks.append((' '.join(current_chunk), start_index, last_sentence_id))\n","                current_chunk = [token]\n","            else:\n","                current_chunk = [token]\n","            start_index = i - sentence_start_index  # Calculate start index relative to the start of the sentence\n","\n","        elif tag.startswith('I-') and current_chunk:\n","            current_chunk.append(token)\n","\n","        elif tag == 'O' and current_chunk:\n","            # Complete the current chunk if it exists\n","            chunks.append((' '.join(current_chunk), start_index, last_sentence_id))\n","            current_chunk = []\n","            start_index = None\n","\n","    if current_chunk:  # Add the last chunk if it exists\n","        chunks.append((' '.join(current_chunk), start_index, last_sentence_id))\n","\n","    return chunks\n","\n","def get_root_of_chunk(nlp, chunk_text):\n","    \"\"\"\n","    This function returns the root token of the chunk based on dependency parsing.\n","    \"\"\"\n","    doc = nlp(chunk_text)\n","    # Usually, the root token is the one whose head is outside the phrase itself or is itself\n","    for token in doc:\n","        if token.head == token or token.head not in doc:\n","            return token\n","    return doc[0]  # Fallback to the first token if no clear root is found\n","\n","def root_index_lookup(root, chunk, chunk_start_index, sentence, nlp):\n","    \"\"\"\n","        Find the index of the root word within the bounds set by the chunk's start index in a SpaCy document.\n","        The goal of this function is for us to efficiently trace the root word back to the original sentence\n","\n","        Parameters:\n","        sentence_doc (SpaCy Doc): The SpaCy Doc object containing the sentence tokens.\n","        root_word (str): The root word to search for within the chunk.\n","        chunk_start_index (int): The start index of the chunk within the sentence tokens.\n","        chunk (str): The text of the chunk as a string.\n","\n","        Returns:\n","        int: The index of the root word within the sentence if found, otherwise -1.\n","        \"\"\"\n","    # Calculate the number of tokens in the chunk\n","    chunk_length = len(nlp(chunk))\n","\n","    # Define the upper bound of the search\n","    end_index = chunk_start_index + chunk_length\n","\n","    for i in range(chunk_start_index, end_index-1):\n","\n","        if sentence[i].text == root.text:\n","            return i\n","\n","    return -1  # Return -1 if the root word is not found within the bounds\n","\n","def check_dependency_path(source_token, target_token):\n","    \"\"\"\n","    Trace the dependency path from source_token to target_token and classify the connection.\n","    Collect paths to root for both source and target, then identify the lowest common ancestor.\n","    \"\"\"\n","    source_path = []\n","    target_path = []\n","    current_token = source_token\n","\n","    # Trace path from source_token to root\n","    while current_token.head != current_token:\n","        source_path.append(current_token)\n","        current_token = current_token.head\n","    source_path.append(current_token)  # Include root\n","\n","    current_token = target_token\n","    # Trace path from target_token to root\n","    while current_token.head != current_token:\n","        target_path.append(current_token)\n","        current_token = current_token.head\n","    target_path.append(current_token)  # Include root\n","\n","    # Find lowest common ancestor\n","    set_source = set(source_path)\n","    common_ancestors = [token for token in target_path if token in set_source]\n","    if common_ancestors:\n","        # Return the path from source to LCA and LCA to target\n","        lca = common_ancestors[0]\n","        source_to_lca = source_path[:source_path.index(lca)+1]\n","        lca_to_target = target_path[:target_path.index(lca)+1]\n","        return [token.dep_ for token in source_to_lca + lca_to_target[::-1]]\n","    return []\n","\n","def analyze_chunk_dependency(sentence_text, source_phrase, target_phrase, nlp):\n","\n","    doc = nlp(sentence_text)\n","\n","    #Since the chunks are sometimes too big and DET are often not relevant we try to pinpoint the root word\n","    #For example in the source_phrase \"The MPON\" we see that MPON is more relevant than The\n","\n","    source_root_token = get_root_of_chunk(nlp, source_phrase[0])\n","    target_root_token = get_root_of_chunk(nlp, target_phrase[0])\n","\n","    # Directly match root tokens based on text and position\n","\n","    source_tokens = [token for token in doc if token.text == source_root_token.text and token.i == root_index_lookup(source_root_token,source_phrase[0], source_phrase[1], doc, nlp)]\n","    target_tokens = [token for token in doc if token.text == target_root_token.text and token.i == root_index_lookup(target_root_token,target_phrase[0], target_phrase[1], doc, nlp)]\n","\n","    if not source_tokens or not target_tokens:\n","        return [\"/\"]  # Return \"/\" indicating no tokens found\n","\n","    all_dependencies = set()  # Use a set to avoid duplicate entries\n","    dependency_found = False  # Reintroducing the boolean to track if any dependency was found\n","\n","    for s_token in source_tokens:\n","        for t_token in target_tokens:\n","            deps = check_dependency_path(s_token, t_token)\n","            if deps:\n","                all_dependencies.update(deps)\n","                dependency_found = True  # Set to True if any dependency is found\n","\n","\n","    if not dependency_found:\n","        #print(\"NO DEPENDENCY FOUND\")\n","        return [\"/\"]  # Return \"/\" if no dependencies were found\n","\n","    return list(all_dependencies)\n","\n","def encode_dependency_path(dependency_path, label_encoder):\n","    \"\"\"\n","    Encodes a single dependency path using a pre-fitted LabelEncoder.\n","\n","    Arguments:\n","    dependency_path (list): The list of dependency tags forming the path.\n","    label_encoder (LabelEncoder): A pre-fitted LabelEncoder with all possible dependency tags.\n","\n","    Returns:\n","    list: A list of integers representing the encoded dependency path.\n","    \"\"\"\n","    encoded_path = []\n","    if not dependency_path:\n","        return -1\n","    else:\n","        for tag in dependency_path:\n","            try:\n","                # Normalize the tag to lower case and encode it\n","                encoded_tag = label_encoder.transform([tag.lower()])[0]\n","            except ValueError:\n","                # If the tag is unknown, assign a default value of -1\n","                encoded_tag = -1\n","            encoded_path.append(encoded_tag)\n","\n","        return encoded_path\n","\n","def initialize_label_encoder():\n","    \"\"\"\n","    Prepares and returns a pre-fitted LabelEncoder based on a predefined list of dependency tags.\n","    \"\"\"\n","    possible_tags = ['acl', 'advcl', 'advmod', 'amod', 'appos', 'attr', 'aux', 'auxpass',\n","    'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass',\n","    'dative', 'dep', 'det', 'discourse', 'dislocated', 'dobj', 'expl',\n","    'fixed', 'flat', 'goeswith', 'iobj', 'intj', 'list', 'mark', 'meta',\n","    'neg', 'nounmod', 'npmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd',\n","    'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep',\n","    'prt', 'punct', 'quantmod', 'relcl', 'root', 'xcomp', 'npadvmod',\n","    'complm', 'infmod', 'partmod', 'hmod', 'hyph', 'num', 'number',\n","    'nmod', 'nn', 'npadvmod', 'possessive', 'rcmod', '/']\n","\n","    label_encoder = LabelEncoder()\n","    label_encoder.fit(possible_tags)\n","    return label_encoder\n","\n","def find_neighboring_tags(entity_idx, ner_tags, direction='prev'):\n","    step = -1 if direction == 'prev' else 1\n","    start, end = (entity_idx - 1, -1) if direction == 'prev' else (entity_idx + 1, len(ner_tags))\n","    for i in range(start, end, step):\n","        if i >= 0 and i < len(ner_tags) and (ner_tags[i].startswith('B-') or ner_tags[i] == 'O'):\n","            return ner_tags[i]\n","    return 'NONE'\n","\n","def create_df(data_in, nlp_in):\n","\n","    data = data_in\n","\n","    nlp = nlp_in\n","    #nlp = spacy.load(\"en_core_web_trf\")  # takes forever, performs comparibly to en_core_web_md\n","    label_encoder = initialize_label_encoder() #To translate the paths into numbers for easier processing\n","\n","    transformed_data = []\n","\n","    tokens = data['tokens']\n","    ner_tags = data['ner_tags']\n","    sentence_ids = data['sentence-IDs']\n","    doc_name = data['document name']\n","    sentences = format_sentences(tokens, sentence_ids)\n","    # Generate POS tags for the tokens\n","    pos_tags = pos_tag(tokens)\n","\n","\n","    document_chunks = get_entity_chunks(data['tokens'], data['ner_tags'], data['sentence-IDs'])\n","\n","    # Initialize entities list using comprehensive condition checks for 'B-' prefixes\n","    entities = {\n","        (sentence_id, token_id): {\n","            'token': tokens[idx],\n","            'type': ner_tags[idx],\n","            'sentence_id': sentence_id,\n","            'pos_tag': pos_tags[idx][1],\n","            'token_id': token_id,\n","            'index': idx\n","        }\n","        for idx, (token_id, sentence_id) in enumerate(zip(data['tokens-IDs'], sentence_ids))\n","        if ner_tags[idx].startswith('B-')\n","    }\n","\n","    relations_dict = {\n","        (doc_name, rel['source-head-sentence-ID'], rel['source-head-word-ID'], rel['target-head-sentence-ID'],\n","          rel['target-head-word-ID']): rel['relation-type']\n","        for rel in data['relations']\n","    }\n","\n","    # Generate all combinations of entities and check for relations within the same sentence\n","    for ((src_sentence_id, src_token_id), source), ((tgt_sentence_id, tgt_token_id), target) in product(\n","            entities.items(), repeat=2):\n","        if (src_sentence_id, src_token_id) != (tgt_sentence_id, tgt_token_id):  # Explicitly prevent self-comparison\n","            if src_sentence_id == tgt_sentence_id:\n","\n","\n","                sentence = sentences[src_sentence_id]\n","\n","                # Extract full chunks for the source and target using their token_ids and sentence_ids.\n","                # It saves a tuple where the first element is the actual text and the second the start index position relative to the sentence.\n","                source_chunk = next(((chunk[0], chunk[1]) for chunk in document_chunks\n","                                      if chunk[1] <= src_token_id and chunk[2] == src_sentence_id and src_token_id <\n","                                      chunk[1] + len(chunk[0].split())),\n","                                    (source['token'], src_token_id))\n","\n","                target_chunk = next(((chunk[0], chunk[1]) for chunk in document_chunks\n","                                      if chunk[1] <= tgt_token_id and chunk[2] == tgt_sentence_id and tgt_token_id <\n","                                      chunk[1] + len(chunk[0].split())), (target['token'], tgt_token_id))\n","\n","\n","                results = analyze_chunk_dependency(sentence, source_chunk, target_chunk, nlp)\n","                results = encode_dependency_path(results, label_encoder)\n","            else:\n","                results = [\"/\"]\n","                results = encode_dependency_path(results, label_encoder)\n","\n","            relation_key = (doc_name, src_sentence_id, src_token_id, tgt_sentence_id, tgt_token_id)\n","            relation_type = relations_dict.get(relation_key, \"no_relation\")\n","\n","            # Get neighboring B-tags or 'O' for source and target\n","            src_prev_tag = find_neighboring_tags(source['index'], ner_tags, 'prev')\n","            src_next_tag = find_neighboring_tags(source['index'], ner_tags, 'next')\n","            tgt_prev_tag = find_neighboring_tags(target['index'], ner_tags, 'prev')\n","            tgt_next_tag = find_neighboring_tags(target['index'], ner_tags, 'next')\n","\n","            row = {\n","                'document_name': doc_name,\n","                'source_token': source['token'],\n","                'source_type': source['type'],\n","                'source_pos_tag': source['pos_tag'],  # Include source POS tag\n","                'source_sentence_ID': src_sentence_id,\n","                'source_token_ID': src_token_id,\n","                'source_prev_tag': src_prev_tag,\n","                'source_next_tag': src_next_tag,\n","                'target_token': target['token'],\n","                'target_type': target['type'],\n","                'target_pos_tag': target['pos_tag'],  # Include target POS tag\n","                'target_sentence_ID': tgt_sentence_id,\n","                'target_token_ID': tgt_token_id,\n","                'target_prev_tag': tgt_prev_tag,\n","                'target_next_tag': tgt_next_tag,\n","                'token_distance': abs(src_token_id - tgt_token_id),\n","                'sentence_distance': abs(src_sentence_id - tgt_sentence_id),\n","                'dependency_tags': results,\n","                'relation_type': relation_type\n","            }\n","            transformed_data.append(row)\n","\n","    df_relations = pd.DataFrame(transformed_data)\n","    print(\"\\n---------------------------------\")\n","    print(\"Feature Generation Done!\")\n","    print(\"---------------------------------\\n\")\n","    filtered_df = df_relations[['source_token', 'target_token', 'dependency_tags', 'relation_type']]\n","    filtered_df = filtered_df[filtered_df['dependency_tags'].apply(lambda x: x != [0])]\n","    filtered_df_again = filtered_df[filtered_df['relation_type'] != \"no_relation\"]\n","    df_true_relations = df_relations[df_relations['relation_type'] != \"no_relation\"]\n","\n","    pd.set_option('display.max_rows', 500)\n","    pd.set_option('display.max_columns', 5)# Example: 500 rows\n","\n","    print(f\"total number of elements in dataframe: {len(df_relations)}\")\n","    \"\"\"print(f\"total number of non empty dependency rows in dataframe: {len(filtered_df)}\")\n","    print(f\"total number of non empty dependency rows WITH RELATION in dataframe: {len(filtered_df_again)}\")\n","    print(f\"total number of non empty relation rows in dataframe: {len(df_true_relations)}\")\"\"\"\n","    print(\"\\n\")\n","\n","    # Convert the 'dependency_tags' column from lists to strings\n","    df_relations['dependency_tags'] = df_relations['dependency_tags'].apply(lambda x: ' '.join(map(str, x)) if isinstance(x, list) else x)\n","\n","    return df_relations\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gPTokwxMV_Uw"},"outputs":[],"source":["# Create a DataFrame\n","pre_processing_data = copy.deepcopy(relational_data) # for clarity\n","df_relations = create_df(pre_processing_data, nlp)\n","\n","try:\n","  data_label = df_relations.relation_type #will all have default 'no relation' value\n","  df_relations.drop('relation_type', axis=1, inplace=True)\n","\n","except KeyError:\n","    print(\"The column 'relation_type' does not exist in the DataFrame.\")\n","print(f\"These are the heads of the table \\n{df_relations.head(5)}\")\n","# only for saving\n","#df_relations.to_csv(\"/content/drive/MyDrive/THESIS/CODING/NAM_TESTING/TESTING DATA/RE_TRAINING_DATA/Misc Testing/PIPELINE_TESTING_CATBOOST_INPUT.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"9Ge3Ts9JZe_j"},"source":["##3.3 PREDICTING WITH PRE-TRAINED CATBOOST MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iACgwWbyZrV1"},"outputs":[],"source":["# @title Prediction Code\n","# Preparing input data and defining categorical data\n","categorical_features_indices = [i for i, typ in enumerate(df_relations.dtypes) if typ == 'object' or pd.api.types.is_categorical_dtype(df_relations.iloc[:, i])]\n","data_pool = Pool(df_relations, cat_features=categorical_features_indices)\n","\n","# Prediction happens here\n","predictions = best_model_neg_sample.predict(data_pool)\n","\n","# Convert numpy array to DataFrame\n","df_array = pd.DataFrame(predictions, columns=['relation_type'])\n","\n","# Concatenate DataFrames along columns (axis=1)\n","result_df = pd.concat([df_relations, df_array], axis=1)\n","\n","# Filter out desired columns\n","selected_columns = result_df.filter(['source_token', 'source_sentence_ID','source_token_ID','source_type', 'target_token', 'target_sentence_ID','target_token_ID','target_type', 'relation_type'])\n","\n","pd.set_option('display.width', 1000)\n","pd.set_option('display.max_columns', 10)\n","\n","relevant_rows = selected_columns[selected_columns['relation_type'] != 'no_relation']\n","print(relevant_rows)\n","print(f\"\\n{relevant_rows.shape[0]} Relations were predicted by Catboost\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"6ZEZjjsr_lk7"},"outputs":[],"source":["# @title Converting Predictions Into a Modelling Format\n","\n","# This function will look op the start word of a chunk and provide the whole text of that chunk as well as the span indices.\n","# This function uses the NER predictions in order to obtain the source and target labels\n","def lookup_tuple(token, sentence_id, token_id, goldstandard_check):\n","\n","  lookup = list()\n","\n","  Hit = False\n","  if goldstandard_check:\n","    checklist = NER_prediction['true_data']\n","\n","  else:\n","    checklist = NER_prediction['pred_data']\n","\n","  for chunk in checklist:\n","\n","    # we can say token_id == chunk['span'][0] because the token we are trying to look up will always be te first token of a text chunk\n","    if chunk['sentence_id'] == sentence_id and token_id == chunk['span'][0] and token in chunk['tokens'].split():\n","      lookup.append(chunk)\n","      Hit = True\n","\n","  if len(lookup) > 1: # we return nothing because there are duplicates and thus invalid, this should never happen\n","    return _, Hit\n","\n","  if len(lookup) == 0:\n","    return _, Hit\n","\n","  else:\n","    return lookup[0], Hit\n","\n","# Extracting RE data into format, using the labels created by NER\n","predicted_relations = list()\n","for i, row in relevant_rows.iterrows():\n","  source,_ = lookup_tuple(row['source_token'], row['source_sentence_ID'], row['source_token_ID'], use_goldstandardNER)\n","  target,_ = lookup_tuple(row['target_token'], row['target_sentence_ID'], row['target_token_ID'], use_goldstandardNER)\n","  s_tuple = tuple(source.values())\n","  t_tuple = tuple(target.values())\n","\n","  predicted_relations.append({\n","      \"relation_type\": row['relation_type'],\n","      \"source_chunk\": s_tuple,\n","      \"target_chunk\": t_tuple\n","  })\n","\n","# For Visualisation:\n","for relation in predicted_relations:\n","  print('{')\n","  print('relation_type: ', relation['relation_type'])\n","  print('source_chunk: ', relation['source_chunk'])\n","  print('target_chunk: ', relation['target_chunk'])\n","  print('}\\n')\n"]},{"cell_type":"markdown","metadata":{"id":"-A3FUx2EXcO3"},"source":["#4 Entity Resolution Task"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IWhKYw8pYorA"},"outputs":[],"source":["# @title Extracting Clusters\n","sentences = ' '.join(word for sentence_data in NER_processed_output for word in sentence_data['sentence']) # from the BERT processed output\n","\n","flag_coreference_trf = '/content/coreference_trf'\n","\n","# Check if the flag file exists\n","if not os.path.exists(flag_coreference_trf):\n","  nlp_coref = spacy.load(\"en_coreference_web_trf\")\n","  # Create the flag file\n","  with open(flag_coreference_trf, 'w') as f:\n","      f.write('Installed')\n","\n","if use_goldstandardRE:\n","  chunking_RE_df = copy.deepcopy(testing_document_df) # use the goldstandard data\n","else:\n","  chunking_RE_df = pd.DataFrame({\n","    'Token': relational_data['tokens'],\n","    'Token_ID': relational_data['tokens-IDs'],\n","    'Sentence_ID': relational_data['sentence-IDs'],\n","    'NER': relational_data['ner_tags']}) # relational_data is the data after NER before it has been put into catboost. So the predicted NER\n","\n","doc = nlp_coref(sentences)\n","\n","labels2resolve = ['Actor', 'Activity Data']\n","\n","# Process Coreference Clusters\n","clusters = list()\n","clusters_RE = list()\n","for cluster_id, cluster in enumerate(doc.spans.values()):\n","    cluster_dict = {\"cluster_id\": cluster_id + 1, \"mentions\": []}\n","    cluster_dict_RE = {\"cluster_id\": cluster_id + 1, \"mentions\": []}\n","\n","    for mention in cluster:\n","        # Find the sentence ID\n","        sent_id = next(i for i, sent in enumerate(doc.sents) if mention.start >= sent.start and mention.end <= sent.end)\n","        sent_start = list(doc.sents)[sent_id].start\n","        span_start = mention.start - sent_start\n","        span_end = mention.end - sent_start - 1\n","\n","        # Extract the mention text and label\n","        value, hit= lookup_tuple(mention[0].text, sent_id, span_start, use_goldstandardNER) # Looks up data based on NER predictions/ Goldstandard NER\n","        if hit and value['label'] in labels2resolve: # We only keep those mentions that have a NER tag extracted. For those who don't we do not add it to the cluster\n","\n","            get = extract_entity_chunk_RE(sent_id,span_start, chunking_RE_df)\n","            cluster_dict[\"mentions\"].append(tuple(value.values()))\n","            cluster_dict_RE[\"mentions\"].append(get)\n","\n","\n","    clusters.append(cluster_dict)\n","    clusters_RE.append(cluster_dict_RE)\n","\n","\n","filtered_clusters = list()\n","clusters = [cluster for cluster in clusters if len(cluster['mentions']) >= 2] # Because some clusters can have only 1 mention, this is because we only take mentions that have a NER tag.\n","clusters_RE = [cluster for cluster in clusters_RE if len(cluster['mentions']) >= 2] # Because some clusters can have only 1 mention, this is because we only take mentions that have a specific NER tag.\n","for cluster in clusters:\n","    cluster['mentions'] = sorted(cluster['mentions'], key=lambda x: len(x[0]), reverse=True) # This sorts the mentions according to string length from largest to smallest (we assume that the most complete entity is the longest one)\n","for cluster in clusters_RE:\n","    cluster['mentions'] = sorted(cluster['mentions'], key=lambda x: len(x[0]), reverse=True) # This sorts the mentions according to string length from largest to smallest (we assume that the most complete entity is the longest one)\n","\n","# Following lines are for re-assigning cluster IDs and for visualisation:\n","if use_goldstandardRE:\n","  clusters_original = clusters_RE #FOR DOCUMENTATION\n","\n","  for id, cluster in enumerate(clusters_RE, start=1):\n","      cluster['cluster_id'] = id\n","      print(f\"\\nCluster ID {cluster['cluster_id']}:\\n\")\n","\n","      for idx, mention in enumerate(cluster['mentions'], start=1):  # Corrected loop\n","          print(f\"Mention ID {idx}: {mention}\")\n","else:\n","  clusters_original = clusters #FOR DOCUMENTATION\n","  for id, cluster in enumerate(clusters, start=1):\n","      cluster['cluster_id'] = id\n","      print(f\"\\nCluster ID {cluster['cluster_id']}:\\n\")\n","      for idx, mention in enumerate(cluster['mentions'], start=1):  # Corrected loop\n","          print(f\"Mention ID {idx}: {mention}\")\n","\n"]},{"cell_type":"code","source":["if use_goldstandardRE:\n","  valid_clusters= copy.deepcopy(clusters_RE)\n","else:\n","  valid_clusters = copy.deepcopy(clusters)"],"metadata":{"id":"1wa9-VC3SliK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Do you wish to delete any of these (incorrect) mentions/clusters?\n","#@markdown Please enter the cluster ID to change/view (as number), leave 0 to skip\n","\n","CLUSTER_SELECTION = 0 #@param {type:\"integer\"}\n","\n","if CLUSTER_SELECTION != 0:\n","  selected_cluster = next((cluster for cluster in valid_clusters if cluster['cluster_id'] == CLUSTER_SELECTION), None)\n","  if selected_cluster:\n","      print(f\"Selected Cluster ID: {selected_cluster['cluster_id']}\")\n","\n","      #@markdown Please enter the mention ID to change/view (as number), leave 0 to skip\n","\n","      MENTION_SELECTION = 0 #@param {type:\"integer\"}\n","      if MENTION_SELECTION > len(selected_cluster['mentions']):\n","        print(\"Selected Mention ID: \")\n","        print(\"Your selection was invalid, please try again.\")\n","\n","      elif MENTION_SELECTION == 0:\n","          print(\"No Mention ID selected.\")\n","\n","      elif MENTION_SELECTION != 0 and selected_cluster:\n","        for idx, mention in enumerate(selected_cluster['mentions'], start=1):\n","          if MENTION_SELECTION == idx:\n","            print(f\"Selected Mention ID {idx}: {mention}\")\n","\n","  else:\n","      print(\"Cluster not found.\")\n","else:\n","    selected_cluster = None\n","    print(\"No modifications needed.\")\n","\n","valid_clusters = [cluster for cluster in valid_clusters if len(cluster['mentions']) >= 2] # Because some clusters can have only 1 mention, this is because we only take mentions that have a NER tag.\n","\n"],"metadata":{"id":"Ab7XViirro90"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Follow up\n","#@markdown Do you confirm to delete the selected mention from the cluster?\n","answer = \"NO\" #@param [\"YES\", \"NO\"] {type:\"string\"}\n","\n","if answer == \"YES\":\n","  mention_to_remove = None\n","  if selected_cluster != None:\n","    for idx, mention in enumerate(selected_cluster['mentions'], start=1):\n","      if MENTION_SELECTION == idx:\n","        mention_to_remove = mention\n","        break\n","    selected_cluster['mentions'].remove(mention_to_remove)\n","    print(f\"\\nRemoved mention: {mention_to_remove}\")\n","    print('--------')\n","    print(f\"These are the updated clusters:\")\n","    for cluster in valid_clusters:\n","      print(f\"\\nCluster ID {cluster['cluster_id']}:\\n\")\n","      for idx, mention in enumerate(cluster['mentions'], start=1):  # Corrected loop\n","          print(f\"Mention ID {idx}: {mention}\")\n","  else:\n","    print(\"No cluster selected.\")\n","else:\n","    print(\"\\n=> Deletion cancelled.\")"],"metadata":{"id":"LdhRq4tBblSY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ZmvP25y2wvl"},"outputs":[],"source":["#@title Resolving NER data using ER data\n","if use_goldstandardNER:\n","  resolved_NER = copy.deepcopy(NER_prediction['true_data'])\n","else:\n","  resolved_NER = copy.deepcopy(NER_prediction['pred_data']) # for clarity\n","\n","target_labels = ['Actor', 'Activity Data']\n","count =0\n","\n","for entry in resolved_NER:\n","  for cluster in valid_clusters:\n","\n","    if tuple([entry['tokens'], entry['label'], entry['sentence_id'], entry['span']]) in cluster['mentions'] and entry['label'] in target_labels:\n","\n","      #print(f\"HIT at {entry}\")\n","      entry['tokens'] = cluster['mentions'][0][0] # aka the string of the first mention\n","      count += 1\n","\n","print(f\"{count} NER entries were resolved\\n\")\n","\n","# for visualisation\n","for NER in resolved_NER:\n","  print('\\n', NER)\n","\n","#print(resolved_NER)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xnnDe6XUwAGz"},"outputs":[],"source":["# @title Resolving RE data using ER data\n","\n","target_labels = ['Actor', 'Activity Data']\n","count = 0\n","\n","if use_goldstandardRE:\n","  resolved_RE = copy.deepcopy(goldstandard_relations) # for clarity\n","  for entry in resolved_RE:\n","    for cluster in valid_clusters: # because use_goldstandardRE is already checked when making valid clusters\n","\n","      if entry['source_chunk'] in cluster['mentions']:\n","        entry['source_chunk'] = tuple([cluster['mentions'][0][0], entry['source_chunk'][1], entry['source_chunk'][2], entry['source_chunk'][3]])\n","        count += 1\n","\n","      if entry['target_chunk'] in cluster['mentions']:\n","        entry['target_chunk'] = tuple([cluster['mentions'][0][0], entry['target_chunk'][1], entry['target_chunk'][2], entry['target_chunk'][3]])\n","        count += 1\n","\n","  print(f\"{count} goldstandard relations were resolved\\n\")\n","\n","else:\n","  resolved_RE = copy.deepcopy(predicted_relations) # for clarity\n","  for entry in resolved_RE:\n","    for cluster in valid_clusters:\n","\n","      if entry['source_chunk'] in cluster['mentions']:\n","        entry['source_chunk'] = tuple([cluster['mentions'][0][0], entry['source_chunk'][1], entry['source_chunk'][2], entry['source_chunk'][3]])\n","        count += 1\n","\n","      if entry['target_chunk'] in cluster['mentions']:\n","        entry['target_chunk'] = tuple([cluster['mentions'][0][0], entry['target_chunk'][1], entry['target_chunk'][2], entry['target_chunk'][3]])\n","        count += 1\n","\n","  print(f\"{count} predicted relations were resolved\\n\")\n","\n","\n","# for visualisation\n","for entry in resolved_RE:\n","  if entry['relation_type'] == 'flow':\n","\n","    print('{')\n","    print('relation_type: ', entry['relation_type'])\n","    print('source_chunk: ', entry['source_chunk'])\n","    print('target_chunk: ', entry['target_chunk'])\n","    print('}\\n')\n","\n","print(resolved_RE)"]},{"cell_type":"markdown","metadata":{"id":"VbNKMG1g-d8H"},"source":["#5 BPMN Modelling"]},{"cell_type":"markdown","metadata":{"id":"UARzO0uwRDB3"},"source":["In this step we will generate a BPMN diagram from the relations.\n","We also include the entities where no relation was found for completeness such that the user can still add nodes and draw edges where necessary."]},{"cell_type":"markdown","metadata":{"id":"YXGCz51wxVIM"},"source":["##5.1 Preprocessing Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJysrzZwRCh8"},"outputs":[],"source":["#@title Transform resolved relations to format\n","#First we transform the relations to the correct format, every string in the 'tokens' section should be lowercased if it isnt already.\n","\n","#check the correctness please => Checked, Nam\n","\n","lowercase_relations = []\n","\n","for relation in resolved_RE:\n","    source_chunk = relation['source_chunk']\n","    target_chunk = relation['target_chunk']\n","\n","    # Convert the tokens in the source_chunk and target_chunk to lowercase\n","    new_source_chunk = (source_chunk[0].lower(), source_chunk[1], source_chunk[2], source_chunk[3])\n","    new_target_chunk = (target_chunk[0].lower(), target_chunk[1], target_chunk[2], target_chunk[3])\n","\n","    # Create a new relation dictionary with the modified chunks\n","    new_relation = {\n","        \"relation_type\": relation['relation_type'],\n","        \"source_chunk\": new_source_chunk,\n","        \"target_chunk\": new_target_chunk\n","    }\n","\n","    lowercase_relations.append(new_relation)\n","\n","sorted_relations = sorted(lowercase_relations, key=lambda x: x['relation_type'] != 'same gateway')\n","\n","BPMN_relations = copy.deepcopy(sorted_relations) #replace  the original relations with the sorted lowercased ones\n","for relation in BPMN_relations:\n","  print('{')\n","  print('relation_type: ', relation['relation_type'])\n","  print('source_chunk: ', relation['source_chunk'])\n","  print('target_chunk: ', relation['target_chunk'])\n","  print('}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bHd3Q4_R6ra"},"outputs":[],"source":["#@title Transform resolved NER entities to format\n","\n","mention_chunks_no_edit = copy.deepcopy(resolved_NER) # for clarity and data protection\n","mention_chunks = []\n","\n","for mention in mention_chunks_no_edit:\n","    token = mention['tokens']\n","    element_type = mention['label']  # or another relevant type if needed\n","    sentence_id = mention['sentence_id']\n","    tokenspan = mention['span']\n","    new_chunk = (token, element_type, sentence_id, tokenspan)\n","    mention_chunks.append(new_chunk)\n","\n","print(mention_chunks)"]},{"cell_type":"markdown","metadata":{"id":"bDXEjFl-xZtf"},"source":["##5.2 Model Generation"]},{"cell_type":"markdown","metadata":{"id":"RhOp39t6SRMi"},"source":["Define node styles and colors depending on the NER type.\n","Initiate all necessary functions.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l5PnMIGPSQzg"},"outputs":[],"source":["#@title Node styles and function initiations\n","\n","# Define node and edge styles\n","node_styles = {\n","    'Activity': 'box',\n","    'XOR Gateway': 'diamond',\n","    'AND Gateway': 'diamond',\n","    'Condition Specification': 'note',\n","    'Actor': 'ellipse',\n","    'Activity Data': 'note',\n","    'Further Specification': 'note',\n","    'Start Event': 'circle',\n","    'End Event': 'doublecircle'\n","}\n","\n","node_colors = {\n","    'Activity': 'black',\n","    'XOR Gateway': 'black',\n","    'AND Gateway': 'black',\n","    'Condition Specification': 'black',\n","    'Actor': 'darkgrey',\n","    'Activity Data': 'darkgrey',\n","    'Further Specification': 'darkgrey'\n","}\n","\n","\n","# Function to add nodes\n","def add_node(chunk):\n","    label, element_type, sent_id, tokenspan = chunk\n","    if element_type == 'Actor':\n","        if label not in actors:\n","            shape = node_styles.get(element_type, 'box')\n","            color = node_colors.get(element_type, 'black')\n","            dot.node(label, label, shape=shape, color=color)\n","            actors.append(label)\n","        \"\"\"else:\n","          print(f\"actor: {label} is already added\")\"\"\"\n","\n","    elif element_type == 'Activity Data':\n","        if label not in activity_datas:\n","            shape = node_styles.get(element_type, 'box')\n","            color = node_colors.get(element_type, 'black')\n","            dot.node(label, label, shape=shape, color=color)\n","            activity_datas.append(label)\n","        \"\"\"else:\n","          print(f\"activity data: {label} is already added\")\"\"\"\n","\n","    elif element_type == 'XOR Gateway':\n","        display_label = 'X'\n","        if chunk not in XOR_gateways:\n","            shape = node_styles.get(element_type, 'box')\n","            color = node_colors.get(element_type, 'black')\n","            new_label = label + '_' + str(sent_id) + '_' + str(tokenspan[0])\n","            dot.node(new_label, display_label, shape=shape, color=color)\n","            XOR_gateways.append(chunk)\n","        \"\"\"else:\n","            print(f\"xor gateway: {label} is already added\")\"\"\"\n","\n","\n","    elif element_type == 'AND Gateway':\n","        display_label = '+'\n","        if chunk not in AND_gateways:\n","            shape = node_styles.get(element_type, 'box')\n","            color = node_colors.get(element_type, 'black')\n","            new_label = label + '_' + str(sent_id) + '_' + str(tokenspan[0])\n","            dot.node(new_label, display_label, shape=shape, color=color)\n","            AND_gateways.append(chunk)\n","        \"\"\"else:\n","            print(f\"and gateway: {label} is already added\")\"\"\"\n","\n","    elif element_type == 'Activity':\n","        display_label = label\n","        if chunk not in activities:\n","            shape = node_styles.get(element_type, 'box')\n","            color = node_colors.get(element_type, 'black')\n","            new_label = label + '_' + str(sent_id) + '_' + str(tokenspan[0])\n","            dot.node(new_label, display_label, shape=shape, color=color)\n","            activities.append(chunk)\n","        \"\"\"else:\n","            print(f\"activity: {label} is already added\")\"\"\"\n","\n","    elif element_type == 'Condition Specification':\n","        #print(f\"This is a Condition Specification chunk which does not need a node: {chunk}\")\n","        specifications.append(chunk)\n","\n","    elif element_type == 'Further Specification':\n","        if chunk not in specifications:\n","            shape = node_styles.get(element_type, 'box')\n","            color = node_colors.get(element_type, 'black')\n","            new_label = label + '_' + str(sent_id) + '_' + str(tokenspan[0])\n","            dot.node(new_label, label, shape=shape, color=color)\n","            specifications.append(chunk)\n","        \"\"\"else:\n","            print(f\"further specification: {label} is already added\")\"\"\"\n","    else:\n","      print(f\"element type: {element_type} is not defined\")\n","\n","\n","# Function to add edges based on relation type\n","def add_edge(source_chunk, target_chunk, relation_type, relations):\n","    source_label = source_chunk[0]\n","    target_label = target_chunk[0]\n","\n","    if relation_type == 'flow':\n","        if target_chunk[1] == 'Condition Specification':\n","            # Add condition specification to the edge label\n","            #print(f\"there should be a condition specification flow between {source_label, target_label}\")\n","            #dot.edge(source_label, added_nodes[target_chunk[0]][0], label=target_label) #relation from xor to condition specification does not need to be printed, only the flow from xor to next activity with corresponding condition specification\n","            specifications.append(target_chunk)\n","\n","        elif source_chunk[1] == 'Condition Specification':\n","            #print(f\"There is a condition specification flow that goes from {source_chunk} to {target_chunk}\")\n","            #source_chunks_new = [relation['source_chunk'] for relation in relations if relation['target_chunk'] == source_chunk]\n","            source_chunk_temp = next((relation['source_chunk'] for relation in relations if relation['target_chunk'] == source_chunk), None)\n","            #print('this is source_chunk_temp: ', source_chunk_temp, 'replacing this: ', source_chunk)\n","            saga_relations = [relation for relation in relations if relation['relation_type'] == 'same gateway']\n","            source_chunk_new = next((relation['source_chunk'] for relation in saga_relations if relation['target_chunk'] == source_chunk_temp), source_chunk_temp)\n","            #print('this is source_chunk_new: ', source_chunk_new, 'replacing this: ', source_chunk)\n","            add_node(source_chunk_new)\n","            add_node(target_chunk)\n","            source_label_new, source_element_type, source_sent_id, source_tokenspan = source_chunk_new\n","            target_label, target_element_type, target_sent_id, target_tokenspan = target_chunk\n","            source_chunk_new_label = source_label_new + '_' + str(source_sent_id) + '_' + str(source_tokenspan[0])\n","            target_chunk_new_label = target_label + '_' + str(target_sent_id) + '_' + str(target_tokenspan[0])\n","            dot.edge(source_chunk_new_label, target_chunk_new_label, style = 'bold', xlabel=source_label)\n","            specifications.append(source_chunk)\n","\n","\n","        elif source_chunk[1] == 'XOR Gateway' and target_chunk[1] != 'Condition Specification':\n","            saga_relations = [relation for relation in relations if relation['relation_type'] == 'same gateway']\n","            new_source_chunk = next((relation['source_chunk'] for relation in saga_relations if relation['target_chunk'] == source_chunk), source_chunk)\n","            add_node(new_source_chunk)\n","            add_node(target_chunk)\n","            source_label_new, source_element_type, source_sent_id, source_tokenspan = new_source_chunk\n","            target_label, target_element_type, target_sent_id, target_tokenspan = target_chunk\n","            source_chunk_new_label = source_label_new + '_' + str(source_sent_id) + '_' + str(source_tokenspan[0])\n","            target_chunk_new_label = target_label + '_' + str(target_sent_id) + '_' + str(target_tokenspan[0])\n","            dot.edge(source_chunk_new_label, target_chunk_new_label, style ='bold')\n","\n","        elif source_chunk[1] != 'Condition Specification' and target_chunk[1] == 'XOR Gateway':\n","            saga_relations = [relation for relation in relations if relation['relation_type'] == 'same gateway']\n","            new_target_chunk = next((relation['source_chunk'] for relation in saga_relations if relation['target_chunk'] == target_chunk), target_chunk)\n","            add_node(source_chunk)\n","            add_node(new_target_chunk)\n","            target_label_new, target_element_type, target_sent_id, target_tokenspan = new_target_chunk\n","            source_label, source_element_type, source_sent_id, source_tokenspan = source_chunk\n","            target_chunk_new_label = target_label_new + '_' + str(target_sent_id) + '_' + str(target_tokenspan[0])\n","            source_chunk_new_label = source_label + '_' + str(source_sent_id) + '_' + str(source_tokenspan[0])\n","            dot.edge(source_chunk_new_label, target_chunk_new_label, style ='bold')\n","            #print(\"This has happened.\")\n","\n","        else:\n","            add_node(source_chunk)\n","            add_node(target_chunk)\n","            source_label, source_element_type, source_sent_id, source_tokenspan = source_chunk\n","            target_label, target_element_type, target_sent_id, target_tokenspan = target_chunk\n","            new_source_label = source_label + '_' + str(source_sent_id) + '_' + str(source_tokenspan[0])\n","            new_target_label = target_label + '_' + str(target_sent_id) + '_' + str(target_tokenspan[0])\n","            dot.edge(new_source_label, new_target_label, style ='bold')\n","            #print(f\"else edge has happened. for source {source_chunk} and target {target_chunk}\")\n","\n","    elif relation_type == 'actor performer':\n","        add_node(source_chunk) #activity\n","        add_node(target_chunk) #actor (performer)\n","        source_label, source_element_type, source_sent_id, source_tokenspan = source_chunk\n","        source_chunk_label = source_label + '_' + str(source_sent_id) + '_' + str(source_tokenspan[0])\n","        dot.edge(source_chunk_label, target_label, style='solid', xlabel='performer', color='blue')\n","\n","    elif relation_type == 'actor recipient':\n","        add_node(source_chunk) #activity\n","        add_node(target_chunk) #actor (recipient)\n","        source_label, source_element_type, source_sent_id, source_tokenspan = source_chunk\n","        source_chunk_label = source_label + '_' + str(source_sent_id) + '_' + str(source_tokenspan[0])\n","        dot.edge(source_chunk_label, target_label, style='solid', xlabel='recipient', color='darkorange')\n","\n","    elif relation_type == 'same gateway':\n","        add_node(source_chunk)\n","        XOR_gateways.append(target_chunk)\n","        add_node(target_chunk)\n","\n","    elif relation_type == 'uses':\n","        add_node(source_chunk)\n","        add_node(target_chunk)\n","        source_label, source_element_type, source_sent_id, source_tokenspan = source_chunk\n","        source_chunk_label = source_label + '_' + str(source_sent_id) + '_' + str(source_tokenspan[0])\n","        dot.edge(source_chunk_label, target_label, style='dashed', xlabel='uses', color='darkgrey')\n","\n","    elif relation_type == 'further specification':\n","        add_node(source_chunk)\n","        add_node(target_chunk)\n","        source_label, source_element_type, source_sent_id, source_tokenspan = source_chunk\n","        target_label, target_element_type, target_sent_id, target_tokenspan = target_chunk\n","        new_source_label = source_label + '_' + str(source_sent_id) + '_' + str(source_tokenspan[0])\n","        new_target_label = target_label + '_' + str(target_sent_id) + '_' + str(target_tokenspan[0])\n","        dot.edge(new_source_label, new_target_label, style='dotted', xlabel='further specification', color='darkgrey')\n","\n","\n","def find_starting_chunk(relations):\n","\n","  outgoing_flow_nodes = set()\n","  incoming_flow_nodes = set()\n","\n","  for relation in relations:\n","    if relation['relation_type'] == 'flow':\n","      outgoing_flow_nodes.add(relation['source_chunk'])\n","      incoming_flow_nodes.add(relation['target_chunk'])\n","\n","  # Find nodes with outgoing flows but no incoming flows\n","  starting_node_candidates_non_sorted = list(outgoing_flow_nodes - incoming_flow_nodes)\n","  starting_node_candidates = sorted(starting_node_candidates_non_sorted, key=lambda x: x[2])\n","\n","\n","  if starting_node_candidates:\n","    starting_node_label = starting_node_candidates[0]  # Assume the first candidate is the main starting node\n","    for relation in relations:\n","      if relation['source_chunk'][0] == starting_node_label[0]:\n","        return relation['source_chunk']\n","\n","  return None\n","\n","def find_possible_last_chunks(relations):\n","\n","  outgoing_flow_nodes = set()\n","  incoming_flow_nodes = set()\n","\n","  for relation in relations:\n","    if relation['relation_type'] == 'flow':\n","      outgoing_flow_nodes.add(relation['source_chunk'])\n","      incoming_flow_nodes.add(relation['target_chunk'])\n","\n","  # Find nodes with incoming flows but no outgoing flows\n","  last_node_candidates = list(incoming_flow_nodes - outgoing_flow_nodes)\n","\n","  last_chunks = []\n","  for candidate in last_node_candidates:\n","    for relation in relations:\n","      if relation['target_chunk'] == candidate:\n","        last_chunks.append(relation['target_chunk'])\n","        break  # Move to the next candidate after finding one match\n","\n","  return last_chunks\n"]},{"cell_type":"markdown","metadata":{"id":"6GfkzbZuTo7g"},"source":["Start creating the directed graph. Running this cell creates the dot graph and visualizes it in a png."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vlg9uwnATpQ7"},"outputs":[],"source":["#@title Generate the BPMN diagram to a png visualization\n","#Start creating the directed graph. Running this cell creates the dot graph and visualizes it in a png.\n","\n","\n","\n","# Create a new directed graph: some examples of starting the dot graph with different 'attributes' for the sake of spacing between the nodes and edges\n","\n","#dot = Digraph('BPMN Diagram', comment='Generated BPMN Diagram', graph_attr={'rankdir': 'LR', 'splines': 'ortho'})\n","#dot = Digraph('BPMN Diagram', comment='Generated BPMN Diagram', graph_attr={'rankdir': 'LR', 'nodesep': '1', 'ranksep': '1.2', 'overlap': 'scale', 'splines': 'ortho'})\n","#dot = Digraph('BPMN Diagram', comment='Generated BPMN Diagram', graph_attr={'rankdir': 'LR', 'nodesep': '2', 'splines': 'ortho', 'overlap': 'scale', 'ranksep': '1.2'})\n","dot = Digraph('BPMN Diagram',\n","              comment='Generated BPMN Diagram',\n","              graph_attr={\n","                  'rankdir': 'LR',    # Left to right graph\n","                  'nodesep': '0.6',     # Increase node separation\n","                  #'splines': 'ortho', # Use orthogonal lines\n","                  'overlap': 'False', # Scale overlap\n","                  'ranksep': '5',   # Increase rank separation\n","                  'fontsize': '12',   # Default text size\n","                  'fontname': 'Arial' # Default font\n","              })\n","\n","\n","# Track added nodes to avoid duplicates, this is a dictionary with as keys the 'tokens' and as values the 'chunk'\n","#added_nodes = {}\n","actors = []\n","activity_datas = []\n","XOR_gateways = []\n","AND_gateways = []\n","activities = []\n","specifications = []\n","\n","\n","#keep track of visited gateways\n","#open_gateways = []\n","#XOR_gateways = []\n","\n","#keep track of condition specifications that have been visited\n","#specifications = []\n","\n","#create the start event\n","dot.node('START', 'START', shape= node_styles.get('Start Event', 'box'), color= 'green', style='filled')\n","\n","#find the first node to connect the start event with\n","starting_chunk = find_starting_chunk(BPMN_relations)\n","if starting_chunk:\n","  #print(\"The starting chunk is:\", starting_chunk)\n","  add_node(starting_chunk)\n","  #create an edge between start event and the first node\n","  start_label, start_element_type, start_sent_id, start_tokenspan = starting_chunk\n","  starting_chunk_label = start_label + '_' + str(start_sent_id) + '_' + str(start_tokenspan[0])\n","  dot.edge('START', starting_chunk_label, style= 'bold')\n","else:\n","  print(\"Could not determine the starting chunk.\")\n","\n","# Add edges\n","for relation in BPMN_relations:\n","    source_chunk = relation['source_chunk']\n","    target_chunk = relation['target_chunk']\n","    relation_type = relation['relation_type']\n","\n","    add_edge(source_chunk, target_chunk, relation_type, BPMN_relations)\n","\n","\n","#when all relations are processed add the END event node\n","dot.node('END', 'END', shape= 'circle', color='red', style='filled')\n","\n","#Find the last_chunks to be connected to the END node\n","last_chunks = find_possible_last_chunks(BPMN_relations)\n","#print(f\"these are the possible last chunks: {last_chunks}\")\n","\n","# Create a unique XOR Gateway if multiple last chunks exist\n","if len(last_chunks) > 1:\n","    xor_gateway_label = 'XOR_Join'\n","    dot.node(xor_gateway_label, 'X_join', shape='diamond')\n","    for last_chunk in last_chunks:\n","        last_label, last_element_type, last_sent_id, last_tokenspan = last_chunk\n","        last_chunk_label = last_label + '_' + str(last_sent_id) + '_' + str(last_tokenspan[0])\n","        dot.edge(last_chunk_label, xor_gateway_label, style='bold')\n","    # Finally, connect the XOR gateway to the end event\n","    dot.edge(xor_gateway_label, 'END', style='bold')\n","elif len(last_chunks) == 1: # Check if last_chunks is not empty\n","    # Direct connection if only one last chunk\n","    last_label, last_element_type, last_sent_id, last_tokenspan = last_chunks[0]\n","    last_chunk_label = last_label + '_' + str(last_sent_id) + '_' + str(last_tokenspan[0])\n","    dot.edge(last_chunk_label, 'END', style='bold')\n","else:\n","    print(\"No last chunks found. The diagram might be incomplete.\") # Handle the case when no last chunks are found\n","\n","\n","#From here on, the BPMN diagram is generated.\n","#It could be that relations were missed and therefore some entities were not generated in the diagram.\n","#The following loop runs over every chunk in the mention_chunk list and checks if an element should be made.\n","\n","for mention_chunk in mention_chunks:\n","    label, element_type, sent_id, tokenspan = mention_chunk\n","\n","    add_node(mention_chunk)\n","\n","    if element_type == 'Condition Specification' or element_type == 'Further Specification':\n","        if mention_chunk not in specifications:\n","            shape = node_styles.get(element_type, 'box')\n","            color = 'grey'\n","            display_label = label + '(' + element_type + ')'\n","            new_label = label + '_' + str(sent_id) + '_' + str(tokenspan[0])\n","            dot.node(new_label, display_label, shape=shape, color=color)\n","            specifications.append(mention_chunk)\n","\n","    else:\n","      print(\"\")\n","\n","# render the graph to a png\n","png_data = dot.pipe(format='png')\n","\n","# display the png_data from above\n","display(Image(png_data))\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NZTzj8YNdo9B"},"source":["##5.3 SAVING all outputs and Changing the BPMN diagram\n","In the following box, there are several options for saving the dot graph as a:\n","\n","\n","*   png\n","*   html\n","*   svg\n","*   dot\n","\n","The .dot file version of the dot graph is saved for the reason of easily changing within this file and reloading it in the next cell accordingly to the user preferences.\n","\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"KS7i7--SbUAY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PIXPSteicZcJ"},"outputs":[],"source":["# Assume 'dot' is your Digraph object from graphviz\n","#dot.format = 'png'  # Set the format to PNG\n","#dot.render('output_filename', cleanup=True)  # Render and save to file\n","\n","#dot.render('doc-20.15_BPMN', format='png', cleanup=False) #this will create the png file as well as the intermediate files (not to be confused with the .dot file)\n","\n","#dot.format = 'pdf'  # Set the format to PDF\n","#dot.render('output_filename', cleanup=True)  # Render and save to file\n","\n","#dot.format = 'svg'  # Set the format to SVG\n","#dot.render('output_filename', cleanup=True)  # Render and save to file\n","\n","#if preferred the graph can be saved to a html file\n","'''\n","# Generate SVG from your dot object\n","svg_output = dot.pipe(format='svg').decode('utf-8')\n","\n","# HTML template to embed SVG\n","html_output = f\"\"\"\n","<html>\n","<head>\n","<title>Graphviz Diagram</title>\n","</head>\n","<body>\n","<h1>My Graphviz Graph</h1>\n","<div>{svg_output}</div>\n","</body>S\n","</html>\n","\"\"\"\n","\n","# Write HTML to a file\n","with open('output.html', 'w') as f:\n","    f.write(html_output)'''\n","\n","doc_name = document_name_selection[:-5]  # replace 'example_graph' with your actual document name\n","base_filename = copy.copy(doc_name + \".dot\")\n","pipeline_path = \"/content/drive/MyDrive/THESIS/CODING/NAM_TESTING/PIPELINE/PIPELINE_OUTPUTS/PREDICTIONS_ON_TEST/\" #adjust to correct file path if gold standard data is used or predictions\n","\n","# Full path for the .dot file\n","full_path = os.path.join(pipeline_path, base_filename)\n","\n","# Save the DOT file\n","with open(full_path, 'w') as f:\n","    f.write(dot.source)\n","\n","# Render the graph to PNG and adjust filename path for proper saving\n","png_filename = os.path.splitext(base_filename)[0]\n","output_path = dot.render(filename=os.path.join(pipeline_path, png_filename), format='png', cleanup=False)\n","\n","# Output paths\n","print(f\"Generated image saved to: {output_path}\")\n","print(f\"DOT source file saved to: {full_path}\")\n","\n"]},{"cell_type":"markdown","source":["Write away the non-resolved mentions and the resolved mentions for the test set documents. This is for documentation purposes."],"metadata":{"id":"qDGX0GJzfEl3"}},{"cell_type":"code","source":["MENTIONS_base_filename = copy.copy(doc_name + \"_MENTIONS\" +\".txt\")\n","MENTIONS_pipeline_path = \"/content/drive/MyDrive/THESIS/CODING/NAM_TESTING/PIPELINE/PIPELINE_OUTPUTS/PREDICTIONS_ON_TEST/\" #adjust to correct file path if gold standard data is used or predictions\n","MENTIONS_full_path = os.path.join(MENTIONS_pipeline_path, MENTIONS_base_filename)\n","\n","with open(MENTIONS_full_path, 'w') as file:\n","    file.write(\"Pure output list of NER predictions per sentence: \\n\")\n","    file.write('\\n')\n","\n","    for sentence in NER_PRED_out:\n","        file.write(str(sentence))\n","        file.write('\\n')\n","\n","    file.write('____________________________________________________________________________________________\\n')\n","    file.write(\"non-resolved mention chunks: \\n\")\n","    file.write('\\n')\n","    for chunk in NER_prediction['pred_data']:\n","        file.write(str(chunk))\n","        file.write('\\n')\n","\n","\n","    file.write('____________________________________________________________________________________________\\n')\n","    file.write(\"resolved mention chunks: \\n\")\n","    file.write('\\n')\n","\n","    for chunk in resolved_NER:\n","        file.write(str(chunk))\n","        file.write('\\n')\n","\n"],"metadata":{"id":"EtxnFm9UfD-q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Write away the non-resolved relations and resolved relations for the test set. This is for documentation purposes."],"metadata":{"id":"N-2FkRbDeuhZ"}},{"cell_type":"code","source":["RELATIONS_base_filename = copy.copy(doc_name + \"_RELATIONS\" +\".txt\")\n","RELATIONS_pipeline_path = \"/content/drive/MyDrive/THESIS/CODING/NAM_TESTING/PIPELINE/PIPELINE_OUTPUTS/PREDICTIONS_ON_TEST/\" #adjust to correct file path if gold standard data is used or predictions\n","RELATIONS_full_path = os.path.join(RELATIONS_pipeline_path, RELATIONS_base_filename)\n","\n","with open(RELATIONS_full_path, 'w') as file:\n","    # Following lines are for visualization only:\n","    file.write(\"non-resolved relations: \\n\")\n","    for relation in predicted_relations:\n","        file.write(f\"relation_type: {relation['relation_type']} \")\n","        file.write(f\"\\n\")\n","        file.write(f\"source_chunk: {relation['source_chunk']} \")\n","        file.write(f\"\\n\")\n","        file.write(f\"target_chunk: {relation['target_chunk']} \")\n","        file.write('\\n')\n","        file.write('\\n')\n","    file.write('____________________________________________________________________________________________\\n')\n","    file.write(\"resolved relations: \\n\")\n","    file.write('\\n')\n","\n","    for relation in BPMN_relations:\n","        file.write(f\"relation_type: {relation['relation_type']} \")\n","        file.write(f\"\\n\")\n","        file.write(f\"source_chunk: {relation['source_chunk']} \")\n","        file.write(f\"\\n\")\n","        file.write(f\"target_chunk: {relation['target_chunk']} \")\n","        file.write('\\n')\n","        file.write('\\n')\n","\n","df_relations.to_csv(f\"/content/drive/MyDrive/THESIS/CODING/NAM_TESTING/PIPELINE/PIPELINE_OUTPUTS/PREDICTIONS_ON_TEST/{doc_name}_relations_DF.csv\", index=False)\n","result_df.to_csv(f\"/content/drive/MyDrive/THESIS/CODING/NAM_TESTING/PIPELINE/PIPELINE_OUTPUTS/PREDICTIONS_ON_TEST/{doc_name}_result_relations_DF.csv\", index=False)"],"metadata":{"id":"RpVfsaDpaUNC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Write away the clusters of the document. This is for documentation."],"metadata":{"id":"pF9igcGje1eh"}},{"cell_type":"code","source":["cluster_base_filename = copy.copy(doc_name + \"CLUSTERS\" +\".txt\")\n","cluster_pipeline_path = \"/content/drive/MyDrive/THESIS/CODING/NAM_TESTING/PIPELINE/PIPELINE_OUTPUTS/PREDICTIONS_ON_TEST/\" #adjust to correct file path if gold standard data is used or predictions\n","cluster_full_path = os.path.join(cluster_pipeline_path, cluster_base_filename)\n","\n","with open(cluster_full_path, 'w') as file:\n","    # Following lines are for visualization only:\n","    file.write(\"These were the original clusters (without user interference): \\n\")\n","    file.write(\"\\n\")\n","\n","    for cluster in clusters_original:\n","        file.write(f\"\\nCluster {cluster['cluster_id']}:\\n\")\n","        for mention in cluster[\"mentions\"]:\n","            file.write(str(mention) + '\\n')\n","\n","    if clusters == valid_clusters:\n","        file.write(\"\\n The user did not change the clusters. \\n\")\n","    else:\n","        file.write(\"\\n The user changed the clusters. \\n\")\n","        file.write('____________________________________________________________________________________________\\n')\n","        file.write(\"These are the adapted clusters by the user: \\n\")\n","        file.write(\"\\n\")\n","        for cluster in valid_clusters:\n","          file.write(f\"\\nCluster {cluster['cluster_id']}:\\n\")\n","          for mention in cluster[\"mentions\"]:\n","              file.write(str(mention) + '\\n')\n","\n"],"metadata":{"id":"GIocheWyWfIr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cwwHz4H-e2sA"},"source":["Now the .dot file can be changed accordingly\n","\n","Open the corrected and saved version of the .dot file\n","Make a new rendering into a png file and display it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T8dtePo1YtRp"},"outputs":[],"source":["from graphviz import Source\n","from IPython.display import Image, display\n","\n","# Load the modified DOT file\n","#with open('/content/GOLD_document6_corrected.dot', 'r') as file:\n","    #dot_source = file.read()\n","\n","\n","with open(full_path, 'r') as file: #REPLACE the string with the correct document name of the corrected graph\n","    dot_source = file.read()\n","\n","# Create a Source object\n","dot = Source(dot_source)\n","\n","# Render the DOT source to a PNG file and display it\n","png_data = dot.pipe(format='png')\n","display(Image(png_data))\n","\n","# Optionally, save the PNG file to colab environment\n","with open('corrected_BPMN.png', 'wb') as f:\n","    f.write(png_data)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1FpogBCfw3b4IEjderoVwbssXZHq9l-bm","timestamp":1720351965786},{"file_id":"1VnYScPOufCH4-xuG0CxR1xsXPWtXN7-z","timestamp":1720175532957},{"file_id":"1U5R966is7O2G3yMhUK0M1BJtM0s5vtwX","timestamp":1720170420712}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}