{"cells":[{"cell_type":"markdown","metadata":{"id":"TGmyYgFOpZLT"},"source":["#1. INTRODUCTION\n","\n","This script contains a complete pipeline to transform text to a BPMN diagram that uses ```BERT base uncased``` for the Named Entity Resolution (NER) task, ```CATBOOST``` for the Relational Extraction (RE) and the ```en_coreference_web_trf``` pretrained model from spacy-experimental used for coreference resolution, also known as Entity Resolution (ER). The script will load pre-trained models. Refer to the latest colab for NER (```CrossValidation with BERT base uncased```) and RE (``CatBoost_Crossvalidation_RE.ipynb```) for the training of these models.\n","\n","The purpose of this colab is to test the performance using own text examples. Your text examples are not being saved.\n","\n","You need to input your business process description. Then, you have an option to see the ouput of each step in the pipeline with ```verbose_choice```). Finally, in subsection 4 you have the option to remove incorrect Coreferences if you so wish.\n","\n","***See section 6 for the model output***"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"7WvxZpy_3SXL"},"outputs":[],"source":["#@title 1.1.Testing Document Input\n","#@markdown Please input a short process description.\n","\n","class EmptyInputException(Exception):\n","    def __init__(self, message=\"An error occurred\", errors=None, custom_print_message=None):\n","        super().__init__(message)\n","        self.errors = errors\n","        self.custom_print_message = \"No input text received, please try again.\"\n","\n","    def __str__(self):\n","        base_message = super().__str__()\n","        if self.custom_print_message:\n","            return f\"{base_message} - {self.custom_print_message}\"\n","        return base_message\n","\n","class LargeInputException(Exception):\n","    def __init__(self, message=\"An error occurred\", errors=None, custom_print_message=None):\n","        super().__init__(message)\n","        self.errors = errors\n","        self.custom_print_message = \"The process description is too long, please shorten it and try again.\"\n","\n","    def __str__(self):\n","        base_message = super().__str__()\n","        if self.custom_print_message:\n","            return f\"{base_message} - {self.custom_print_message}\"\n","        return base_message\n","\n","text = \"\" # @param {type: \"string\"}\n","\n","maxlength = 512\n","try:\n","  if text != \"\" and len(text) < maxlength:\n","    print(\"Thank you for your input!\")\n","  else:\n","    if len(text) >= maxlength:\n","      raise LargeInputException()\n","    else:\n","      raise EmptyInputException()\n","except EmptyInputException as e:\n","  print(e)\n","except LargeInputException as l:\n","  print(l)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"JQgRv0PVaSAA"},"outputs":[],"source":["#@title 1.2. Option to see pipeline step outputs\n","#@markdown Do you want to see the output of each step in the pipeline?\n","verbose_choice = \"Yes\" # @param['Yes', 'No' ]\n","\n","if verbose_choice == 'Yes':\n","  verbose = True\n","else:\n","  verbose = False"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"0CqPH5XVzb4D","cellView":"form"},"outputs":[],"source":["#@title 1.3. Installations and Downloads (takes \\~3-4 minutes on Intel Xeon CPU @2.20GHz)\n","%%capture\n","import os\n","# Define the path to the flag file, we do this so you can rerun the whole colab on different examples without having to wait 4 minutes each time.\n","flag_file_installations = '/content/installed_flag'\n","\n","# Check if the flag file exists\n","if not os.path.exists(flag_file_installations):\n","    # Installations\n","    !pip install spacy\n","    !pip install nltk\n","    !pip install catboost\n","    !pip install spacy-experimental==0.6.2\n","    !pip install https://github.com/explosion/spacy-experimental/releases/download/v0.6.1/en_coreference_web_trf-3.4.0a2-py3-none-any.whl\n","    !python -m spacy download en_core_web_md\n","    !pip install graphviz\n","\n","    # Create the flag file\n","    with open(flag_file_installations, 'w') as f:\n","        f.write('Installed')\n","else:\n","    print(\"Packages already installed. Skipping installations.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D-SSMbQSznG3","cellView":"form"},"outputs":[],"source":["#@title 1.4. Import Libraries\n","%%capture\n","# Define the path to the flag file\n","flag_file_imports = '/content/imports_flag'\n","\n","# Check if the flag file exists\n","if not os.path.exists(flag_file_imports):\n","    # Perform the imports and any necessary downloads\n","    import os\n","    import json\n","    import copy\n","    import random\n","    import torch\n","    import spacy\n","    import numpy as np\n","    import pandas as pd\n","    import seaborn as sns\n","    import matplotlib.pyplot as plt\n","    import gdown\n","    from tqdm import tqdm\n","    from collections import defaultdict\n","    from torch.optim import AdamW\n","    from torch.utils.data import DataLoader, Dataset, random_split, Subset\n","    from transformers import (\n","        BertTokenizer, BertForTokenClassification,\n","        get_linear_schedule_with_warmup,\n","        AutoModelForTokenClassification,\n","        TrainingArguments, Trainer,\n","        RobertaTokenizer, RobertaForTokenClassification\n","    )\n","    from sklearn.model_selection import (\n","        GroupShuffleSplit, GroupKFold, train_test_split\n","    )\n","    from sklearn.metrics import (\n","        precision_recall_fscore_support, confusion_matrix,\n","        accuracy_score, f1_score\n","    )\n","    from sklearn.preprocessing import LabelEncoder\n","    from catboost import CatBoostClassifier, Pool, metrics, cv\n","    from itertools import product\n","    import nltk\n","    from nltk import pos_tag\n","    from nltk.tokenize import word_tokenize\n","    from graphviz import Digraph\n","    from IPython.display import Image, display\n","\n","    # NLTK downloads\n","    nltk.download('averaged_perceptron_tagger')\n","\n","    # Create the flag file to indicate that imports have been done\n","    with open(flag_file_imports, 'w') as f:\n","        f.write('Imports completed')\n","else:\n","    print(\"Imports already completed. Skipping imports.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJ4KnaXU1-uh"},"outputs":[],"source":["#@title 1.5. Acces Pre-Trained Model\n","%%capture\n","flag_model_loading = '/content/model_loading_flag'\n","\n","# Check if the flag file exists\n","if not os.path.exists(flag_model_loading):\n","\n","  # List of Google Drive links, viewer mode only.\n","  links = {'CATBOOST':'https://drive.google.com/file/d/1a5JGJVwM9KGWOZsqszfE0gpzeeoPuAH8/view?usp=share_link',\n","          'BERT_MODEL':'https://drive.google.com/drive/folders/1-5PS_Rmu58QCstMquakjeQM0NufOOF0i?usp=share_link',\n","          'BERT_TOKENIZER':'https://drive.google.com/drive/folders/1-GlY0MjXZkhikb9lfAWCP_0gouGWJ_1L?usp=share_link'\n","          }\n","\n","  # Function to extract ID from shareable link\n","  def extract_id(shareable_link):\n","      if 'file/d/' in shareable_link:\n","          return shareable_link.split('/file/d/')[1].split('/')[0]\n","      elif 'drive/folders/' in shareable_link:\n","          return shareable_link.split('/drive/folders/')[1].split('?')[0]\n","      else:\n","          raise ValueError(\"Invalid shareable link\")\n","\n","  # Function to download content based on ID type\n","  def download_content(shareable_link, destination):\n","      file_or_folder_id = extract_id(shareable_link)\n","      if 'file/d/' in shareable_link:\n","          url = f\"https://drive.google.com/uc?id={file_or_folder_id}\"\n","          gdown.download(url, destination, quiet=False)\n","      elif 'drive/folders/' in shareable_link:\n","          gdown.download_folder(id=file_or_folder_id, output=destination)\n","      else:\n","          raise ValueError(\"Invalid shareable link\")\n","\n","  # Download each link to the specified destination\n","  for key, value in links.items():\n","      destination = f'/content/{key}'\n","      download_content(value, destination)\n","\n","  BERT_MODEL_PATH = '/content/BERT_MODEL'\n","  BERT_TOKENIZER_PATH = '/content/BERT_TOKENIZER'\n","  CATBOOST_MODEL_PATH = '/content/CATBOOST'\n","\n","  print(\"All downloads completed!\")\n","\n","  # Create the flag file to indicate that imports have been done\n","  with open(flag_model_loading, 'w') as f:\n","      f.write('Model Loaded')\n","\n","else:\n","    print(\"Model already loaded. Skipping step.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9f5hxTRsw6Kf"},"outputs":[],"source":["#@title 1.6. Pre-Processing text input\n","\n","flag_en_core_web_md = '/content/en_core_web_md_flag'\n","# Check if the flag file exists\n","if not os.path.exists(flag_en_core_web_md):\n","  nlp = spacy.load(\"en_core_web_md\")\n","  # Create the flag file to indicate that imports have been done\n","  with open(flag_en_core_web_md, 'w') as f:\n","      f.write('spacy md Loaded')\n","\n","def process_text(text, nlp):\n","\n","    # Process the text\n","    doc = nlp(text)\n","\n","    sentences_data = []\n","    # Define a set of special characters\n","    special_chars = {\"'\", \"-\", \"â€™\", \"(\", \"&\", \")\"}  # Add more as needed\n","\n","    # Iterate through sentences\n","    for sent_id, sent in enumerate(doc.sents):\n","        tokens = [token.text for token in sent]\n","\n","        # Initialize an empty list to hold the cleaned tokens\n","        cleaned_tokens = []\n","\n","        i = 0\n","        while i < len(tokens):\n","            token = tokens[i]\n","            if any(char in token for char in special_chars):\n","                # Remove special characters and combine with previous token if not the first token\n","                if cleaned_tokens:\n","                    cleaned_tokens[-1] += token.replace(\"'\", \"\").replace(\"-\", \"\").replace(\"â€™\", \"\").replace(\"&\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n","                else:\n","                    cleaned_tokens.append(token.replace(\"'\", \"\").replace(\"-\", \"\").replace(\"â€™\", \"\").replace(\"&\", \"\").replace(\"()\", \"\")).replace(\")\", \"\")\n","            elif token in special_chars and i > 0 and i < len(tokens) - 1:\n","                # Combine previous token with next token and skip the next token\n","                cleaned_tokens[-1] += tokens[i + 1].replace(\"'\", \"\").replace(\"-\", \"\").replace(\"â€™\", \"\").replace(\"â€”\", \"\").replace(\"â€“\", \"\")\n","                i += 1  # Skip the next token\n","            else:\n","                # Append the token as is\n","                cleaned_tokens.append(token)\n","            i += 1\n","\n","        ner_tags = ['O'] * len(tokens)  # Dummy NER tags\n","\n","        sentence_data = {\n","            \"document name\": 'testing', #default value\n","            \"sentence-ID\": sent_id,\n","            \"tokens\": cleaned_tokens,\n","            \"ner-tags\": ner_tags\n","        }\n","\n","        sentences_data.append(sentence_data)\n","\n","    return sentences_data\n","\n","text = str(text)\n","flat_data = process_text(text, nlp)\n"]},{"cell_type":"markdown","metadata":{"id":"rQ2tkQD2g6En"},"source":["#2. NER Task\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1usum5N3xNOO"},"source":["##2.1. LOADING THE MODEL\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"eifjlIngfX1_"},"outputs":[],"source":["# @title 2.1.1 Variable setup for BERT\n","\n","# Set a fixed seed for all random operations\n","SEED = 42\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","random.seed(SEED)\n","\n","# If you're using CUDA: # will not be the case for deployment\n","torch.cuda.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)  # For multi-GPU setups\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","#Is needed for the translation between floats and labels, as BERT's output is a float.\n","label_map = {\n","    \"O\": 0,\n","    \"B-Actor\": 1, \"I-Actor\": 2,\n","    \"B-Activity\": 3, \"I-Activity\": 4,\n","    \"B-Activity Data\": 5, \"I-Activity Data\": 6,\n","    \"B-Further Specification\": 7, \"I-Further Specification\": 8,\n","    \"B-XOR Gateway\": 9, \"I-XOR Gateway\": 10,\n","    \"B-Condition Specification\": 11, \"I-Condition Specification\": 12,\n","    \"B-AND Gateway\": 13, \"I-AND Gateway\": 14\n","}\n","\n","short_label_map = {\n","    \"O\": 0,\n","    \"Actor\": 1,\n","    \"Activity\": 2,\n","    \"Activity Data\": 3,\n","    \"Further Specification\": 4,\n","    \"XOR Gateway\": 5,\n","    \"Condition Specification\": 6,\n","    \"AND Gateway\": 7\n","}\n","\n","# Reverse map for evaluation purposes\n","reverse_label_map = {v: k for k, v in label_map.items()}\n","NUM_LABELS = len(label_map)  # Correctly reflects the actual classification labels\n","\n","# Constants, can be changed for model optimisation\n","MAX_LEN = 128  # Or any max length suited to your data\n","BATCH_SIZE = 8\n","EPOCHS = 10"]},{"cell_type":"markdown","metadata":{"id":"5mloozYgwLIX"},"source":["\n","The code below are pre-requisite code that needs to be run before prediction can happen on the testing document. This code is similar to the code used for training BERT but with some modifications."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3yuZaR1t51p3"},"outputs":[],"source":["# @title 2.1.2 Helper Code for Prediction & Evaluation\n","class NERDataset(Dataset):\n","    def __init__(self, doc_name, sentences, labels, tokenizer, max_len, label_map):\n","        self.doc_name = doc_name #To keep track of what document we are analyzing\n","        self.sentences = sentences  # List of sentences (each sentence is a list of words)\n","        self.labels = labels        # List of label sequences corresponding to each sentence\n","        self.tokenizer = tokenizer  # BERT tokenizer\n","        self.max_len = max_len      # Maximum sequence length\n","        self.label_map = label_map  # Mapping from label strings to integers\n","\n","    def __len__(self):\n","        return len(self.sentences)\n","\n","    def __getitem__(self, idx):\n","        words = self.sentences[idx]\n","        word_labels = self.labels[idx]\n","\n","        # Tokenize words and align labels with tokens\n","        tokens = []\n","        aligned_labels = []\n","        for word, label in zip(words, word_labels):\n","            word_tokens = self.tokenizer.tokenize(word)\n","            tokens.extend(word_tokens)\n","            # Extend the label to all subwords\n","            aligned_labels.extend([label] * len(word_tokens))\n","\n","        # Truncate tokens and labels if they exceed max_len\n","        tokens = tokens[:self.max_len-2]\n","        aligned_labels = aligned_labels[:self.max_len-2]\n","\n","        # Convert tokens and labels to model inputs\n","        input_ids = self.tokenizer.convert_tokens_to_ids(['[CLS]'] + tokens + ['[SEP]'])\n","        attention_mask = [1] * len(input_ids)\n","        label_ids = [self.label_map['O']] + [self.label_map[label] for label in aligned_labels] + [self.label_map['O']]\n","\n","        # Padding\n","        padding_length = self.max_len - len(input_ids)\n","        input_ids += [self.tokenizer.pad_token_id] * padding_length\n","        attention_mask += [0] * padding_length\n","        label_ids += [self.label_map['O']] * padding_length\n","\n","        return {\n","            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n","            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n","            'labels': torch.tensor(label_ids, dtype=torch.long),\n","            'doc_name': self.doc_name[idx]\n","        }\n","\n","def load_and_group_ner_data(file_path):\n","    grouped_data = {}\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            entry = json.loads(line)\n","            document_name = entry['document name']  # Adjusted to use 'document name'\n","            if document_name not in grouped_data:\n","                grouped_data[document_name] = []\n","            grouped_data[document_name].append(entry)\n","\n","    # Sort each group by 'sentence-ID'\n","    for doc in grouped_data.values():\n","        doc.sort(key=lambda x: x['sentence-ID'])\n","\n","    return list(grouped_data.values())\n","\n","def convert_IOB2_to_chunks(nested_label_list, reverse_label_map):\n","\n","    all_chunks = []\n","    #print(nested_label_list)\n","    for sentence in nested_label_list:\n","        #print(sentence)\n","        chunks = []\n","        current_chunk = []\n","        current_type = None\n","\n","        for idx, label_idx in enumerate(sentence):\n","            label = reverse_label_map[label_idx]\n","\n","            if label.startswith('B-'):\n","                if current_chunk:\n","                    chunks.append((current_type, current_chunk))\n","                    current_chunk = []\n","                current_type = label[2:]\n","                current_chunk = [idx, idx]  # Start a new chunk\n","\n","            elif label.startswith('I-') and current_type == label[2:]:\n","                current_chunk[1] = idx  # Extend the current chunk\n","\n","            elif label == 'O':\n","                if current_type != 'O':  # Start a new 'O' chunk if the last chunk wasn't 'O'\n","                    if current_chunk:\n","                        chunks.append((current_type, current_chunk))\n","                        current_chunk = []\n","                    current_type = 'O'\n","                current_chunk = [idx, idx] if not current_chunk else [current_chunk[0], idx]\n","\n","            else:  # For non-matching 'I-' or different entity types\n","                if current_chunk:\n","                    chunks.append((current_type, current_chunk))\n","                    current_chunk = []\n","                current_type = None\n","\n","        if current_chunk:  # Add the last chunk if exists\n","            chunks.append((current_type, current_chunk))\n","\n","        all_chunks.append(chunks)\n","\n","    return all_chunks\n","\n","def convert_int_2string(nested_label_list, reverse_label_map):\n","\n","    all_labels = []\n","    for sentence in nested_label_list:\n","        string_labels = [reverse_label_map[label_idx] for label_idx in sentence]\n","        all_labels.append(string_labels)\n","\n","    return all_labels\n","\n","def evaluate_model(model, val_dataloader, reverse_label_map, tokenizer):\n","    # Set the device to GPU if available, else CPU\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    # Switch the model to evaluation mode to disable dropout layers\n","    model.eval()\n","\n","    # Initialize variables to accumulate loss and store predictions and true labels\n","    eval_loss = 0\n","    nb_eval_steps = 0\n","    true_labels = []\n","    pred_labels = []\n","    documents = []\n","    doc_names = []\n","\n","    # Iterate over batches in the validation dataloader\n","    for batch in val_dataloader:\n","        # Move batch data to the same device as the model\n","        #batch = {k: v.to(device) for k, v in batch.items()}\n","\n","        # To filter out the passed document name so it does not bother the model prediction when bringing it to device\n","        batch = {k: v.to(device) if k != 'doc_name' else v for k, v in batch.items()}\n","\n","        # Perform inference (forward pass) without computing gradients\n","        with torch.no_grad():\n","            #To filter out the passed document name so it does not bother the model prediction\n","            outputs = model(**{k: v for k, v in batch.items() if k != 'doc_name'})\n","            #outputs = model(**batch)\n","            logits = outputs.logits\n","            eval_loss += outputs.loss.item()\n","\n","        # Convert logits to predicted class indices\n","        preds = np.argmax(logits.detach().cpu().numpy(), axis=2)\n","\n","        # Collect true and predicted labels for each sentence in the batch\n","        for i in range(batch[\"input_ids\"].shape[0]):\n","            # Use attention mask to filter out padding tokens\n","            input_ids = batch[\"input_ids\"][i].cpu().numpy()\n","            mask = batch[\"attention_mask\"][i].cpu().numpy()\n","            true_sequence = batch[\"labels\"][i].cpu().numpy()[mask == 1]\n","            pred_sequence = preds[i][mask == 1]\n","            filtered_input_ids = input_ids[mask == 1]\n","            sentence_tokens = [tokenizer.decode([tok_id], skip_special_tokens=True).replace(\" ##\", \"\") for tok_id in filtered_input_ids]\n","\n","            # Append individual sentence-level lists\n","            true_labels.append(true_sequence.tolist())  # Convert to list for consistency\n","            pred_labels.append(pred_sequence.tolist())\n","            documents.append(sentence_tokens)\n","            doc_names.append(batch['doc_name'][i])\n","            #print(f\"these are the batch tokens {batch_tokens}\")\n","\n","        nb_eval_steps += 1\n","\n","\n","    #Then passed it through a function that converts the predictions into entities and at the same time from a integer to a string. We do this for the True and Predicted labels altogether\n","    #We get something like ['Actor', [1, 3]]\n","    true_labels_entities = convert_IOB2_to_chunks(true_labels, reverse_label_map)\n","    pred_labels_entities = convert_IOB2_to_chunks(pred_labels, reverse_label_map)\n","\n","    return documents, doc_names,true_labels_entities, pred_labels_entities, true_labels, pred_labels\n","\n","def dataloader_internal(document):\n","    val_dataset = NERDataset(\n","                doc_name=[entry['document name'] for entry in document],\n","                sentences=[entry['tokens'] for entry in document],\n","                labels=[entry['ner-tags'] for entry in document],\n","                tokenizer=tokenizer,\n","                max_len=MAX_LEN,\n","                label_map=label_map\n","            )\n","\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n","\n","    return val_loader\n","\n","# Function to process model evaluation output and restructure data for the SpaCy Neuralcoref to parse later\n","def process_evaluation_output(documents, true_chunks_list, pred_chunks_list, doc_names):\n","    processed_output = []\n","    sentence_ID = 0\n","    target_labels = ['Further Specification', 'AND Gateway', 'Activity', 'Activity Data', 'Actor', 'Condition Specification', 'XOR Gateway']\n","\n","    for sentence, true_chunks, pred_chunks, doc_name in zip(documents, true_chunks_list, pred_chunks_list, doc_names):\n","        # Handle concatenation by respecting spaces at the beginning and end of the sentence\n","        formatted_sentence = ''\n","        no_preceding_space_chars = {\"'\", \",\", \".\", \"s\", \";\", \"?\", \"!\", \":\", \"-\"}\n","        sharp_adjustments = {}\n","        sharp_adjustment_count = 0\n","        #print(sentence)\n","        #print(f\"\\n sentence {sentence_ID}\")\n","        #print(\"---\")\n","        for i, token in enumerate(sentence):\n","          #print(f\"\\n {token}\")\n","          #Because BERT uses ## to cut up words into part of words it can recognize.\n","          clean_token = token.replace('##', '')\n","\n","          # Determine if a space should be added before the current token\n","          should_add_space = True\n","          if i == 0:  # Do not add space before the first token\n","              should_add_space = False\n","          elif token.startswith('##'):  # Do not add space for continuation tokens\n","              sharp_adjustment_count += 1\n","              should_add_space = False\n","          elif clean_token in no_preceding_space_chars:  # Check against no_preceding_space_chars\n","              should_add_space = False\n","          elif sentence[i - 1] == \" \":  # Do not add space if the previous token is an explicit space\n","              should_add_space = False\n","\n","          # Add space before the token if the condition is met\n","          if should_add_space:\n","              formatted_sentence += ' ' + clean_token\n","          else:\n","              formatted_sentence += clean_token\n","\n","          # Record the adjustment count at this position\n","          sharp_adjustments[i] = sharp_adjustment_count\n","\n","        # Remove leading and trailing spaces, and fix tokens starting with '##'\n","        formatted_sentence = formatted_sentence.strip()\n","\n","        # Initialize list to hold relevant token spans\n","        relevant_true_spans = []\n","        relevant_pred_spans = []\n","\n","        for label, span in true_chunks:\n","            if label in target_labels:\n","              if sentence[span[0]].startswith('##'):\n","                continue\n","\n","              adjusted_start = span[0] - 1 - sharp_adjustments[span[0] - 1]\n","              adjusted_end = span[1] - 1 - sharp_adjustments[span[1] - 1]\n","              # Ensure the end index does not go beyond the start index for single-word entities\n","              if sentence[span[1]].startswith('##'):\n","                  adjusted_end -= 1\n","\n","              adjusted_end = max(adjusted_start, adjusted_end)\n","              adjusted_span = (adjusted_start, adjusted_end) #we subtract one to get a zero-based indexing as python\n","              relevant_true_spans.append((label, sentence_ID, adjusted_span))\n","\n","        # Keep a list of predicted labels too\n","        for label, span in pred_chunks:\n","            if label in target_labels:\n","              if sentence[span[0]].startswith('##'):\n","                continue\n","\n","              adjusted_start = span[0] - 1 - sharp_adjustments[span[0] - 1]\n","              adjusted_end = span[1] - 1 - sharp_adjustments[span[1] - 1]\n","\n","              if sentence[span[1]].startswith('##'):\n","                  adjusted_end -= 1\n","\n","              # Ensure adjusted_end is not less than adjusted_start\n","              adjusted_end = max(adjusted_start, adjusted_end)\n","              adjusted_span = (adjusted_start, adjusted_end)\n","              relevant_pred_spans.append((label, sentence_ID, adjusted_span))\n","\n","        # Structure the output for the current sentence\n","        sentence_data = {\n","            \"document_name\": doc_name,\n","            \"sentence\": [formatted_sentence],\n","            \"sentence_indexed\": sentence,  # Preserve the original sentence list\n","            \"relevant_true_spans\": relevant_true_spans,\n","            \"relevant_pred_spans\": relevant_pred_spans\n","        }\n","\n","        # Append structured data to the aggregate list\n","        processed_output.append(sentence_data)\n","        sentence_ID += 1\n","\n","    return processed_output"]},{"cell_type":"markdown","metadata":{"id":"gx3Eay7ywnKE"},"source":["Now that all the pre-requisites and prep work are done, we can load the pre-trained model using the cell below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-kvdpjkDxZp2"},"outputs":[],"source":["#@title 2.1.3 BERT Model Loading\n","#Change depending on your config\n","model_path_BERT = copy.copy(BERT_MODEL_PATH)\n","tokenizer_path = copy.copy(BERT_TOKENIZER_PATH)\n","\n","model = BertForTokenClassification.from_pretrained(model_path_BERT)\n","tokenizer = BertTokenizer.from_pretrained(tokenizer_path)"]},{"cell_type":"markdown","metadata":{"id":"RMaaYDM_5vbf"},"source":["##2.2. PREDICTING WITH PRE-TRAINED BERT MODEL\n","\n","The following code takes the test data (flat_data variable, see section 2.1) and let's the model label the tokens. Then a specialized processing function extracts the relevant information (like the label, sentence ID and token ID spans) from the evaluation of the model. **NOTE: THE NER PREDICTION HAPPENS HERE**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Om4Jfmui7V14"},"outputs":[],"source":["# @title 2.2.1 Prediction Code and Keeping a List\n","# Transform the data into a useable format\n","val_loader = dataloader_internal(flat_data) #contains the document\n","# Prediction generation:\n","documents, doc_names, true_chunks_list, pred_chunks_list, true_labels_IOB, pred_labels_IOB = evaluate_model(model, val_loader, reverse_label_map, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4lizFU3htW0v"},"outputs":[],"source":["# @title 2.2.2 Code to filter out BERT Tokenizer and NER prediction into format\n","\n","new_tokens = []\n","new_true_labels = []\n","new_pred_labels = []\n","\n","# In what follows we try to combine subtokens containing '##' and filter their labels to only capture the main token label.\n","# We go through each sentence in the document\n","for sentence_tokens, sentence_true_labels, sentence_pred_labels in zip(documents, true_labels_IOB, pred_labels_IOB):\n","    filtered_tokens = []\n","    filtered_true_labels = []\n","    filtered_pred_labels = []\n","\n","    concatenated_token = \"\"\n","    main_label_true = None\n","    main_label_pred = None\n","\n","    # for each sentence we go through each token\n","    for i, (token, true_label, pred_label) in enumerate(zip(sentence_tokens, sentence_true_labels, sentence_pred_labels)):\n","\n","        if token.startswith(\"##\"):\n","            concatenated_token += token[2:]\n","        else:\n","            if concatenated_token: # So if this value is not empty \"\"\n","                filtered_tokens.append(concatenated_token)\n","                filtered_true_labels.append(main_label_true)\n","                filtered_pred_labels.append(main_label_pred)\n","\n","            concatenated_token = token\n","            main_label_true = true_label\n","            main_label_pred = pred_label\n","\n","            # If the next token is not a subtoken, we add the current token as standalone token, or if this is the last token we add it as well\n","            if i == len(sentence_tokens) - 1 or not sentence_tokens[i + 1].startswith(\"##\"):\n","                filtered_tokens.append(concatenated_token)\n","                filtered_true_labels.append(main_label_true)\n","                filtered_pred_labels.append(main_label_pred)\n","                concatenated_token = \"\"\n","\n","    new_tokens.append(filtered_tokens)\n","    new_true_labels.append(filtered_true_labels)\n","    new_pred_labels.append(filtered_pred_labels)\n","\n","# Here we process the NER output to collect the token chunks belonging to the same entity type\n","NER_processed_output = process_evaluation_output(new_tokens, convert_IOB2_to_chunks(new_true_labels,reverse_label_map), convert_IOB2_to_chunks(new_pred_labels,reverse_label_map), doc_names)\n","\n","NER_prediction = {'true_data': [], 'pred_data': []}\n","\n","for sentence_data in NER_processed_output:\n","  print(sentence_data['sentence'][0])\n","  doc = nlp(sentence_data['sentence'][0]) # for tokenization\n","  tokens = [token.text for token in doc]\n","\n","  for chunk in sentence_data['relevant_true_spans']:\n","\n","    if chunk[2][0] != chunk[2][1]:\n","      word = ' '.join(tokens[chunk[2][0]:chunk[2][1]+1])\n","\n","    else:\n","      word = tokens[chunk[2][0]]\n","\n","    NER_prediction['true_data'].append({'tokens': word,\n","                           'label': chunk[0],\n","                           'sentence_id': chunk[1],\n","                           'span':chunk[2]\n","                           })\n","\n","  for chunk in sentence_data['relevant_pred_spans']:\n","\n","\n","    if chunk[2][0] != chunk[2][1]:\n","      word = ' '.join(tokens[chunk[2][0]:chunk[2][1]+1])\n","\n","    else:\n","      word = tokens[chunk[2][0]]\n","\n","    NER_prediction['pred_data'].append({'tokens': word,\n","                            'label': chunk[0],\n","                            'sentence_id': chunk[1],\n","                            'span':chunk[2]\n","                            })\n","\n","\n","NER_TRUE_out= convert_int_2string(new_true_labels, reverse_label_map)\n","NER_PRED_out= convert_int_2string(new_pred_labels, reverse_label_map)\n"]},{"cell_type":"markdown","metadata":{"id":"HOlmtPhad4kP"},"source":["##2.3. Output of NER\n","\n","You can optionally print the output if you have set the 'verbose' variable to True."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZRY4EU7QeDD_"},"outputs":[],"source":["# for visualisation\n","if verbose:\n","  for NER in NER_prediction['pred_data']:\n","    print('\\n', NER)"]},{"cell_type":"markdown","metadata":{"id":"VOpBSN0FQnMf"},"source":["#3. RE Task"]},{"cell_type":"markdown","metadata":{"id":"DpeTZJzejZVD"},"source":["##3.1. Loading Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8lksPh-4RHl7"},"outputs":[],"source":["#@title 3.1.1 Catboost Model Loading\n","# Create an instance of the CatBoostClassifier\n","%%capture\n","best_model_neg_sample = CatBoostClassifier()\n","\n","# Load the model from the file\n","model_path_CATBOOST = copy.copy(CATBOOST_MODEL_PATH)\n","best_model_neg_sample.load_model(model_path_CATBOOST, format=\"cbm\")"]},{"cell_type":"markdown","metadata":{"id":"-q7GMJIxkBmz"},"source":["##3.2 Pre-Processing of NER data\n","\n","In this subsection, we will create the RE data using information from the NER step. So the previous code needs to be run as well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sJUvHrMXVG2E","cellView":"form"},"outputs":[],"source":["#@title 3.2.1 goldstandard data use selection\n","#@markdown This section can be disregarded. It was used in pipeline evaluation.\n","\n","goldstandard_NER_check = \"False\" #@param [\"False\"] {type: \"string\"}\n","goldstandard_RE_check = \"False\" #@param [\"False\"] {type: \"string\"}\n","\n","use_goldstandardNER= False\n","if goldstandard_NER_check == \"True\":\n","  use_goldstandardNER = True\n","\n","use_goldstandardRE = False\n","if goldstandard_RE_check == \"True\":\n","  use_goldstandardRE = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2Ti_tQUv7gU"},"outputs":[],"source":["# @title 3.2.2 Pre-Processing of NER Data into RE dictionary\n","# First we create a formatted relations dictionary from the existing NER data step.\n","doc_name = doc_names[0] #!use doc_name\n","\n","# Initialize an empty list to hold the tokens and Token IDs\n","tokens = []\n","token_IDs = []\n","sentence_IDs = []\n","sentence_ID = 0\n","# Iterate over each sublist in the documents list\n","for sentence in new_tokens:\n","    # Filter out empty strings and strings that are just spaces\n","    filtered_sentence = [word for word in sentence if word != '']\n","    # Extend the tokens list with the filtered sublist\n","    tokens.extend(filtered_sentence)\n","    # Generate token IDs for the current sublist and append to token_IDs\n","    token_IDs.extend([i for i in range(len(filtered_sentence))])\n","    # Generate sentence IDs for the current sublist and extend the sentence_IDs list\n","    sentence_IDs.extend([sentence_ID] * len(filtered_sentence))\n","    # Increment the sentence ID for the next sublist\n","    sentence_ID += 1\n","\n","if use_goldstandardNER:\n","  # Initialize an empty list to hold the flattened NER tags\n","  NER_tags = []\n","  # Iterate over each sublist in the NER_tags_data list\n","  for sentence in NER_TRUE_out:\n","      # Remove the first and last element of each sublist because these tags denote white space and flatten\n","      if len(sentence) > 2:  # Ensure there are at least three elements to remove first and last\n","          NER_tags.extend(sentence[1:-1])\n","\n","else:\n","  # Initialize an empty list to hold the flattened NER tags\n","  NER_tags = []\n","  # Iterate over each sublist in the NER_tags_data list\n","  for sentence in NER_PRED_out:\n","      # Remove the first and last element of each sublist because these tags denote white space and flatten\n","      if len(sentence) > 2:  # Ensure there are at least three elements to remove first and last\n","          NER_tags.extend(sentence[1:-1])\n","\n","relational_data = {\n","    \"document name\": doc_names[0],\n","    \"tokens\": tokens,\n","    \"tokens-IDs\": token_IDs,\n","    \"ner_tags\": NER_tags,\n","    \"sentence-IDs\": sentence_IDs,\n","    \"relations\": []\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27xtjrj0VT-A"},"outputs":[],"source":["# @title 3.2.3 Data and Feature Generation code for Catboost\n","\n","def format_sentences(tokens, sentence_IDs):\n","\n","    # Initialize a dictionary to hold sentence IDs as keys and their corresponding formatted sentences as values\n","    sentences = {}\n","\n","    # Set characters that should not be preceded by a space\n","    no_preceding_space_chars = {\"'\", \",\", \".\", \"s\", \";\", \"?\", \"!\", \":\", \"-\"}\n","\n","    for token, sentence_id in zip(tokens, sentence_IDs):\n","        # Initialize the sentence key in the dictionary if not already present\n","        if sentence_id not in sentences:\n","            sentences[sentence_id] = ''\n","\n","        # Clean token if it is a subword part that BERT might have split\n","        clean_token = token.replace('##', '')\n","\n","        # Determine if a space should be added\n","        should_add_space = True\n","        if clean_token in no_preceding_space_chars:  # Check against no_preceding_space_chars\n","            should_add_space = False\n","        if sentences[sentence_id] == '':  # Do not add space before the first token\n","            should_add_space = False\n","\n","        # Add space before the token if the condition is met\n","        if should_add_space:\n","            sentences[sentence_id] += ' ' + clean_token\n","        else:\n","            sentences[sentence_id] += clean_token\n","\n","    return sentences\n","\n","def get_entity_chunks(tokens, ner_tags, sentence_ids):\n","    chunks = []\n","    current_chunk = []\n","    start_index = None\n","    last_sentence_id = None  # Variable to track the sentence ID of the previous token\n","    sentence_start_index = 0  # Index where the current sentence starts in the tokens list\n","\n","    for i, (token, tag, sentence_id) in enumerate(zip(tokens, ner_tags, sentence_ids)):\n","        if sentence_id != last_sentence_id:\n","            # Reset start_index relative to the sentence when sentence_id changes\n","            sentence_start_index = i\n","            last_sentence_id = sentence_id\n","\n","        if tag.startswith('B-'):\n","            if current_chunk:\n","                # Append the current chunk before starting a new one\n","                chunks.append((' '.join(current_chunk), start_index, last_sentence_id))\n","                current_chunk = [token]\n","            else:\n","                current_chunk = [token]\n","            start_index = i - sentence_start_index  # Calculate start index relative to the start of the sentence\n","\n","        elif tag.startswith('I-') and current_chunk:\n","            current_chunk.append(token)\n","\n","        elif tag == 'O' and current_chunk:\n","            # Complete the current chunk if it exists\n","            chunks.append((' '.join(current_chunk), start_index, last_sentence_id))\n","            current_chunk = []\n","            start_index = None\n","\n","    if current_chunk:  # Add the last chunk if it exists\n","        chunks.append((' '.join(current_chunk), start_index, last_sentence_id))\n","\n","    return chunks\n","\n","def get_root_of_chunk(nlp, chunk_text):\n","\n","    doc = nlp(chunk_text)\n","    # Usually, the root token is the one whose head is outside the phrase itself or is itself\n","    for token in doc:\n","        if token.head == token or token.head not in doc:\n","            return token\n","    return doc[0]  # Fallback to the first token if no clear root is found\n","\n","def root_index_lookup(root, chunk, chunk_start_index, sentence, nlp):\n","\n","    # Calculate the number of tokens in the chunk\n","    chunk_length = len(nlp(chunk))\n","\n","    # Define the upper bound of the search\n","    end_index = chunk_start_index + chunk_length\n","\n","    for i in range(chunk_start_index, end_index-1):\n","\n","        if sentence[i].text == root.text:\n","            return i\n","\n","    return -1  # Return -1 if the root word is not found within the bounds\n","\n","def check_dependency_path(source_token, target_token):\n","\n","    source_path = []\n","    target_path = []\n","    current_token = source_token\n","\n","    # Trace path from source_token to root\n","    while current_token.head != current_token:\n","        source_path.append(current_token)\n","        current_token = current_token.head\n","    source_path.append(current_token)  # Include root\n","\n","    current_token = target_token\n","    # Trace path from target_token to root\n","    while current_token.head != current_token:\n","        target_path.append(current_token)\n","        current_token = current_token.head\n","    target_path.append(current_token)  # Include root\n","\n","    # Find lowest common ancestor\n","    set_source = set(source_path)\n","    common_ancestors = [token for token in target_path if token in set_source]\n","    if common_ancestors:\n","        # Return the path from source to LCA and LCA to target\n","        lca = common_ancestors[0]\n","        source_to_lca = source_path[:source_path.index(lca)+1]\n","        lca_to_target = target_path[:target_path.index(lca)+1]\n","        return [token.dep_ for token in source_to_lca + lca_to_target[::-1]]\n","    return []\n","\n","def analyze_chunk_dependency(sentence_text, source_phrase, target_phrase, nlp):\n","\n","    doc = nlp(sentence_text)\n","\n","    #Since the chunks are sometimes too big and DET are often not relevant we try to pinpoint the root word\n","    #For example in the source_phrase \"The MPON\" we see that MPON is more relevant than The\n","\n","    source_root_token = get_root_of_chunk(nlp, source_phrase[0])\n","    target_root_token = get_root_of_chunk(nlp, target_phrase[0])\n","\n","    # Directly match root tokens based on text and position\n","\n","    source_tokens = [token for token in doc if token.text == source_root_token.text and token.i == root_index_lookup(source_root_token,source_phrase[0], source_phrase[1], doc, nlp)]\n","    target_tokens = [token for token in doc if token.text == target_root_token.text and token.i == root_index_lookup(target_root_token,target_phrase[0], target_phrase[1], doc, nlp)]\n","\n","    if not source_tokens or not target_tokens:\n","        return [\"/\"]  # Return \"/\" indicating no tokens found\n","\n","    all_dependencies = set()  # Use a set to avoid duplicate entries\n","    dependency_found = False  # Reintroducing the boolean to track if any dependency was found\n","\n","    for s_token in source_tokens:\n","        for t_token in target_tokens:\n","            deps = check_dependency_path(s_token, t_token)\n","            if deps:\n","                all_dependencies.update(deps)\n","                dependency_found = True  # Set to True if any dependency is found\n","\n","\n","    if not dependency_found:\n","        #print(\"NO DEPENDENCY FOUND\")\n","        return [\"/\"]  # Return \"/\" if no dependencies were found\n","\n","    return list(all_dependencies)\n","\n","def encode_dependency_path(dependency_path, label_encoder):\n","\n","    encoded_path = []\n","    if not dependency_path:\n","        return -1\n","    else:\n","        for tag in dependency_path:\n","            try:\n","                # Normalize the tag to lower case and encode it\n","                encoded_tag = label_encoder.transform([tag.lower()])[0]\n","            except ValueError:\n","                # If the tag is unknown, assign a default value of -1\n","                encoded_tag = -1\n","            encoded_path.append(encoded_tag)\n","\n","        return encoded_path\n","\n","def initialize_label_encoder():\n","\n","    possible_tags = ['acl', 'advcl', 'advmod', 'amod', 'appos', 'attr', 'aux', 'auxpass',\n","    'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass',\n","    'dative', 'dep', 'det', 'discourse', 'dislocated', 'dobj', 'expl',\n","    'fixed', 'flat', 'goeswith', 'iobj', 'intj', 'list', 'mark', 'meta',\n","    'neg', 'nounmod', 'npmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd',\n","    'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep',\n","    'prt', 'punct', 'quantmod', 'relcl', 'root', 'xcomp', 'npadvmod',\n","    'complm', 'infmod', 'partmod', 'hmod', 'hyph', 'num', 'number',\n","    'nmod', 'nn', 'npadvmod', 'possessive', 'rcmod', '/']\n","\n","    label_encoder = LabelEncoder()\n","    label_encoder.fit(possible_tags)\n","    return label_encoder\n","\n","def find_neighboring_tags(entity_idx, ner_tags, direction='prev'):\n","    step = -1 if direction == 'prev' else 1\n","    start, end = (entity_idx - 1, -1) if direction == 'prev' else (entity_idx + 1, len(ner_tags))\n","    for i in range(start, end, step):\n","        if i >= 0 and i < len(ner_tags) and (ner_tags[i].startswith('B-') or ner_tags[i] == 'O'):\n","            return ner_tags[i]\n","    return 'NONE'\n","\n","def create_df(data_in, nlp_in):\n","\n","    data = data_in\n","\n","    nlp = nlp_in\n","    #nlp = spacy.load(\"en_core_web_trf\")  # takes forever, performs comparibly to en_core_web_md\n","    label_encoder = initialize_label_encoder() #To translate the paths into numbers for easier processing\n","\n","    transformed_data = []\n","\n","    tokens = data['tokens']\n","    ner_tags = data['ner_tags']\n","    sentence_ids = data['sentence-IDs']\n","    doc_name = data['document name']\n","    sentences = format_sentences(tokens, sentence_ids)\n","    # Generate POS tags for the tokens\n","    pos_tags = pos_tag(tokens)\n","\n","\n","    document_chunks = get_entity_chunks(data['tokens'], data['ner_tags'], data['sentence-IDs'])\n","\n","    # Initialize entities list using comprehensive condition checks for 'B-' prefixes\n","    entities = {\n","        (sentence_id, token_id): {\n","            'token': tokens[idx],\n","            'type': ner_tags[idx],\n","            'sentence_id': sentence_id,\n","            'pos_tag': pos_tags[idx][1],\n","            'token_id': token_id,\n","            'index': idx\n","        }\n","        for idx, (token_id, sentence_id) in enumerate(zip(data['tokens-IDs'], sentence_ids))\n","        if ner_tags[idx].startswith('B-')\n","    }\n","\n","    relations_dict = {\n","        (doc_name, rel['source-head-sentence-ID'], rel['source-head-word-ID'], rel['target-head-sentence-ID'],\n","          rel['target-head-word-ID']): rel['relation-type']\n","        for rel in data['relations']\n","    }\n","\n","    # Generate all combinations of entities and check for relations within the same sentence\n","    for ((src_sentence_id, src_token_id), source), ((tgt_sentence_id, tgt_token_id), target) in product(\n","            entities.items(), repeat=2):\n","        if (src_sentence_id, src_token_id) != (tgt_sentence_id, tgt_token_id):  # Explicitly prevent self-comparison\n","            if src_sentence_id == tgt_sentence_id:\n","\n","\n","                sentence = sentences[src_sentence_id]\n","\n","                # Extract full chunks for the source and target using their token_ids and sentence_ids.\n","                # It saves a tuple where the first element is the actual text and the second the start index position relative to the sentence.\n","                source_chunk = next(((chunk[0], chunk[1]) for chunk in document_chunks\n","                                      if chunk[1] <= src_token_id and chunk[2] == src_sentence_id and src_token_id <\n","                                      chunk[1] + len(chunk[0].split())),\n","                                    (source['token'], src_token_id))\n","\n","                target_chunk = next(((chunk[0], chunk[1]) for chunk in document_chunks\n","                                      if chunk[1] <= tgt_token_id and chunk[2] == tgt_sentence_id and tgt_token_id <\n","                                      chunk[1] + len(chunk[0].split())), (target['token'], tgt_token_id))\n","\n","\n","                results = analyze_chunk_dependency(sentence, source_chunk, target_chunk, nlp)\n","                results = encode_dependency_path(results, label_encoder)\n","            else:\n","                results = [\"/\"]\n","                results = encode_dependency_path(results, label_encoder)\n","\n","            relation_key = (doc_name, src_sentence_id, src_token_id, tgt_sentence_id, tgt_token_id)\n","            relation_type = relations_dict.get(relation_key, \"no_relation\")\n","\n","            # Get neighboring B-tags or 'O' for source and target\n","            src_prev_tag = find_neighboring_tags(source['index'], ner_tags, 'prev')\n","            src_next_tag = find_neighboring_tags(source['index'], ner_tags, 'next')\n","            tgt_prev_tag = find_neighboring_tags(target['index'], ner_tags, 'prev')\n","            tgt_next_tag = find_neighboring_tags(target['index'], ner_tags, 'next')\n","\n","            row = {\n","                'document_name': doc_name,\n","                'source_token': source['token'],\n","                'source_type': source['type'],\n","                'source_pos_tag': source['pos_tag'],  # Include source POS tag\n","                'source_sentence_ID': src_sentence_id,\n","                'source_token_ID': src_token_id,\n","                'source_prev_tag': src_prev_tag,\n","                'source_next_tag': src_next_tag,\n","                'target_token': target['token'],\n","                'target_type': target['type'],\n","                'target_pos_tag': target['pos_tag'],  # Include target POS tag\n","                'target_sentence_ID': tgt_sentence_id,\n","                'target_token_ID': tgt_token_id,\n","                'target_prev_tag': tgt_prev_tag,\n","                'target_next_tag': tgt_next_tag,\n","                'token_distance': abs(src_token_id - tgt_token_id),\n","                'sentence_distance': abs(src_sentence_id - tgt_sentence_id),\n","                'dependency_tags': results,\n","                'relation_type': relation_type\n","            }\n","            transformed_data.append(row)\n","\n","    df_relations = pd.DataFrame(transformed_data)\n","    if verbose:\n","      print(\"\\n---------------------------------\")\n","      print(\"Feature Generation Done!\")\n","      print(\"---------------------------------\\n\")\n","    filtered_df = df_relations[['source_token', 'target_token', 'dependency_tags', 'relation_type']]\n","    filtered_df = filtered_df[filtered_df['dependency_tags'].apply(lambda x: x != [0])]\n","    filtered_df_again = filtered_df[filtered_df['relation_type'] != \"no_relation\"]\n","    df_true_relations = df_relations[df_relations['relation_type'] != \"no_relation\"]\n","\n","    pd.set_option('display.max_rows', 500)\n","    pd.set_option('display.max_columns', 5)# Example: 500 rows\n","    if verbose:\n","      print(f\"total number of elements in dataframe: {len(df_relations)}\")\n","      \"\"\"print(f\"total number of non empty dependency rows in dataframe: {len(filtered_df)}\")\n","      print(f\"total number of non empty dependency rows WITH RELATION in dataframe: {len(filtered_df_again)}\")\n","      print(f\"total number of non empty relation rows in dataframe: {len(df_true_relations)}\")\"\"\"\n","      print(\"\\n\")\n","\n","    # Convert the 'dependency_tags' column from lists to strings\n","    df_relations['dependency_tags'] = df_relations['dependency_tags'].apply(lambda x: ' '.join(map(str, x)) if isinstance(x, list) else x)\n","\n","    return df_relations\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gPTokwxMV_Uw"},"outputs":[],"source":["#@title 3.2.4 Creating the input for Catboost\n","# Create a DataFrame\n","\n","pre_processing_data = copy.deepcopy(relational_data) # for clarity\n","df_relations = create_df(pre_processing_data, nlp)\n","\n","try:\n","  data_label = df_relations.relation_type #will all have default 'no relation' value\n","  df_relations.drop('relation_type', axis=1, inplace=True)\n","\n","except KeyError:\n","    print(\"The column 'relation_type' does not exist in the DataFrame.\")\n","\n","if verbose:\n","  print(f\"These are the heads of the table \\n{df_relations.head(5)}\")\n","# only for saving\n","#df_relations.to_csv(\"/content/drive/MyDrive/THESIS/CODING/NAM_TESTING/TESTING DATA/RE_TRAINING_DATA/Misc Testing/PIPELINE_TESTING_CATBOOST_INPUT.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"9Ge3Ts9JZe_j"},"source":["##3.3 PREDICTING WITH PRE-TRAINED CATBOOST MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iACgwWbyZrV1"},"outputs":[],"source":["# @title 3.3.1 Prediction Code\n","# Preparing input data and defining categorical data\n","categorical_features_indices = [i for i, typ in enumerate(df_relations.dtypes) if typ == 'object' or pd.api.types.is_categorical_dtype(df_relations.iloc[:, i])]\n","data_pool = Pool(df_relations, cat_features=categorical_features_indices)\n","\n","# Prediction happens here\n","predictions = best_model_neg_sample.predict(data_pool)\n","\n","# Convert numpy array to DataFrame\n","df_array = pd.DataFrame(predictions, columns=['relation_type'])\n","\n","# Concatenate DataFrames along columns (axis=1)\n","result_df = pd.concat([df_relations, df_array], axis=1)\n","\n","# Filter out desired columns\n","selected_columns = result_df.filter(['source_token', 'source_sentence_ID','source_token_ID','source_type', 'target_token', 'target_sentence_ID','target_token_ID','target_type', 'relation_type'])\n","\n","pd.set_option('display.width', 1000)\n","pd.set_option('display.max_columns', 10)\n","\n","relevant_rows = selected_columns[selected_columns['relation_type'] != 'no_relation']\n","if verbose:\n","  print(relevant_rows)\n","  print(f\"\\n{relevant_rows.shape[0]} Relations were predicted by Catboost\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"c-Ef0rtifRGL"},"source":["##3.4 Catboost Output\n","\n","You can optionally print the output if you have set the 'verbose' variable to True."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"6ZEZjjsr_lk7"},"outputs":[],"source":["# @title 3.4.1 Converting Predictions Into a Modelling Format\n","\n","# This function will look op the start word of a chunk and provide the whole text of that chunk as well as the span indices.\n","# This function uses the NER predictions in order to obtain the source and target labels\n","def lookup_tuple(token, sentence_id, token_id, goldstandard_check):\n","\n","  lookup = list()\n","\n","  Hit = False\n","  if goldstandard_check:\n","    checklist = NER_prediction['true_data']\n","\n","  else:\n","    checklist = NER_prediction['pred_data']\n","\n","  for chunk in checklist:\n","\n","    # we can say token_id == chunk['span'][0] because the token we are trying to look up will always be te first token of a text chunk\n","    if chunk['sentence_id'] == sentence_id and token_id == chunk['span'][0] and token in chunk['tokens'].split():\n","      lookup.append(chunk)\n","      Hit = True\n","\n","  if len(lookup) > 1: # we return nothing because there are duplicates and thus invalid, this should never happen\n","    return \"Duplicates found\", Hit\n","\n","  if len(lookup) == 0:\n","    return \"No match found\", Hit\n","\n","  else:\n","    return lookup[0], Hit\n","\n","# Extracting RE data into format, using the labels created by NER\n","predicted_relations = list()\n","for i, row in relevant_rows.iterrows():\n","  source,_ = lookup_tuple(row['source_token'], row['source_sentence_ID'], row['source_token_ID'], use_goldstandardNER)\n","  target,_ = lookup_tuple(row['target_token'], row['target_sentence_ID'], row['target_token_ID'], use_goldstandardNER)\n","  try:\n","    s_tuple = tuple(source.values())\n","    t_tuple = tuple(target.values())\n","  except AttributeError as a:\n","    print(a)\n","    print(f\"source: {source}\")\n","    print(row['source_token'], row['source_sentence_ID'], row['source_token_ID'])\n","    print('\\n')\n","    print(f\"target: {target}\")\n","    print(row['target_token'], row['target_sentence_ID'], row['target_token_ID'])\n","    print('---')\n","\n","  predicted_relations.append({\n","      \"relation_type\": row['relation_type'],\n","      \"source_chunk\": s_tuple,\n","      \"target_chunk\": t_tuple\n","  })\n","\n","# For Visualisation:\n","if verbose:\n","  for relation in predicted_relations:\n","    print('{')\n","    print('relation_type: ', relation['relation_type'])\n","    print('source_chunk: ', relation['source_chunk'])\n","    print('target_chunk: ', relation['target_chunk'])\n","    print('}\\n')\n"]},{"cell_type":"markdown","metadata":{"id":"-A3FUx2EXcO3"},"source":["#4 Entity Resolution Task\n","\n","In this section we will resolve the NER and RE data using Coreference Resolution."]},{"cell_type":"markdown","metadata":{"id":"QoILT4VPfr2i"},"source":["##4.1. Extracting Clusters\n","\n","Clusters of the same entity that are mentioned multiple times are put together here. Since the accuracy of the extracted clusters is not guaranteed, we have built in an option for you to manually correct (remove) incorrect mentions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IWhKYw8pYorA"},"outputs":[],"source":["# @title 4.1.1 Extracting Clusters Code\n","sentences = ' '.join(word for sentence_data in NER_processed_output for word in sentence_data['sentence']) # from the BERT processed output\n","\n","flag_coreference_trf = '/content/coreference_trf'\n","\n","# Check if the flag file exists\n","if not os.path.exists(flag_coreference_trf):\n","  nlp_coref = spacy.load(\"en_coreference_web_trf\")\n","  # Create the flag file\n","  with open(flag_coreference_trf, 'w') as f:\n","      f.write('Installed')\n","\n","doc = nlp_coref(sentences)\n","\n","labels2resolve = ['Actor', 'Activity Data']\n","\n","# Process Coreference Clusters\n","clusters = list()\n","clusters_RE = list()\n","for cluster_id, cluster in enumerate(doc.spans.values()):\n","    cluster_dict = {\"cluster_id\": cluster_id + 1, \"mentions\": []}\n","    cluster_dict_RE = {\"cluster_id\": cluster_id + 1, \"mentions\": []}\n","\n","    for mention in cluster:\n","        # Find the sentence ID\n","        sent_id = next(i for i, sent in enumerate(doc.sents) if mention.start >= sent.start and mention.end <= sent.end)\n","        sent_start = list(doc.sents)[sent_id].start\n","        span_start = mention.start - sent_start\n","        span_end = mention.end - sent_start - 1\n","\n","        # Extract the mention text and label\n","        value, hit= lookup_tuple(mention[0].text, sent_id, span_start, use_goldstandardNER) # Looks up data based on NER predictions/ Goldstandard NER\n","        if hit and value['label'] in labels2resolve: # We only keep those mentions that have a NER tag extracted. For those who don't we do not add it to the cluster\n","            if use_goldstandardRE:\n","              get = extract_entity_chunk_RE(sent_id,span_start, testing_document_df)\n","\n","            else:\n","              get = None # does not matter what get is because we do not use clusters_RE. This is deprecated.\n","            cluster_dict[\"mentions\"].append(tuple(value.values()))\n","            cluster_dict_RE[\"mentions\"].append(get)\n","\n","    clusters.append(cluster_dict)\n","    clusters_RE.append(cluster_dict_RE)\n","\n","filtered_clusters = list()\n","clusters = [cluster for cluster in clusters if len(cluster['mentions']) >= 2] # Because some clusters can have only 1 mention, this is because we only take mentions that have a NER tag.\n","for idx, cluster in enumerate(clusters): # because the ids may not be in order anymore\n","    cluster['cluster_id'] = idx + 1\n","clusters_RE = [cluster for cluster in clusters_RE if len(cluster['mentions']) >= 2] # Because some clusters can have only 1 mention, this is because we only take mentions that have a specific NER tag.\n","for idx, cluster in enumerate(clusters_RE):\n","    cluster['cluster_id'] = idx + 1\n","\n","for cluster in clusters:\n","    cluster['mentions'] = sorted(cluster['mentions'], key=lambda x: len(x[0]), reverse=True) # This sorts the mentions according to string length from largest to smallest (we assume that the most complete entity is the longest one)\n","for cluster in clusters_RE:\n","    try:\n","      cluster['mentions'] = sorted(cluster['mentions'], key=lambda x: len(x[0]), reverse=True) # This sorts the mentions according to string length from largest to smallest (we assume that the most complete entity is the longest one)\n","    except TypeError as t:\n","      continue\n","\n","\n","# Following lines are for re-assigning cluster IDs and for visualization:\n","for id, cluster in enumerate(clusters, start=1):\n","    cluster['cluster_id'] = id\n","    print(f\"\\nCluster ID {cluster['cluster_id']}:\\n\")\n","    for idx, mention in enumerate(cluster['mentions'], start=1):  # Corrected loop\n","        print(f\"Mention ID {idx}: {mention}\")\n","\n","\n","#Initiate or reinitiate the 'valid_clusters' if something went wrong deleting the mentions of a cluster in the next cells.\n","if use_goldstandardRE:\n","  valid_clusters= copy.deepcopy(clusters_RE)\n","else:\n","  valid_clusters = copy.deepcopy(clusters)\n","\n"]},{"cell_type":"code","source":["#@title Do you wish to delete mentions from specific clusters?\n","#@markdown Please enter the cluster ID\n","#@markdown. Note that you rerun this cell multiple times.\n","\n","\n","CLUSTER_ID = 0 #@param {type:\"integer\"}\n","\n","if CLUSTER_SELECTION != 0:\n","  selected_cluster = next((cluster for cluster in valid_clusters if cluster['cluster_id'] == CLUSTER_SELECTION), None)\n","  if selected_cluster:\n","      print(f\"Selected Cluster ID: {selected_cluster['cluster_id']}\")\n","\n","      #@markdown Please enter the mention ID\n","\n","      MENTION_ID = 0 #@param {type:\"integer\"}\n","      if MENTION_SELECTION > len(selected_cluster['mentions']):\n","        print(\"Selected Mention ID: \")\n","        print(\"Your selection was invalid, please try again.\")\n","\n","      elif MENTION_SELECTION == 0:\n","          print(\"No Mention ID selected.\")\n","\n","      elif MENTION_SELECTION != 0 and selected_cluster:\n","        for idx, mention in enumerate(selected_cluster['mentions'], start=1):\n","          if MENTION_SELECTION == idx:\n","            print(f\"Selected Mention ID {idx}: {mention}\")\n","\n","  else:\n","      print(\"Cluster not found.\")\n","else:\n","    selected_cluster = None\n","    print(\"No modifications needed.\")\n","\n"],"metadata":{"id":"YARfnoMfsm-M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Follow up\n","#@markdown Do you confirm to delete the selected mention from the cluster?\n","answer = \"NO\" #@param [\"YES\", \"NO\"] {type:\"string\"}\n","\n","if answer == \"YES\":\n","  mention_to_remove = None\n","  if selected_cluster != None:\n","    for idx, mention in enumerate(selected_cluster['mentions'], start=1):\n","      if MENTION_SELECTION == idx:\n","        mention_to_remove = mention\n","        break\n","    selected_cluster['mentions'].remove(mention_to_remove)\n","    print(f\"\\nRemoved mention: {mention_to_remove}\")\n","    print('--------')\n","    print(f\"These are the updated clusters:\")\n","    for cluster in valid_clusters:\n","      print(f\"\\nCluster ID {cluster['cluster_id']}:\\n\")\n","      for idx, mention in enumerate(cluster['mentions'], start=1):  # Corrected loop\n","          print(f\"Mention ID {idx}: {mention}\")\n","  else:\n","    print(\"No cluster selected.\")\n","else:\n","    print(\"\\n=> Deletion cancelled.\")"],"metadata":{"id":"uSqSWTDB2u5S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yvNOt2N1fxAC"},"source":["##4.2. ER output for NER\n","\n","You can optionally print the output if you have set the 'verbose' variable to True."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ZmvP25y2wvl"},"outputs":[],"source":["#@title 4.2.1 Resolving NER data using ER data\n","if use_goldstandardNER:\n","  resolved_NER = copy.deepcopy(NER_prediction['true_data'])\n","else:\n","  resolved_NER = copy.deepcopy(NER_prediction['pred_data']) # for clarity\n","\n","target_labels = ['Actor', 'Activity Data']\n","count =0\n","\n","for entry in resolved_NER:\n","  for cluster in valid_clusters:\n","\n","    if tuple([entry['tokens'], entry['label'], entry['sentence_id'], entry['span']]) in cluster['mentions'] and entry['label'] in target_labels:\n","\n","      #print(f\"HIT at {entry}\")\n","      entry['tokens'] = cluster['mentions'][0][0] # aka the string of the first mention\n","      count += 1\n","\n","# for visualisation\n","if verbose:\n","  print(f\"{count} NER entries were resolved\\n\")\n","  for NER in resolved_NER:\n","    print('\\n', NER)\n"]},{"cell_type":"markdown","metadata":{"id":"2Rd6N6yMf97H"},"source":["##4.3. ER output for RE\n","\n","You can optionally print the output if you have set the 'verbose' variable to True."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xnnDe6XUwAGz"},"outputs":[],"source":["# @title 4.3.1 Resolving RE data using ER data\n","\n","target_labels = ['Actor', 'Activity Data']\n","count = 0\n","\n","if use_goldstandardRE:\n","  resolved_RE = copy.deepcopy(goldstandard_relations) # for clarity\n","  for entry in resolved_RE:\n","    for cluster in valid_clusters: # because use_goldstandardRE is already checked when making valid clusters\n","\n","      if entry['source_chunk'] in cluster['mentions']:\n","        entry['source_chunk'] = tuple([cluster['mentions'][0][0], entry['source_chunk'][1], entry['source_chunk'][2], entry['source_chunk'][3]])\n","        count += 1\n","\n","      if entry['target_chunk'] in cluster['mentions']:\n","        entry['target_chunk'] = tuple([cluster['mentions'][0][0], entry['target_chunk'][1], entry['target_chunk'][2], entry['target_chunk'][3]])\n","        count += 1\n","\n","  print(f\"{count} goldstandard relations were resolved\\n\")\n","\n","else:\n","  resolved_RE = copy.deepcopy(predicted_relations) # for clarity\n","  for entry in resolved_RE:\n","    for cluster in valid_clusters: # because use_goldstandardRE is already checked when making valid clusters\n","\n","      if entry['source_chunk'] in cluster['mentions']:\n","        entry['source_chunk'] = tuple([cluster['mentions'][0][0], entry['source_chunk'][1], entry['source_chunk'][2], entry['source_chunk'][3]])\n","        count += 1\n","\n","      if entry['target_chunk'] in cluster['mentions']:\n","        entry['target_chunk'] = tuple([cluster['mentions'][0][0], entry['target_chunk'][1], entry['target_chunk'][2], entry['target_chunk'][3]])\n","        count += 1\n","\n","# for visualisation\n","if verbose:\n","\n","  print(f\"{count} predicted relations were resolved\\n\")\n","  for entry in resolved_RE:\n","    if entry['relation_type'] == 'flow':\n","\n","      print('{')\n","      print('relation_type: ', entry['relation_type'])\n","      print('source_chunk: ', entry['source_chunk'])\n","      print('target_chunk: ', entry['target_chunk'])\n","      print('}\\n')\n"]},{"cell_type":"markdown","metadata":{"id":"VbNKMG1g-d8H"},"source":["#5 BPMN Modelling\n","\n","You can find the code for the Model Generation phase in this section."]},{"cell_type":"markdown","metadata":{"id":"UARzO0uwRDB3"},"source":["In this step we will generate a BPMN diagram from the relations.\n","We also include the entities where no relation was found for completeness such that the user can still add nodes and draw edges where necessary."]},{"cell_type":"markdown","metadata":{"id":"YXGCz51wxVIM"},"source":["##5.1 Preprocessing Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJysrzZwRCh8"},"outputs":[],"source":["#@title Transform resolved relations to format\n","#First we transform the relations to the correct format, every string in the 'tokens' section should be lowercased if it isnt already.\n","\n","#check the correctness please => Checked, Nam\n","\n","lowercase_relations = []\n","\n","for relation in resolved_RE:\n","    source_chunk = relation['source_chunk']\n","    target_chunk = relation['target_chunk']\n","\n","    # Convert the tokens in the source_chunk and target_chunk to lowercase\n","    new_source_chunk = (source_chunk[0].lower(), source_chunk[1], source_chunk[2], source_chunk[3])\n","    new_target_chunk = (target_chunk[0].lower(), target_chunk[1], target_chunk[2], target_chunk[3])\n","\n","    # Create a new relation dictionary with the modified chunks\n","    new_relation = {\n","        \"relation_type\": relation['relation_type'],\n","        \"source_chunk\": new_source_chunk,\n","        \"target_chunk\": new_target_chunk\n","    }\n","\n","    lowercase_relations.append(new_relation)\n","\n","sorted_relations = sorted(lowercase_relations, key=lambda x: x['relation_type'] != 'same gateway') #sort with same gateway relations first, as we do not want to unnecessary nodes\n","\n","\n","BPMN_relations = copy.deepcopy(sorted_relations) #replace  the original relations with the sorted lowercased ones\n","if verbose:\n","    for relation in BPMN_relations:\n","        print('{')\n","        print('relation_type: ', relation['relation_type'])\n","        print('source_chunk: ', relation['source_chunk'])\n","        print('target_chunk: ', relation['target_chunk'])\n","        print('}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bHd3Q4_R6ra"},"outputs":[],"source":["#@title Transform resolved NER entities to format\n","\n","mention_chunks_no_edit = copy.deepcopy(resolved_NER) # for clarity and data protection\n","mention_chunks = []\n","\n","for mention in mention_chunks_no_edit:\n","    token = mention['tokens']\n","    element_type = mention['label']  # or another relevant type if needed\n","    sentence_id = mention['sentence_id']\n","    tokenspan = mention['span']\n","    new_chunk = (token, element_type, sentence_id, tokenspan)\n","    mention_chunks.append(new_chunk)\n","\n","if verbose:\n","    print(mention_chunks)"]},{"cell_type":"markdown","metadata":{"id":"bDXEjFl-xZtf"},"source":["##5.2 Model Generation"]},{"cell_type":"markdown","metadata":{"id":"RhOp39t6SRMi"},"source":["Define node styles and colors depending on the NER type.\n","Initiate all necessary functions.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l5PnMIGPSQzg"},"outputs":[],"source":["#@title Node styles and function initiations\n","\n","# Define node and edge styles\n","node_styles = {\n","    'Activity': 'box',\n","    'XOR Gateway': 'diamond',\n","    'AND Gateway': 'diamond',\n","    'Condition Specification': 'note',\n","    'Actor': 'ellipse',\n","    'Activity Data': 'note',\n","    'Further Specification': 'note',\n","    'Start Event': 'circle',\n","    'End Event': 'doublecircle'\n","}\n","\n","node_colors = {\n","    'Activity': 'black',\n","    'XOR Gateway': 'black',\n","    'AND Gateway': 'black',\n","    'Condition Specification': 'black',\n","    'Actor': 'darkgrey',\n","    'Activity Data': 'darkgrey',\n","    'Further Specification': 'darkgrey'\n","}\n","\n","# Function to add nodes\n","def add_node(chunk):\n","    label, element_type, sent_id, tokenspan = chunk\n","    if element_type == 'Actor':\n","        if label not in actors:\n","            shape = node_styles.get(element_type, 'box')\n","            color = node_colors.get(element_type, 'black')\n","            dot.node(label, label, shape=shape, color=color)\n","            actors.append(label)\n","        \"\"\"else:\n","          print(f\"actor: {label} is already added\")\"\"\"\n","\n","    elif element_type == 'Activity Data':\n","        if label not in activity_datas:\n","            shape = node_styles.get(element_type, 'box')\n","            color = node_colors.get(element_type, 'black')\n","            dot.node(label, label, shape=shape, color=color)\n","            activity_datas.append(label)\n","        \"\"\"else:\n","          print(f\"activity data: {label} is already added\")\"\"\"\n","\n","    elif element_type == 'XOR Gateway':\n","        display_label = 'X'\n","        if chunk not in XOR_gateways:\n","            shape = node_styles.get(element_type, 'box')\n","            color = node_colors.get(element_type, 'black')\n","            new_label = label + '_' + str(sent_id) + '_' + str(tokenspan[0])\n","            dot.node(new_label, display_label, shape=shape, color=color)\n","            XOR_gateways.append(chunk)\n","        \"\"\"else:\n","            print(f\"xor gateway: {label} is already added\")\"\"\"\n","\n","\n","    elif element_type == 'AND Gateway':\n","        display_label = '+'\n","        if chunk not in AND_gateways:\n","            shape = node_styles.get(element_type, 'box')\n","            color = node_colors.get(element_type, 'black')\n","            new_label = label + '_' + str(sent_id) + '_' + str(tokenspan[0])\n","            dot.node(new_label, display_label, shape=shape, color=color)\n","            AND_gateways.append(chunk)\n","        \"\"\"else:\n","            print(f\"and gateway: {label} is already added\")\"\"\"\n","\n","    elif element_type == 'Activity':\n","        display_label = label\n","        if chunk not in activities:\n","            shape = node_styles.get(element_type, 'box')\n","            color = node_colors.get(element_type, 'black')\n","            new_label = label + '_' + str(sent_id) + '_' + str(tokenspan[0])\n","            dot.node(new_label, display_label, shape=shape, color=color)\n","            activities.append(chunk)\n","        \"\"\"else:\n","            print(f\"activity: {label} is already added\")\"\"\"\n","\n","    elif element_type == 'Condition Specification':\n","        #print(f\"This is a Condition Specification chunk which does not need a node: {chunk}\")\n","        specifications.append(chunk)\n","\n","    elif element_type == 'Further Specification':\n","        if chunk not in specifications:\n","            shape = node_styles.get(element_type, 'box')\n","            color = node_colors.get(element_type, 'black')\n","            new_label = label + '_' + str(sent_id) + '_' + str(tokenspan[0])\n","            dot.node(new_label, label, shape=shape, color=color)\n","            specifications.append(chunk)\n","        \"\"\"else:\n","            print(f\"further specification: {label} is already added\")\"\"\"\n","    else:\n","      print(f\"element type: {element_type} is not defined\")\n","\n","\n","# Function to add edges based on relation type\n","def add_edge(source_chunk, target_chunk, relation_type, relations):\n","    source_label = source_chunk[0]\n","    target_label = target_chunk[0]\n","\n","    if relation_type == 'flow':\n","        if target_chunk[1] == 'Condition Specification':\n","            # Add condition specification to the edge label\n","            #print(f\"there should be a condition specification flow between {source_label, target_label}\")\n","            #dot.edge(source_label, added_nodes[target_chunk[0]][0], label=target_label) #relation from xor to condition specification does not need to be printed, only the flow from xor to next activity with corresponding condition specification\n","            specifications.append(target_chunk)\n","\n","        elif source_chunk[1] == 'Condition Specification':\n","            #print(f\"There is a condition specification flow that goes from {source_chunk} to {target_chunk}\")\n","            #source_chunks_new = [relation['source_chunk'] for relation in relations if relation['target_chunk'] == source_chunk]\n","            source_chunk_temp = next((relation['source_chunk'] for relation in relations if relation['target_chunk'] == source_chunk), None)\n","            #print('this is source_chunk_temp: ', source_chunk_temp, 'replacing this: ', source_chunk)\n","            saga_relations = [relation for relation in relations if relation['relation_type'] == 'same gateway']\n","            source_chunk_new = next((relation['source_chunk'] for relation in saga_relations if relation['target_chunk'] == source_chunk_temp), source_chunk_temp)\n","            #print('this is source_chunk_new: ', source_chunk_new, 'replacing this: ', source_chunk)\n","            add_node(source_chunk_new)\n","            add_node(target_chunk)\n","            source_label_new, source_element_type, source_sent_id, source_tokenspan = source_chunk_new\n","            target_label, target_element_type, target_sent_id, target_tokenspan = target_chunk\n","            source_chunk_new_label = source_label_new + '_' + str(source_sent_id) + '_' + str(source_tokenspan[0])\n","            target_chunk_new_label = target_label + '_' + str(target_sent_id) + '_' + str(target_tokenspan[0])\n","            dot.edge(source_chunk_new_label, target_chunk_new_label, style = 'bold', xlabel=source_label)\n","            specifications.append(source_chunk)\n","\n","\n","        elif source_chunk[1] == 'XOR Gateway' and target_chunk[1] != 'Condition Specification':\n","            saga_relations = [relation for relation in relations if relation['relation_type'] == 'same gateway']\n","            new_source_chunk = next((relation['source_chunk'] for relation in saga_relations if relation['target_chunk'] == source_chunk), source_chunk)\n","            add_node(new_source_chunk)\n","            add_node(target_chunk)\n","            source_label_new, source_element_type, source_sent_id, source_tokenspan = new_source_chunk\n","            target_label, target_element_type, target_sent_id, target_tokenspan = target_chunk\n","            source_chunk_new_label = source_label_new + '_' + str(source_sent_id) + '_' + str(source_tokenspan[0])\n","            target_chunk_new_label = target_label + '_' + str(target_sent_id) + '_' + str(target_tokenspan[0])\n","            dot.edge(source_chunk_new_label, target_chunk_new_label, style ='bold')\n","\n","        elif source_chunk[1] != 'Condition Specification' and target_chunk[1] == 'XOR Gateway':\n","            saga_relations = [relation for relation in relations if relation['relation_type'] == 'same gateway']\n","            new_target_chunk = next((relation['source_chunk'] for relation in saga_relations if relation['target_chunk'] == target_chunk), target_chunk)\n","            add_node(source_chunk)\n","            add_node(new_target_chunk)\n","            target_label_new, target_element_type, target_sent_id, target_tokenspan = new_target_chunk\n","            source_label, source_element_type, source_sent_id, source_tokenspan = source_chunk\n","            target_chunk_new_label = target_label_new + '_' + str(target_sent_id) + '_' + str(target_tokenspan[0])\n","            source_chunk_new_label = source_label + '_' + str(source_sent_id) + '_' + str(source_tokenspan[0])\n","            dot.edge(source_chunk_new_label, target_chunk_new_label, style ='bold')\n","            #print(\"This has happened.\")\n","\n","        else:\n","            add_node(source_chunk)\n","            add_node(target_chunk)\n","            source_label, source_element_type, source_sent_id, source_tokenspan = source_chunk\n","            target_label, target_element_type, target_sent_id, target_tokenspan = target_chunk\n","            new_source_label = source_label + '_' + str(source_sent_id) + '_' + str(source_tokenspan[0])\n","            new_target_label = target_label + '_' + str(target_sent_id) + '_' + str(target_tokenspan[0])\n","            dot.edge(new_source_label, new_target_label, style ='bold')\n","            #print(f\"else edge has happened. for source {source_chunk} and target {target_chunk}\")\n","\n","    elif relation_type == 'actor performer':\n","        add_node(source_chunk) #activity\n","        add_node(target_chunk) #actor (performer)\n","        source_label, source_element_type, source_sent_id, source_tokenspan = source_chunk\n","        source_chunk_label = source_label + '_' + str(source_sent_id) + '_' + str(source_tokenspan[0])\n","        dot.edge(source_chunk_label, target_label, style='solid', xlabel='performer', color='blue')\n","\n","    elif relation_type == 'actor recipient':\n","        add_node(source_chunk) #activity\n","        add_node(target_chunk) #actor (recipient)\n","        source_label, source_element_type, source_sent_id, source_tokenspan = source_chunk\n","        source_chunk_label = source_label + '_' + str(source_sent_id) + '_' + str(source_tokenspan[0])\n","        dot.edge(source_chunk_label, target_label, style='solid', xlabel='recipient', color='darkorange')\n","\n","    elif relation_type == 'same gateway':\n","        add_node(source_chunk)\n","        XOR_gateways.append(target_chunk)\n","        add_node(target_chunk)\n","\n","    elif relation_type == 'uses':\n","        add_node(source_chunk)\n","        add_node(target_chunk)\n","        source_label, source_element_type, source_sent_id, source_tokenspan = source_chunk\n","        source_chunk_label = source_label + '_' + str(source_sent_id) + '_' + str(source_tokenspan[0])\n","        dot.edge(source_chunk_label, target_label, style='dashed', xlabel='uses', color='darkgrey')\n","\n","    elif relation_type == 'further specification':\n","        add_node(source_chunk)\n","        add_node(target_chunk)\n","        source_label, source_element_type, source_sent_id, source_tokenspan = source_chunk\n","        target_label, target_element_type, target_sent_id, target_tokenspan = target_chunk\n","        new_source_label = source_label + '_' + str(source_sent_id) + '_' + str(source_tokenspan[0])\n","        new_target_label = target_label + '_' + str(target_sent_id) + '_' + str(target_tokenspan[0])\n","        dot.edge(new_source_label, new_target_label, style='dotted', xlabel='further specification', color='darkgrey')\n","\n","\n","def find_starting_chunk(relations):\n","  \"\"\"\n","  Finds the chunk representing the starting node.\n","\n","  Args:\n","    relations: A list of dictionaries representing relations between nodes.\n","\n","  Returns:\n","    The chunk representing the starting node or None if not found.\n","  \"\"\"\n","\n","  outgoing_flow_nodes = set()\n","  incoming_flow_nodes = set()\n","\n","  for relation in relations:\n","    if relation['relation_type'] == 'flow':\n","      outgoing_flow_nodes.add(relation['source_chunk'])\n","      incoming_flow_nodes.add(relation['target_chunk'])\n","\n","  # Find nodes with outgoing flows but no incoming flows\n","  starting_node_candidates_non_sorted = list(outgoing_flow_nodes - incoming_flow_nodes)\n","  starting_node_candidates = sorted(starting_node_candidates_non_sorted, key=lambda x: x[2])\n","\n","\n","  if starting_node_candidates:\n","    starting_node_label = starting_node_candidates[0]  # Assume the first candidate is the main starting node\n","    for relation in relations:\n","      if relation['source_chunk'][0] == starting_node_label[0]:\n","        return relation['source_chunk']\n","\n","  return None\n","\n","def find_possible_last_chunks(relations):\n","\n","  outgoing_flow_nodes = set()\n","  incoming_flow_nodes = set()\n","\n","  for relation in relations:\n","    if relation['relation_type'] == 'flow':\n","      outgoing_flow_nodes.add(relation['source_chunk'])\n","      incoming_flow_nodes.add(relation['target_chunk'])\n","\n","  # Find nodes with incoming flows but no outgoing flows\n","  last_node_candidates = list(incoming_flow_nodes - outgoing_flow_nodes)\n","\n","  last_chunks = []\n","  for candidate in last_node_candidates:\n","    for relation in relations:\n","      if relation['target_chunk'] == candidate:\n","        last_chunks.append(relation['target_chunk'])\n","        break  # Move to the next candidate after finding one match\n","\n","  return last_chunks\n"]},{"cell_type":"markdown","metadata":{"id":"6GfkzbZuTo7g"},"source":["Start creating the directed graph. Running this cell creates the dot graph and visualizes it in a png."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vlg9uwnATpQ7"},"outputs":[],"source":["#@title Generate the BPMN diagram to a png visualization\n","#Start creating the directed graph. Running this cell creates the dot graph and visualizes it in a png.\n","\n","\n","# Create a new directed graph: some examples of starting the dot graph with different 'attributes' for the sake of spacing between the nodes and edges\n","\n","#dot = Digraph('BPMN Diagram', comment='Generated BPMN Diagram', graph_attr={'rankdir': 'LR', 'splines': 'ortho'})\n","#dot = Digraph('BPMN Diagram', comment='Generated BPMN Diagram', graph_attr={'rankdir': 'LR', 'nodesep': '1', 'ranksep': '1.2', 'overlap': 'scale', 'splines': 'ortho'})\n","#dot = Digraph('BPMN Diagram', comment='Generated BPMN Diagram', graph_attr={'rankdir': 'LR', 'nodesep': '2', 'splines': 'ortho', 'overlap': 'scale', 'ranksep': '1.2'})\n","\n","\n","# Create a new directed graph: some examples of starting the dot graph with different 'attributes' for the sake of spacing between the nodes and edges\n","\n","#dot = Digraph('BPMN Diagram', comment='Generated BPMN Diagram', graph_attr={'rankdir': 'LR', 'splines': 'ortho'})\n","#dot = Digraph('BPMN Diagram', comment='Generated BPMN Diagram', graph_attr={'rankdir': 'LR', 'nodesep': '1', 'ranksep': '1.2', 'overlap': 'scale', 'splines': 'ortho'})\n","#dot = Digraph('BPMN Diagram', comment='Generated BPMN Diagram', graph_attr={'rankdir': 'LR', 'nodesep': '2', 'splines': 'ortho', 'overlap': 'scale', 'ranksep': '1.2'})\n","dot = Digraph('BPMN Diagram',\n","              comment='Generated BPMN Diagram',\n","              graph_attr={\n","                  'rankdir': 'LR',    # Left to right graph\n","                  'nodesep': '0.6',     # Increase node separation\n","                  #'splines': 'ortho', # Use orthogonal lines\n","                  'overlap': 'False', # Scale overlap\n","                  'ranksep': '5',   # Increase rank separation\n","                  'fontsize': '12',   # Default text size\n","                  'fontname': 'Arial' # Default font\n","              })\n","\n","\n","# Track added nodes to avoid duplicates, this is a dictionary with as keys the 'tokens' and as values the 'chunk'\n","#added_nodes = {}\n","actors = []\n","activity_datas = []\n","XOR_gateways = []\n","AND_gateways = []\n","activities = []\n","specifications = []\n","\n","\n","#keep track of visited gateways\n","#open_gateways = []\n","#XOR_gateways = []\n","\n","#keep track of condition specifications that have been visited\n","#specifications = []\n","\n","#create the start event\n","dot.node('START', 'START', shape= node_styles.get('Start Event', 'box'), color= 'green', style='filled')\n","\n","#find the first node to connect the start event with\n","starting_chunk = find_starting_chunk(BPMN_relations)\n","if starting_chunk:\n","  #print(\"The starting chunk is:\", starting_chunk)\n","  add_node(starting_chunk)\n","  #create an edge between start event and the first node\n","  start_label, start_element_type, start_sent_id, start_tokenspan = starting_chunk\n","  starting_chunk_label = start_label + '_' + str(start_sent_id) + '_' + str(start_tokenspan[0])\n","  dot.edge('START', starting_chunk_label, style= 'bold')\n","else:\n","  print(\"Could not determine the starting chunk.\")\n","\n","# Add edges\n","for relation in BPMN_relations:\n","    source_chunk = relation['source_chunk']\n","    target_chunk = relation['target_chunk']\n","    relation_type = relation['relation_type']\n","\n","    add_edge(source_chunk, target_chunk, relation_type, BPMN_relations)\n","\n","\n","#when all relations are processed add the END event node\n","dot.node('END', 'END', shape= 'circle', color='red', style='filled')\n","\n","#Find the last_chunks to be connected to the END node\n","last_chunks = find_possible_last_chunks(BPMN_relations)\n","#print(f\"these are the possible last chunks: {last_chunks}\")\n","\n","# Create a unique XOR Gateway if multiple last chunks exist\n","if len(last_chunks) > 1:\n","    xor_gateway_label = 'XOR_Join'\n","    dot.node(xor_gateway_label, 'X_join', shape='diamond')\n","    for last_chunk in last_chunks:\n","        last_label, last_element_type, last_sent_id, last_tokenspan = last_chunk\n","        last_chunk_label = last_label + '_' + str(last_sent_id) + '_' + str(last_tokenspan[0])\n","        dot.edge(last_chunk_label, xor_gateway_label, style='bold')\n","    # Finally, connect the XOR gateway to the end event\n","    dot.edge(xor_gateway_label, 'END', style='bold')\n","elif len(last_chunks) == 1: # Check if last_chunks is not empty\n","    # Direct connection if only one last chunk\n","    last_label, last_element_type, last_sent_id, last_tokenspan = last_chunks[0]\n","    last_chunk_label = last_label + '_' + str(last_sent_id) + '_' + str(last_tokenspan[0])\n","    dot.edge(last_chunk_label, 'END', style='bold')\n","else:\n","    print(\"No last chunks found. The diagram might be incomplete.\") # Handle the case when no last chunks are found\n","\n","\n","#From here on, the BPMN diagram is generated.\n","#It could be that relations were missed and therefore some entities were not generated in the diagram.\n","#The following loop runs over every chunk in the mention_chunk list and checks if an element should be made.\n","\n","for mention_chunk in mention_chunks:\n","    label, element_type, sent_id, tokenspan = mention_chunk\n","\n","    add_node(mention_chunk)\n","\n","    if element_type == 'Condition Specification' or element_type == 'Further Specification':\n","        if mention_chunk not in specifications:\n","            shape = node_styles.get(element_type, 'box')\n","            color = 'grey'\n","            display_label = label + '(' + element_type + ')'\n","            new_label = label + '_' + str(sent_id) + '_' + str(tokenspan[0])\n","            dot.node(new_label, display_label, shape=shape, color=color)\n","            specifications.append(mention_chunk)\n","\n","    else:\n","      print(\"\")\n","\n","\n","# render the graph to a png\n","png_data = dot.pipe(format='png')\n","\n","# display the png_data from above\n","#display(Image(png_data))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"69lPaM4Fc6dS"},"source":["#6 Pipeline Output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gAYUTnvAc131"},"outputs":[],"source":["display(Image(png_data))"]},{"cell_type":"markdown","source":["Write to a dot file to see the order of drawing the model."],"metadata":{"id":"RudkBAvR3BpZ"}},{"cell_type":"code","source":["with open('/content/graph.dot', 'w') as file:\n","    file.write(dot.source)"],"metadata":{"id":"TYXxfUW_thtk"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1FpogBCfw3b4IEjderoVwbssXZHq9l-bm","timestamp":1720351965786},{"file_id":"1VnYScPOufCH4-xuG0CxR1xsXPWtXN7-z","timestamp":1720175532957},{"file_id":"1U5R966is7O2G3yMhUK0M1BJtM0s5vtwX","timestamp":1720170420712}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}